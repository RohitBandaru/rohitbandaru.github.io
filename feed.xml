<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://rohitbandaru.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rohitbandaru.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-09T18:11:51+00:00</updated><id>https://rohitbandaru.github.io/feed.xml</id><title type="html">blank</title><subtitle>ML blog.</subtitle><entry><title type="html">Transformer Design Guide (Part 1: Vanilla)</title><link href="https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt1/" rel="alternate" type="text/html" title="Transformer Design Guide (Part 1: Vanilla)"/><published>2024-11-03T00:00:00+00:00</published><updated>2024-11-03T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt1</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt1/"><![CDATA[<p>The Transformer architecture has emerged as the cornerstone of deep learning and artificial intelligence. Despite its conceptual simplicity, the specific details of the architecture can be difficult to understand and reason about. This two-part blog series aims to provide a thorough examination of the Transformer, demystifying its core components and recent advancements. The goal is to cover the fundamental and cutting-edge concepts needed to design transformer-based models for any application in any modality.</p> <p>This blog post will be in two parts:</p> <p><strong>Part 1 will be a deep dive of the standard Transformer architecture.</strong> It is a highly modular architecture, so we will explain each component in detail and how they integrate. This will also cover how to design the components for different use cases. It was introduced by the famous paper <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. There is no shortage of resources to learn about transformers, but I hope to offer some new perspectives.</p> <p><strong>Part 2 will cover recent advancements that have further advanced the capabilities of transformers.</strong> The original transformer architecture is robust and versatile and has led to many successful applications. However, in recent years with the surge in investment into transformers / LLMs, we have seen many useful advances. These impart new capabilities such as longer context length, faster training, and more efficient inference. This is a guide to designing modern transformer architectures for any use case.</p> <h1 id="transformer-architecture">Transformer Architecture</h1> <p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> introduced the transformer architecture in 2017 specifically for machine translation. Since then, architectures derived from this have been used not only for various NLP tasks but also for other modalities such as vision, audio, and time series. We’ll take a modality-agnostic approach. As we explore each component, we’ll focus on how to design it for different modalities and use cases. For instance, position embeddings might be designed differently for text than for images.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/transformer-480.webp 480w,/assets/img/blog/transformer_pt1/transformer-800.webp 800w,/assets/img/blog/transformer_pt1/transformer-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/transformer.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="Transformer diagram" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Annotated from source: <a href="https://arxiv.org/abs/1706.03762"> Source </a> </figcaption> </figure> <p>Transformers are so generalizable because they have relatively few inductive biases. Unlike CNNs, which require Euclidean input, Transformers do not enforce a data structure. It is up to the designer to incorporate domain-specific inductive biases into the model. These design decisions are important for the model to be effective at a given task.</p> <hr/> <p>The transformer architecture can be understood as 3 steps.</p> <ol> <li>Input processing (Generation of a set of embeddings to input into the transformer)</li> <li>Transformer blocks (Bulk of the computation)</li> <li>Output processing (Using the output embeddings of the transformer to perform a task and train the model)</li> </ol> <h1 id="input-processing">Input Processing</h1> <p>The input to the transformer is an unordered set of embeddings. These embeddings are high dimension vectors of floating point values that represent a part of the input. We refer to input processing as the steps taken to compute these embeddings. Input processing changes the most between modalities.</p> <p>The general pattern for input processing is as follows</p> <ol> <li>Split up the input into pieces</li> <li>Map each piece to an embedding</li> </ol> <p>The output of these two steps is a set of embeddings that represent the raw input in a way the transformer architecture can process.</p> <h2 id="image-processing">Image Processing</h2> <p>Transformers for images were introduced in the <a href="https://arxiv.org/abs/2010.11929">ViT</a> paper. The input image is processed in two steps:</p> <ol> <li>The image is split into patches of 16x16.</li> <li>The pixel values of each patch are flattened to vector and fed through a learned linear projection, resulting in patch embeddings.</li> </ol> <p>The result is a set of embeddings that can be processed by the transformer.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/vit_input-480.webp 480w,/assets/img/blog/transformer_pt1/vit_input-800.webp 800w,/assets/img/blog/transformer_pt1/vit_input-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/vit_input.png" width="100%" height="auto" alt="Image Tokenization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> </figcaption> </figure> <h2 id="text-tokenizer">Text Tokenizer</h2> <p>Text is represented as a sequence of characters or bytes. Unlike images, text isn’t inherently numerical data that can be directly transformed into embeddings. Text is processed by tokenization, which is mapping it to a sequence of discrete tokens. Tokenizers create a vocabulary, which is mapping of all possible tokens to vocab indices. These indices are used to retrieve a learned embedding from a table. Text input processing involves two steps: tokenization and embedding lookup.</p> <p>Let’s consider two basic options for tokenization:</p> <ul> <li><strong>Character-Level Tokenization:</strong> Every character in the text becomes a separate token. This creates a very long sequence but has a very small vocabulary.</li> <li><strong>Word-Level Tokenization:</strong> Each word is a distinct token. This results in a more manageable sequence length but results in a huge vocabulary.</li> </ul> <p>There is an obvious tradeoff between vocabulary size and input size. Character-level tokenization creates very long sequences, which can be inefficient for transformers. On the other hand, word-level tokenization can lead to a massive vocabulary size, requiring a large embedding table. This can be computationally expensive and struggle with unseen words or typos.</p> <p>The ideal approach considers several factors:</p> <ul> <li><strong>Sequence Length:</strong> Shorter sequences are generally more efficient for processing, but extremely short sequences may not capture enough context.</li> <li><strong>Embedding Table Size:</strong> A larger vocabulary requires a bigger embedding table, increasing memory usage and training time.</li> <li><strong>Rare Words:</strong> Very infrequent tokens may not be adequately learned during training, impacting model performance.</li> <li><strong>Token Complexity:</strong> A single token shouldn’t represent too much information. Complex concepts might benefit from being broken down into smaller tokens for better processing by the model.</li> </ul> <p>In practice, finding the right balance often involves a compromise between character-level and word-level tokenization. Techniques like subword tokenization (splitting words into smaller meaningful units) can offer a middle ground, achieving a balance between sequence length, vocabulary size, and capturing text information effectively.</p> <h3 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h3> <p>The most common approach to implementing sub-word tokenization is Byte Pair Encoding (BPE).</p> <p>It works by first starting with the individual characters (bytes) in the text as its initial vocabulary. This ensures that all text can be encoded, though not efficiently. BPE then iteratively identifies the most frequently occurring pair of characters and merges them into a single new token. This process continues until a predefined maximum number of tokens is reached, preventing the vocabulary from becoming too large.</p> <p>One interesting feature of this approach is that the tokenizer uses a small and separate dataset for BPE. This dataset can be engineered to achieve certain properties in the tokenizer. For example, it is beneficial for this data to be balanced between different languages. For example, if the amount of data for Japanese is significantly lower than that for English. Rare pairs in English would be prioritized over common pairs in Japanese. This would be unfair to Japanese, and Japanese text would require far more tokens. To address this, the tokenizer dataset can be balanced between different languages.</p> <p>See <a href="https://platform.openai.com/tokenizer">platform.openai.com/tokenizer</a> for an interactive demo on how text is tokenized and mapped in token indices.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/text_tokenization-480.webp 480w,/assets/img/blog/transformer_pt1/text_tokenization-800.webp 800w,/assets/img/blog/transformer_pt1/text_tokenization-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/text_tokenization.png" width="100%" height="auto" alt="Example of tokenized text." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Example of tokenized text <a href="https://platform.openai.com/tokenizer"> Source </a> </figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/token_indices-480.webp 480w,/assets/img/blog/transformer_pt1/token_indices-800.webp 800w,/assets/img/blog/transformer_pt1/token_indices-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/token_indices.png" width="100%" height="auto" alt="Generated token indices" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Generated token indices <a href="https://platform.openai.com/tokenizer"> Source </a> </figcaption> </figure> <p>BPE also involves some hardcoded rules. Some bytes, such as punctuation can be ignored in the tokenizer merging. GPT tokenizers use different regex patterns to split the string prior to tokenization to prevent certain bytes from merging.</p> <p>BPE is expensive to run and to encode/decode text since it is an iterative process. This is optimized by OpenAI (<a href="https://github.com/openai/tiktoken">tiktoken</a>) by implementing it in Rust. <a href="https://github.com/google/sentencepiece">SentencePiece</a> by Google is another popular tokenizer. SentencePiece runs BPE on Unicode code points (Unicode characters). It falls back to bytes for rare code points. Unicode has nearly 150k code points, a large number of which are very rare. Most tokenizers use less than 100k tokens. Having 150k tokens before adding more through BPE is not practical.</p> <p>Once we have a trained tokenizer, we use it to map input text to token indices. These token indices are mapped to learned embeddings. Transformer models often include embedding tables, which store learned embeddings for each item in the model’s vocabulary.</p> <p>See this <a href="https://www.youtube.com/watch?v=zduSFxRajkE&amp;t=24s&amp;ab_channel=AndrejKarpathy">video</a> by Andrej Karpathy for a deep dive into text tokenizers.</p> <h2 id="audio-and-other-modalities">Audio and Other Modalities</h2> <p>Like images, audio is a continuous data modality. A popular method of tokenizing audio is to generate a spectrogram using a Fourier Transform. This creates an image that can be tokenized in the same way as images in ViT. The <a href="https://arxiv.org/abs/2104.01778">AST: Audio Spectrogram Transformer</a> paper does exactly this.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/audio_input-480.webp 480w,/assets/img/blog/transformer_pt1/audio_input-800.webp 800w,/assets/img/blog/transformer_pt1/audio_input-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/audio_input.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="Audio tokenization from AST" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Audio tokenization from AST <a href="https://arxiv.org/abs/2104.01778"> Source </a> </figcaption> </figure> <p>This paper uses a 2D position embedding so they can warmstart from a ViT model. If it were to train from audio only, a 1D position embedding could be used, as in OpenAI’s <a href="https://arxiv.org/abs/2212.04356">Whisper</a>.</p> <p>We have covered the basic methods for tokenizing text and continuous data domains, however, there is a lot of research covering alternative methods. This includes vector quantization which generates discrete tokens from continuous data modalities.</p> <h1 id="position-embedding">Position Embedding</h1> <p>The transformer takes a set of tokens as input. However, many inputs are better represented as sequences, such as text and audio Where a token occurs in the sequence is a crucial piece of information. Position embeddings can be added to the token embeddings to encode the position of the token in the sequence. Although the input is still a set, we are not losing the information of the order of the tokens within the input sequence. Position embeddings implicitly turn the transformer from a set processing architecture to a sequence processing one.</p> <p>The original transformer paper evaluates two methods of configuring the position embedding. These have equivalent results.</p> <ul> <li>Learned: A separate learned embedding is used for every position in the sequence up to the maximum sequence length.</li> <li>Fixed: The embedding values aren’t learned but are configured as a function of the position. \(i\) is the index in the embedding. \(d_{model}\) values have to be generated so the position embedding can be the same dimension as the token embedding.</li> </ul> \[PE_{(pos,2i)} = \sin\left(pos/10000^{2i/d_{model}}\right) \\ PE_{(pos,2i+1)} = \cos\left(pos/10000^{2i/d_{model}}\right)\] <p>These embeddings are added to the input token embeddings. This assumes that the addition of the positional encoding doesn’t cause conflicts in the embedding space (which is typically in a high dimension). However, it is also possible to concatenate position embedding values.</p> <p>For language, 1D position encodings are used. These embeddings should be designed to fit the data. For example, in images, the position embedding is 2 dimensional. For videos, an additional time dimension can be added. The embedding can be designed in any way to encode the structure of the data.</p> <h1 id="transformer-blocks">Transformer Blocks</h1> <p>The transformer blocks are where the bulk of the computation takes place. We will first go through the components needed to build these blocks, and then put them together.</p> <h2 id="attention">Attention</h2> <p>The core component of the Transformer architecture, as highlighted in the title “Attention Is All You Need,” is Attention. This concept predates transformers in Natural Language Processing (NLP). The fundamental equation for attention is:</p> \[\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V\] <p>Attention comes in two forms: self-attention and cross-attention. We’ll begin with self-attention. It can be conceptualized as a set-to-set mapping of embeddings where information is shared among all embeddings. Here’s how it works:</p> <p>\(Q\), \(K\), and \(V\) represent queries, keys, and values. These represent different linear projections of the input embeddings that are used in the attention operation. You can think of attention as tokens communicating information with each other. Each token’s query determines which other tokens it wants to read from, while its key determines which tokens will read from it. When a token’s query matches well with another token’s key, it receives more of that token’s value. The value represents the information that the token shares with others . Through these learned projections, the model determines how information flows between tokens.</p> <p>Attention is implemented by first generating these three matrices. Let’s say the input embeddings are stored in a matrix \(X\). Learned weight matrices \(W^Q\) , \(W^K\), and \(W^V\). Are used to project the input embeddings: \(Q = W^QX, K = W^KX, V = W^VX\).</p> <p>At this stage, no information has been transferred between tokens. Given the variable number of tokens, we can’t use a single large MLP layer. To mix the information, we set each embedding to be a weighted sum of all value embeddings.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/scaled_attention-480.webp 480w,/assets/img/blog/transformer_pt1/scaled_attention-800.webp 800w,/assets/img/blog/transformer_pt1/scaled_attention-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/scaled_attention.png" class="image-fluid mx-auto d-block" width="200" height="auto" alt="Scaled Dot-Product Attention" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Scaled Dot-Product Attention <a href="https://arxiv.org/abs/1706.03762"> Source </a> </figcaption> </figure> <p>To compute this weighted sum, we first compute the attention matrix \(QK^T\). This matrix is of shape \((N, N)\). This is the source of the \(N^2\) complexity of transformers. The attention matrix contains scores for every combination of token query and key embeddings: \(q*k\), which is scaled by a factor \(\frac{1}{\sqrt{d_k}}\). This scaling is applied to normalize the gradients, such that the magnitude of the dot product isn’t dependent on the embedding dimension. This specific attention formulation is called scaled dot-product attention.</p> <p>A softmax is applied to each row or column, creating a weight vector for each token. This is multiplied by the value matrix to generate a weighted sum for each token. Because all matrices are learned, each token can determine which tokens to attend to, which tokens should attend to it, and what information to broadcast. This function is highly flexible. A token could even learn to nullify its own information and instead read from other tokens.</p> <p>Self-attention is like a fully connected neural network layer in that information from all tokens can propagate to other tokens. However, self-attention has the benefit of supporting variable length input.</p> <h2 id="cross-attention">Cross-Attention</h2> <p>Self-attention is a mechanism where queries, keys, and values all derive from the same set of input embeddings. In contrast, cross-attention operates on two distinct sets of embeddings, which can have different lengths. The queries come from one set, while the keys and values come from another.</p> <p>In cross-attention, the attention matrix \(QK^T\) is of shape \((N_Q, N_K)\) . \(N_Q\) is the sequence length of the queries, and \(N_K\) is the sequence length of keys. The softmax is taken on the rows, so each query token has a probability distribution with respect to keys.</p> <p>Cross-attention is particularly relevant for machine translation. In this context, the keys and values come from the source language text, while the queries come from the target language. As the model generates the translation, it attends to the set of tokens from the source language, allowing it to draw information from the original text throughout the translation process.</p> <h2 id="masked-self-attention">Masked Self-Attention</h2> <p>The transformer decoder uses causal masking. The decoder is trained to predict the next token. This task becomes trivial if the next token and all future tokens are visible, as in full self-attention. Causal masking constrains the attention operation to only look at tokens to the left, making the decoder auto-regressive (meaning each output depends only on previous outputs). This one-way flow of information is essential for generating sequences one token at a time.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/masked_attention-480.webp 480w,/assets/img/blog/transformer_pt1/masked_attention-800.webp 800w,/assets/img/blog/transformer_pt1/masked_attention-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/masked_attention.png" width="100%" height="auto" alt="Example of a masked attention matrix" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Example of a masked attention matrix </figcaption> </figure> <p>Masking is applied on the \(QK^T\) matrix. Masked indices are set to \(-\infty\), this causes the softmax function to assign zero weight to these tokens. In many implementations of attention, the mask can be customized by passing in a Boolean matrix.</p> <h2 id="multi-head-attention">Multi-Head Attention</h2> <p>Multi-head attention (MHA) is a way to increase the expressivity of the attention operator. It is essentially running multiple attention operations in parallel and concatenating the output. This improves expressivity because each head is free to attend to different tokens.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/mha-480.webp 480w,/assets/img/blog/transformer_pt1/mha-800.webp 800w,/assets/img/blog/transformer_pt1/mha-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/mha.png" class="image-fluid mx-auto d-block" width="300" height="auto" alt="Multi-Head Attention" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Multi-Head Attention <a href="https://arxiv.org/abs/1706.03762"> Source </a> </figcaption> </figure> <p>Multi-head attention has two scaling parameters. Feature dimension for each head \(d_v\), and number of heads \(h\).</p> <p>Each head projects the input embeddings into queries, keys, and values of size \(d_v\). This means that the weight matrices \(W^Q\) , \(W^K\), and \(W^V\) are of size \((d_{model}, d_v)\). Attention is applied to the set of queries, keys, and values independently. This results in \(h\) sets of output embeddings of size \(d_v\). The output embeddings of each head are concatenated resulting in embeddings of size \(d_v*h\). The output needs to be the same dimension as the input, so there is linear projection back to size \(d_{model}\).</p> <p>Typically the embedding dimension to each head is \(d_v = d_{model}/h\). In this case, the concatenated output is the same dimension as the input token embeddings. However, it is possible to set \(d_v\) to be higher or lower.</p> <p>The output projection, which is a linear layer \(W^O\), learns to combine the outputs of the different heads. The output size of this layer is the same as the input token embedding size. This layer allows the model to give different importance to different attention heads. When \(d_v\) is set to \(d_{model}/h\), this projection is not required for dimensionality matching. However, it is beneficial in that the information from different heads can be mixed before the residual connection.</p> <p>Multi-head attention (MHA) effectively divides the softmax operation into separate parts. Each head has a fixed amount of attention weight to distribute among different value functions. This multi-headed approach allows for more complex token interactions. One of the advantages of the attention mechanism is its interpretability. For each head, it’s possible to examine which tokens are attending to which other tokens, providing insight into the model’s internal workings.</p> <h2 id="normalization">Normalization</h2> <p>In transformer architectures, layer normalization is typically used. Unlike batch normalization, the values are independent of other items in the batch. This is because batch-wide statistics aren’t used. Layer Normalization was <a href="https://arxiv.org/abs/1607.06450">introduced</a> just a year prior to transformers.</p> <p>For each set \(x\) in the batch, the mean \(\mu(x)\) and variance \(\sigma(x)^2\) of the embedding values (across all \(d_{model}\) values of each embedding) are calculated. LayerNorm operates on each embedding in the input completely independently. These values are used to normalize each embedding value:</p> \[\mathrm{LN}(x) = \frac{x-\mu(x)}{\sqrt{\sigma(x)^2+\epsilon}} *\gamma +\beta\] <p>\(\gamma\) and \(\beta\) are learned scalar parameters. \(\epsilon\) is a small constant used for numerical stability.</p> <p>Layer norm is effective for multiple reasons. Since batch statistics aren’t used, it makes data parallelism more efficient. This is because the batch statistics do not have to be communicated between GPUs. LayerNorm is also not affected by the size of the batch, which means different batch sizes can be used at different times.</p> <p>In <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>, the layer normalization occurs after each attention and feed-forward layer (Post-LN architecture). However, it is now more popular to put the layer normalization before these layers (Pre-LN architecture). This <a href="https://arxiv.org/abs/2002.04745">paper</a> from 2020 shows that the Pre-LN architecture generally performs better. This is the only fundamental change to the original transformer architecture.</p> <h2 id="feed-forward">Feed-Forward</h2> <p>After each attention layer, a small feed forward neural network processes each token embedding. This is a position-wise operation. In the original paper, this is a two layer network with a ReLU activation after the first layer. The first layer outputs a dimension \(d_{ff}=2048\). The second layer projects these embeddings back to the original token embedding dimension \(d_{model}=512\). The first layer is set to the 4x the size of the token embedding. This multiplier is arbitrary but is considered to be an effective value given the efficiency tradeoff.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/feed_forward-480.webp 480w,/assets/img/blog/transformer_pt1/feed_forward-800.webp 800w,/assets/img/blog/transformer_pt1/feed_forward-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/feed_forward.png" class="image-fluid mx-auto d-block" width="200" height="auto" alt="Feed Forward Layer" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Feed Forward Layer </figcaption> </figure> <p>The attention layer has \(4*d_{model}*d_{model}\) parameters (accounting for query, key, value, and output projection matrices), which is \(1.0*10^6\) for the default embedding size. The feed forward layer has \(d_{model}*d_{ff} + d_{ff}*d_{model}\) which is over \(2.1*10^6\) parameters with the default configuration. When \(d_{ff} = 4*d_{model}\), this is equivalent to \(8*d_{model}*d_{model}\). The feed forward layer has roughly twice the number of parameters.</p> <p>The attention layer computation scales quadratically with input length (default value is \(n=1024\)). The computational complexity of the feed forward layers is \(n*d_{model}*d_{ff}\). For attention, it is \(n^2d_{model}+d_{model}*d_{model}\). A recent trend is increasing the sequence length \(n\), which causes the attention layer to further dominate the computational cost. Due to the complexity of the attention operation and the different ways to implement it on hardware, we will skip calculating numerical values of the computation.</p> <p>The feed forward layers contain the bulk of the transformer’s parameters, while the attention layers have the bulk of the computation. Attention is meant to learn the relationships between tokens, while the feed forward layers are meant to learn the individual token representations themselves. The attention operation is computationally intense in modeling the relationships between tokens, but it does not process individual token embeddings as much. The feed forward layers complement attention by enabling complex transformations of these embeddings.</p> <p>This <a href="https://arxiv.org/abs/2012.14913">paper</a> by Geva et al. argues that the feed forward layers act as key value memories. The high parameter counts of these layers enable the model to store rich information about the data they are trained on. Models like GPT-4 may not have their impressive world knowledge without the storage capacity of the feed forward layer. The transformer is a powerful architecture due to its balance of computational complexity and high parameterization.</p> <h2 id="blocks">Blocks</h2> <p>Now that we have covered each component, we can describe the transformer blocks. There are three main types of transformer blocks: encoder, decoder with cross-attention, and decoder without cross-attention. These blocks can be repeated any number of times.</p> <h3 id="encoder-block">Encoder Block</h3> <p>The encoder block maps a set of embeddings to another set of embeddings. It uses full self-attention, so each token can attend to all other tokens.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/encoder_block-480.webp 480w,/assets/img/blog/transformer_pt1/encoder_block-800.webp 800w,/assets/img/blog/transformer_pt1/encoder_block-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/encoder_block.png" class="image-fluid mx-auto d-block" width="200" height="auto" alt="Encoder Block" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Encoder Block </figcaption> </figure> <h2 id="decoder-block">Decoder Block</h2> <p>The decoder block takes in a set of input embeddings but also attends to a set of embeddings from the encoder. The first attention layer processes input embeddings with causal attention. The second attention layer is cross-attention, where the keys and values come from the encoder output. This kind of block is only used in encoder-decoder architectures since it relies on the encoder output embeddings.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/decoder_cross_attention_block-480.webp 480w,/assets/img/blog/transformer_pt1/decoder_cross_attention_block-800.webp 800w,/assets/img/blog/transformer_pt1/decoder_cross_attention_block-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/decoder_cross_attention_block.png" class="image-fluid mx-auto d-block" width="300" height="auto" alt="Decoder block with cross-attention" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Decoder block with cross-attention </figcaption> </figure> <p>The cross-attention block is omitted in decoder-only transformers. This is because there are no encoder tokens to attend to. This block is identical to the encoder block, but the attention is masked.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/decoder_block-480.webp 480w,/assets/img/blog/transformer_pt1/decoder_block-800.webp 800w,/assets/img/blog/transformer_pt1/decoder_block-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/decoder_block.png" class="image-fluid mx-auto d-block" width="200" height="auto" alt="Decoder block without cross-attention" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Decoder block without cross-attention </figcaption> </figure> <h2 id="encoder-only-decoder-only-and-encoder-decoder-architectures">Encoder-Only, Decoder-Only, and Encoder-Decoder Architectures</h2> <p>The original transformer paper introduced an encoder-decoder architecture. Since then, encoder-only and decoder-only architectures have gained significant popularity for various use cases. You can even have “encoder-heavy” or “decoder-heavy” architectures where one part of the transformer has more layers than the other. Let’s explore the different types of transformer models and their applications.</p> <h3 id="encoder-decoder">Encoder-Decoder</h3> <p>The encoder-decoder architecture can be viewed as two interconnected transformers. An encoder, which is a stack of encoder blocks, maps a sequence of input embeddings to output embeddings. A stack of decoder blocks with cross-attention then processes these embeddings. The decoder blocks read the final output embeddings from the encoder in the cross-attention layer.</p> <p>The encoder and decoder can process different types of data. For instance, in speech recognition, the encoder might encode audio while the decoder translates it into text.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/whisper-480.webp 480w,/assets/img/blog/transformer_pt1/whisper-800.webp 800w,/assets/img/blog/transformer_pt1/whisper-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/whisper.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="OpenAI Whisper Encoder-Decoder Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> OpenAI Whisper Encoder-Decoder Architecture <a href="https://cdn.openai.com/papers/whisper.pdf"> Source </a> </figcaption> </figure> <p>This architecture employs cross-attention, whereas encoder-only and decoder-only architectures rely solely on self-attention.</p> <h3 id="encoder-only">Encoder-Only</h3> <p>Encoder-only transformers, popularized by <a href="https://arxiv.org/abs/1810.04805">BERT</a>, perform a one-to-one mapping of input embeddings to output embeddings. They can’t perform sequence-to-sequence modeling unless the input and output sequences have identical lengths.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/vit-480.webp 480w,/assets/img/blog/transformer_pt1/vit-800.webp 800w,/assets/img/blog/transformer_pt1/vit-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/vit.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="ViT Encoder-Only Architecture " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> ViT Encoder-Only Architecture <a href="https://arxiv.org/abs/2010.11929"> Source </a> </figcaption> </figure> <p>These models excel at scalar prediction tasks, such as classification or regression, where the output is a single value rather than a set or sequence. Text sentiment analysis and ImageNet classification are prime examples of their application.</p> <p>Encoder-only models are useful in tasks reducible to token classification. For instance, <a href="https://arxiv.org/abs/1810.04805">BERT</a>’s evaluation on the Stanford Question Answering Dataset (SQuAD) doesn’t generate text answers but identifies the relevant span in the input text. The task becomes classifying which tokens mark the start and end of the answer span. Similarly, Vision Transformer (ViT), another encoder-only architecture, is trained for ImageNet classification.</p> <h3 id="decoder-only">Decoder-Only</h3> <p>Decoder-only transformers have become the go-to architecture for Large Language Models (LLMs), popularized by OpenAI’s <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT</a> models.</p> <p>A decoder-only model can tackle any task an encoder-decoder can handle. Instead of processing source data through a separate encoder, all data flows through the decoder. The decoder omits the cross-attention layer since there’s no encoder to attend to. Its architecture mirrors that of the encoder-only transformer, with the key difference being causal attention.</p> <p>The encoder-decoder architecture can be viewed as a constrained version of the decoder-only architecture. The separate encoding of source data in encoder-decoder models represents a form of inductive bias that decoder-only architectures generalize away from.</p> <p>Encoder-decoder models require paired source and target text sequences for training, which can limit their flexibility. In contrast, decoder-only models can process a single input sequence, making them more versatile and adaptable to various tasks.</p> <p>Decoder-only architectures have surpassed encoder-decoder models in popularity due to their simplicity and versatility. However, encoder-decoder models still offer unique advantages, such as the ability to train on encoder-specific objectives or fine-tune the encoder for downstream tasks.</p> <h1 id="output-processing">Output Processing</h1> <p>The architecture of the output processing is simple. There is a final linear layer that maps embeddings of \(d_{model}\) to the size of the prediction. The output of the linear layer and how it is applied depends on the task the model is trained on.</p> <p>Transformer models can be trained with different objectives and losses based on the use case and architecture type. We will discuss different types of objectives and how they are used for training. We will also explain how inference works under these objectives.</p> <h2 id="next-token-prediction">Next Token Prediction</h2> <p>Encoder-Decoder and Decoder-only transformers are primarily trained on next token prediction. This task involves predicting the subsequent word in a sequence based on the preceding words. The output layer is applied to each embedding, with the output size matching the input vocabulary size. A softmax function then creates a probability distribution over the token vocabulary, from which the next token is sampled.</p> <p>During training, the decoder learns to predict the next word given the context. Causal masking ensures that for each token, the model can’t see the next or subsequent tokens in the sequence.</p> <p>For each token, the ground truth preceding tokens are used as context. This is known as teacher forcing, as the generated tokens aren’t used as context. However, at inference time, autoregressive decoding is used instead. We start with a context which is the set of input tokens. For chatbots, this might be the user’s input; for translation, the source text. The model predicts the next token, which is then added to the context, and another token is sampled. This process iterates until a special <END> token is produced or a predetermined limit is reached.</END></p> <p>At each step, the model outputs a probability distribution for the next token. This allows for sampling random sequences with a tunable temperature parameter, a common input for LLM APIs. Top-k sampling is another technique where you only consider the top-k tokens after the softmax. The probabilities are renormalized before sampling again. This prevents low probability tokens from ever getting predicted. Top-p or nucleus sampling is similar but possible tokens are selected so that their probabilities do not exceed p. This is more robust to changing entropies / confidence in the model’s predictions. These parameters are tuned to strike a balance between creativity and quality.</p> <p>The paper <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> and many other NLP applications use beam search for text generation. <a href="https://en.wikipedia.org/wiki/Beam_search">Beam search</a> is a search algorithm widely used in text decoding. <a href="https://thinking-teams.com/wp-content/uploads/2020/11/nmt.pdf">Wu et al. 2016</a> provides insights into how it is used for transformer decoding. With beam search, instead of greedily selecting the most probable next token, beam search maintains the top \(k\) most likely sequences, where \(k\) is the beam width. At each step, the model expands these \(k\) sequences with their most probable next tokens. To prevent the number of sequences from growing exponentially (\(k^n\)), only the top \(k\) paths are retained after each step. The path probability is calculated as the sum of negative log likelihoods, normalized by dividing by \(length^{\alpha}\). This method allows the model to explore multiple promising paths simultaneously, avoiding local optima that might occur from selecting single tokens. To manage computational costs and control output length, beam search requires constraints. The beam size determines the number of computed paths and token selections per step, and the length penalty \(\alpha\) exponentially penalizes longer sequences. <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> implements this with a beam size of 4 and an \(\alpha\) value of 0.6.</p> <p>Next token prediction training is highly parallelizable due to teacher forcing. A single forward pass generates predictions and losses for each token in the input. However, inference is an iterative process requiring a forward pass for each generated token.</p> <h2 id="masked-language-modeling">Masked Language Modeling</h2> <p>Masked Language Modeling (MLM) is a training objective introduced by <a href="https://arxiv.org/abs/1810.04805">BERT</a>. This method trains encoder-only transformers. MLM training follows these steps:</p> <ol> <li>Randomly select 15% of the input tokens for potential masking.</li> <li>Of these selected tokens: <ul> <li>• 80% are replaced with a special [MASK] token</li> <li>• 10% are replaced with a random token</li> <li>• 10% are left unchanged</li> </ul> </li> <li>Apply the linear output layer to all masked predictions. Use cross-entropy loss to predict the correct token, regardless of how it was masked.</li> </ol> <p>The intuition behind this technique differs fundamentally from next token prediction in that it’s bidirectional. As it’s an encoder-only architecture with full self-attention, tokens to the left and right are used to predict the masked tokens accurately.</p> <p>MLM is a pretraining method that doesn’t yield directly interpretable output. The model can be used for embedding representations, where the output embedding is aggregated and used in another application. The model can also be fine-tuned on a scalar prediction task.</p> <h2 id="scalar-predictions">Scalar Predictions</h2> <p>Encoder-only transformers support a wide variety of losses in addition to MLM. While MLM is a per-token objective where outputs are generated from multiple tokens, many objectives require using an output linear layer on a singular embedding to get a single output.</p> <p>There are multiple ways to achieve this:</p> <ul> <li>• Special output token <ul> <li>BERT and ViT add a <CLS> token to the input. The output embedding from this input is used for predictions.</CLS></li> </ul> </li> <li>• Output pooling <ul> <li>Alternatively, you can take the average of all the embeddings and apply the output layer on this pooled embedding.</li> </ul> </li> <li>• Attentive probing <ul> <li>Attention can process the output. A learnable query vector attends to all token embeddings, producing a weighted sum that is then used for the output layer. This is essentially a cross-attention with a fixed number of query embeddings.</li> </ul> </li> </ul> <p>Once you have a singular output embedding, it can be processed by the output linear layer, and then any loss function relevant to the objective.</p> <h1 id="conclusion">Conclusion</h1> <p>This blog post covered the basic components of early transformers. In part 2, we will cover more recent innovations that further optimize these models and enable new capabilities.</p>]]></content><author><name></name></author><category term="transformer"/><summary type="html"><![CDATA[The Transformer architecture has emerged as the cornerstone of deep learning and artificial intelligence. Despite its conceptual simplicity, the specific details of the architecture can be difficult to understand and reason about. This two-part blog series aims to provide a thorough examination of the Transformer, demystifying its core components and recent advancements. The goal is to cover the fundamental and cutting-edge concepts needed to design transformer-based models for any application in any modality.]]></summary></entry><entry><title type="html">Self-Supervision from Videos</title><link href="https://rohitbandaru.github.io/blog/Self-Supervision-from-Videos/" rel="alternate" type="text/html" title="Self-Supervision from Videos"/><published>2024-10-06T00:00:00+00:00</published><updated>2024-10-06T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Self-Supervision-from-Videos</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Self-Supervision-from-Videos/"><![CDATA[<p>In a previous <a href="https://rohitbandaru.github.io/blog/Self-Supervised-Learning/">blog post</a>, we explored image-based self-supervised learning primarily with contrastive learning. Self-supervised learning offers a way to train an ML model to generate useful image representations using large amounts of unlabeled data. The resulting models can be used for a wide variety of downstream tasks, such as classification, object detection, or segmentation on different datasets.</p> <p>Many of the current state-of-the-art results come from contrastive self-supervised learning. In contrastive SSL, an image from the dataset is transformed using multiple data augmentations, including cropping, color distortion, and flipping. These augmentations are inputted into a neural network to get representations. A contrastive loss is applied to push representations from the same image (different augmentations) closer together, and representations from different images are pushed further apart. Training with this objective will learn image representations that encode the content of the image. The data augmentations are needed for the network to avoid trivial solutions to optimize the contrastive loss, which would be comparing pixel values instead of the image’s semantic content.</p> <p>Contrastive learning is limited by its dependence on data augmentations. These augmentations are hacky and unnatural. They can change the meaning of the image. The act of augmenting an image destroys some amount of information.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/contrastive_ssl-480.webp 480w,/assets/img/blog/video-ssl/contrastive_ssl-800.webp 800w,/assets/img/blog/video-ssl/contrastive_ssl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/contrastive_ssl.png" width="500" height="auto" alt="Contrastive SSL" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/Purushwalkam-480.webp 480w,/assets/img/blog/video-ssl/Purushwalkam-800.webp 800w,/assets/img/blog/video-ssl/Purushwalkam-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/Purushwalkam.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Example showing that cropping can remove semantic information in images. <a href="https://arxiv.org/abs/2007.13916"> Source </a> </figcaption> </figure> <p>Example from <a href="https://arxiv.org/abs/2007.13916">Purushwalkam et al</a> showing that cropping can remove semantic information in images.</p> <p>Many AI researchers believe that humans learn through self-supervised learning. However, humans most likely don’t learn with data augmentations. It is unlikely we mentally do crops and color distortions. It is more likely we learn by tracking objects through time. For example, if you are watching a dog, the dog looks different from one point in time to another. The dog is likely in a different pose, different location, and has different lighting conditions. Through time, we can get <em>natural data augmentations</em>, but can they help train better image representation models?</p> <p>The blog post explores whether video can improve self supervised computer vision models. We look at some papers with different approaches to the problem.</p> <h1 id="image-vs-video-representations">Image vs Video Representations</h1> <p>Image and video representations are two distinct research problems. Image representation learning aims to learn fixed-size embeddings of images, while video representation learning aims to learn fixed-size embeddings of videos. There are many papers on applying contrastive learning techniques to videos; however, these involve applying data augmentations to videos. In this post, we will also discuss using videos to learn image representations so that we can utilize the concept of obtaining natural data augmentations from videos.</p> <h1 id="learning-video-representations">Learning Video Representations</h1> <p>Video representation learning can be understood through its downstream tasks. One of the most common datasets is <a href="https://www.deepmind.com/open-source/kinetics">Kinetics</a>. Which consists of short clips of human actions which are to be classified into classes such as “saluting” and “cutting cake”.</p> <h2 id="spatiotemporal-contrastive-video-representation-learning"><a href="https://arxiv.org/abs/2008.03800">Spatiotemporal Contrastive Video Representation Learning</a></h2> <p>This work is very similar to <a href="https://rohitbandaru.github.io/blog/Self-Supervised-Learning/#simclr">SimCLR</a>. It uses contrastive learning to learn representations of videos. Like SimCLR, InfoNCE loss is used to bring closer the representations of positive pairs and repel those of negative pairs. Positive pairs are data augmentations of the same video. There are two main differences with SimCLR:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/cvrl-480.webp 480w,/assets/img/blog/video-ssl/cvrl-800.webp 800w,/assets/img/blog/video-ssl/cvrl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/cvrl.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2008.03800"> Source </a> </figcaption> </figure> <h3 id="1-3d-cnn-model-architecture-for-the-video-encoder-to-produce-representations">1) 3D CNN model architecture for the video encoder to produce representations</h3> <p>3D-ResNets are used instead of 2D-ResNets.</p> <p>This is a natural extension of CNNs to handle video that treats time as a third dimension. This architecture obtains a fixed-size representation from a fixed number of frames of a certain resolution.</p> <h3 id="2-spatiotemporal-data-augmentations">2) Spatiotemporal data augmentations</h3> <p>In addition to the standard image data augmentations used in other contrastive SSL methods (color jittering, cropping, etc.), the authors add spatial and temporal augmentations. These are designed specifically for videos. It is important to carefully design the data augmentations used for videos. If the augmentations are too aggressive, the representations will be invariant to useful information. If the augmentations are too weak, the representations may learn trivial solutions.</p> <p><strong>Temporal Augmentations</strong></p> <p>The temporal interval is sampled first. This interval is the time difference between the two augmentations. Smaller intervals have more probability, which is desired because temporally distant augmentations might be too far apart. We want the content of the two augmentations to be the same. The further apart in the video they are, the less likely this is.</p> <p><strong>Spatial Augmentations</strong></p> <p>Applying spatial augmentations to each frame independently has the disadvantage of destroying motion between frames. If each frame is randomly cropped, the objects will randomly move around frame to frame. We want to preserve the consistency between frames. The solution to this is to sample a frame-level augmentation once per video clip and apply the same transformation to each frame. If each frame is cropped to exactly the same pixels, the motion will be perfectly preserved. For contrastive learning, we use data augmentations to make negative examples more different. We want two video clips to be different, but there is no need to make the frames of a single clip different from each other.</p> <p>This paper is a great example of how image SSL techniques can be extended to videos. It just requires different model architectures and data sampling / augmentation techniques. There are some gaps to this approach:</p> <ol> <li>It can only produce representations for fixed-size video clips. The resulting model can’t be used for downstream image tasks.</li> <li>Temporal data augmentations can still destroy some temporal information in the video, especially fine-grained details.</li> <li>Spatial data augmentations such as cropping are required and can be used to learn unwanted spatial invariances.</li> </ol> <h1 id="learning-image-representations-from-video">Learning Image Representations from Video</h1> <h2 id="self-supervised-learning-of-video-induced-visual-invariances-vivi"><a href="https://arxiv.org/abs/1912.02783">Self-Supervised Learning of Video-Induced Visual Invariances (VIVI)</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/vivi-480.webp 480w,/assets/img/blog/video-ssl/vivi-800.webp 800w,/assets/img/blog/video-ssl/vivi-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/vivi.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1912.02783"> Source </a> </figcaption> </figure> <p>They developed a video-based self-supervised learning framework for image representations and evaluated it on the <a href="https://ai.googleblog.com/2019/11/the-visual-task-adaptation-benchmark.html">VTAB</a> image representation benchmark.</p> <p>This uses the <a href="https://arxiv.org/abs/1609.08675">YouTube 8M</a> dataset. This is a larger dataset than Kinetics and contains longer and more complex videos. Videos are composed of frames. In this work, they also utilize “shots,” which are sequences of continuous frames within a video. Shots have a high-level relationship with each other. Frames within the same shot are used as positive pairs, and shots from different shots are used as negative pairs.</p> <p>Shot embeddings are defined as pooled (mean pooled or attention pooled) frame embeddings. These are used for shot order prediction. An LSTM or MLP is used to predict the next shot embedding given the current shot embedding. This models the relationship between shots in a video. An alternative to shot embedding prediction, would be to contrast shot embeddings between videos.</p> <p>This paper uses co-training with the ImageNet supervised classification task. There is a long way to go since YouTube-8M is much larger than ImageNet, so ImageNet should not be needed for good results. This gap may be due to the data itself. ImageNet is a relatively clean dataset where the object in each image is usually centered. Video frames from YouTube are much noisier.</p> <h2 id="demystifying-contrastive-self-supervised-learning-invariances-augmentations-and-dataset-biases"><a href="https://arxiv.org/abs/2007.13916"><strong>Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases</strong></a></h2> <p>In this paper, the authors observe that the cropping required for contrastive SSL hurts performance on downstream tasks such as object detection. They make a distinction between scene-centric and object-centric datasets. ImageNet is object-centric, which means that a single object is presented in the image and it is centered. The data in video often contains multiple objects in different parts of the frame. For certain tasks, ImageNet is a far superior training dataset due to this bias.</p> <p>They pretrain and evaluate an SSL model (MOCOv2) with the MSCOCO dataset (scene-centric) and MSCOCO bounding box cropped dataset (object-centric). The results show the effect of cropping when pretraining. They find that object-centric pretraining leads to better object-centric evaluation, while scene-centric pretraining leads to better scene-centric evaluation.</p> <p>This shows that for scene-centric evaluation tasks like object detection, cropping while pretraining is harmful.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/video-moco-480.webp 480w,/assets/img/blog/video-ssl/video-moco-800.webp 800w,/assets/img/blog/video-ssl/video-moco-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/video-moco.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2007.13916"> Source </a> </figcaption> </figure> <p>To replace cropping, videos can be used to get “temporal transformations”.</p> \[\mathcal{V}_{pairs} = \{(z_i, z_{i+k})\ |\ z \in \mathcal{V}, i \in \textnormal{N}(z), i \bmod k = 0\}\] <p>Pairs are formed by frames consecutive frames subsampled by $k$. We do not want frames that are too similar too each other in time, as their difference would be too insignificant. $k$ can be set depending on the frame rate of the video. If it is equal to 60, every 60th frame is considered and adjacent frames are pairs.</p> \[\mathcal{D}^+ = \{(t_i(z_i), t_j(z_{i+k}))\ |\ t_i,\ t_j \in T, (z_i, z_{i+\Delta}) \in V_{pairs}\}\] <p>The dataset is then formed by applying transformations to the pairs of frames. For this work, the same transformations as MOCO are used. MOCO doesn’t require negative examples to train. Positive example pairs are formed by applying a transformation on the pairs of frames. Although this work explores using video for natural transformations, it still relies on the traditional data augmentations used in SSL.</p> <p>This contrastive learning setup uses the whole frame, but the authors want to train on object-centric data to make the representations more robust for object recognition.</p> <p>They use region tracking to make the frames of the video object-centric. They track the same object across multiple frames and use these cropped versions of the frames. This way, the model learns how objects change in a video while ignoring the scene-wide changes. For example, the position of an object in a scene is ignored.</p> <h1 id="videomae"><a href="https://arxiv.org/abs/2203.12602">VideoMAE</a></h1> <p>This work extends the <a href="https://arxiv.org/abs/2111.06377">MAE</a> work (<a href="https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers/#masked-autoencoders-are-scalable-vision-learners">SSL with Vision Transformers blog post</a>) to video. Rather than using contrastive learning, masked patches are predicted in the video.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/videomae-480.webp 480w,/assets/img/blog/video-ssl/videomae-800.webp 800w,/assets/img/blog/video-ssl/videomae-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/videomae.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2203.12602"> Source </a> </figcaption> </figure> <p>The video is represented as a 3D image. The video is split into cubes (in this paper, it’s size 2 × 16 × 16). These cubes are treated like patches in ViT, but with an added time dimension. Each cube in the video is linearly projected and treated as a token embedding in the transformer. A position embedding is also added to the cube embedding. These cubes are treated as tokens in the transformer.</p> <p>For each video, a tube mask is applied. This means the same patch in multiple consecutive frames of the video is masked. This is meant to make the SSL task harder, as patches don’t change much frame to frame. There is a high temporal correlation that needs to be broken in designing the objective. A high masking ratio, along with the tube masking strategy, ensures the SSL task is difficult and forces the model to learn higher-level spatiotemporal information.</p> <p>An encoder processes the unmasked tokens. With a high masking ratio, we can save on computation cost by only processing the unmasked tokens. This is feasible with the transformer architecture, but not with CNNs. The encoder maps the token embeddings to a latent space. The decoder then predicts the masked cubes of the video. This can be trained with a simple reconstruction loss like MSE (mean squared error).</p> <p>One interesting architecture decision is that “joint space-time attention” is just full self-attention. This means the attention captures all pairwise interactions between all tokens. It would be interesting to introduce causal attention on the time dimension. This would mean that within a frame, there is full attention. But cubes can only attend to cubes in the future. However, this type of causal masking would likely require a lower cube masking ratio to be effective.</p> <p>Many other video SSL methods utilize image data in addition to video data. However, VideoMAE is able to achieve SOTA results on Kinetics-400 (video classification) without this external data. Ideally, we want video SSL methods that do not need to rely on images at all. This paper does not report results on image evaluation tasks. But this architecture would support this. We want video-pretrained models to achieve superior performance to image models on both video and image evaluation tasks. However, current video pretraining methods lag behind image-pretrained methods.</p> <p>This work leverages the flexibility of the transformer architecture to directly predict elements of the video. This is simpler in that it does not require tracking objects, constructing triplets, or applying data augmentations. It is closer to how language models are trained. This also allows producing image representations and video representations, unlike the CNN methods. This is because the number of input frames to the transformer is variable.</p> <h1 id="conclusion">Conclusion</h1> <p>Video lags behind images in representation learning for several reasons. While videos contain more information than static images, this information is spread across a much larger volume of data, making it computationally inefficient to process. The first frame of a video provides substantial information. You can learn what objects are present in the scene and the setting of the video. Subsequent frames offer diminishing returns. Future frames might just have one object moving across the frame. Because of this temporal redundancy, images are more information-dense than videos. Language models are more advanced than image models because language is a significantly more information-dense modality. Videos are an additional order of magnitude less information-dense than images. Data quality also plays a role. Video datasets, though vast, often lack the curated quality of image datasets like ImageNet, impacting the performance of video-based models. These challenges make it harder to build effective models with only video.</p> <p>Looking ahead, progress in video-based AI will be driven by improved datasets, advancements in computational power, and novel modeling techniques. As these developments unfold, we can expect more sophisticated vision models that effectively incorporate video data, bridging the current gap between image and video understanding in AI systems. This may unlock new capabilities in computer vision models.</p> <h3 id="datasets">Datasets</h3> <p><a href="https://www.image-net.org/">ImageNet</a>: 1,000,000 images, 1000 classes</p> <p><a href="https://arxiv.org/abs/1907.06987">Kinetics 700</a>: 650,000 videos, 700 classes, ~10 seconds each, 1800 hours</p> <p><a href="https://arxiv.org/abs/1609.08675">YouTube 8M</a>: ∼8 million videos, 500K hours of video—annotated with a vocabulary of 4800 entities</p>]]></content><author><name></name></author><category term="computer-vision"/><category term="self-supervised-learning"/><summary type="html"><![CDATA[In a previous blog post, we explored image-based self-supervised learning primarily with contrastive learning. Self-supervised learning offers a way to train an ML model to generate useful image representations using large amounts of unlabeled data. The resulting models can be used for a wide variety of downstream tasks, such as classification, object detection, or segmentation on different datasets.]]></summary></entry><entry><title type="html">SSL with Vision Transformers</title><link href="https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers/" rel="alternate" type="text/html" title="SSL with Vision Transformers"/><published>2024-08-01T00:00:00+00:00</published><updated>2024-08-01T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers/"><![CDATA[<p>In recent years, self-supervised learning (SSL) has emerged as a powerful paradigm in computer vision, allowing models to learn meaningful representations from unlabeled data. Prior work in this field focuses on using CNN architectures such as ResNet for this task. However, as evidenced by the success of self-supervised language models, transformers are a natural fit for self-supervised training. We will cover a set of recent papers that apply transformers for self-supervised visual learning.</p> <p>One key variation is that you often see masking in these methods. CNN-based SSL methods rely more on data augmentations to create a prediction task for the model. Masking is advantageous for several reasons outlined below, and it also aligns more with language model training (example: BERT).</p> <ul> <li>Computational efficiency <ul> <li>You do not have to process the masked regions of the image when a large portion of the image is masked.</li> </ul> </li> <li>Data augmentations can introduce unwanted invariances and remove useful information <ul> <li>For example, a data augmentation that strongly distorts the color may result in representations that do not encode color.</li> </ul> </li> </ul> <p>Masking is more naturally enabled by the transformer architecture. There is a reason that masking-based SSL training hasn’t worked well with CNNs.</p> <p>By examining these different methods, we’ll discuss what makes transformers work for vision.</p> <h1 id="dino"><a href="https://arxiv.org/abs/2104.14294"><strong>DINO</strong></a></h1> <p>This paper (Emerging Properties in Self-Supervised Vision Transformers) by Caron et al. introduces a new self-supervised training method called DINO, which they apply to vision transformers. They argue that transformers are better than CNNs for images with SSL training, more so than with supervised training. Transformers can match the performance of CNNs with supervised training, albeit with more training cost. However, they have more useful properties with SSL training. This follows our intuition that SSL and transformers are a natural combination.</p> <p>DINO takes inspiration from <a href="https://arxiv.org/abs/2006.07733">BYOL</a> but introduces two key innovations:</p> <ol> <li>A novel loss function that enables direct matching between student and teacher outputs</li> <li>Elimination of the prediction layer on the student, simplifying the architecture</li> </ol> <p>These changes result in a self-distillation approach that proves particularly effective with vision transformers.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/dino-480.webp 480w,/assets/img/blog/ssl-vit/dino-800.webp 800w,/assets/img/blog/ssl-vit/dino-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/dino.png" class="img-fluid mx-auto d-block" width="400" height="auto" alt="DINO architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2006.07733"> Source </a> </figcaption> </figure> <ol> <li>Two views of an image \(x\), \(x_1\) and \(x_2\) are generated through data augmentations. <ol> <li>A multi crop strategy is used in which two large global views are generated along with a set of smaller cropped local views. The teacher only processes global views, while the student processes all views, with the constraint that the loss is not trying to match the same views to each other. This method was introduced in the <a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper_files/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf&amp;hl=en&amp;sa=T&amp;oi=gsr-r-gga&amp;ct=res&amp;cd=0&amp;d=13209348926291080860&amp;ei=QYYkZu2RB5SCy9YP29Cc0AY&amp;scisig=AFWwaea44-zuGhikZl27njOvnygp">SwAV</a> paper, and helps the model learn local to global correspondences. Restricting the teacher to only global views also encourages the encoders to output global representations.</li> <li>Are position embeddings used?</li> </ol> </li> <li>The views are passed to their respective encoder (teacher/student)</li> <li>The teacher encoding is “centered”. <ol> <li>Perhaps centering allows this method to work without having the predictor layer. The center is a exponential moving average of the teacher encoding (of both views). This vector is subtracted from the teacher’s encoding before the softmax. A temperature is also applied with the softmax to achieve a “sharpening”. These methods help the teacher avoid collapse. Centering ensures that a single component of the vector doesn’t dominate. Sharpening ensures that it doesn’t collapse to a uniform vector.</li> </ol> </li> <li>Softmax is applied to each encoding. The student is trained with a cross entropy loss to match the teacher. The teachers weights are updated as an exponential moving average of the student.</li> </ol> <p>This paper compares the performance of DINO with ResNet and ViT architectures against <a href="https://rohitbandaru.github.io/blog//SSL-with-Vision-Transformers/">SOTA SSL methods</a> such as <a href="https://arxiv.org/abs/2006.07733">BYOL</a>, MoCov2, and SwAV. The combination os DINO and ViT has the most significant advantage. Interestingly, it is 6.6% better than ViT with BYOL training on linear ImageNet evaluation, despite minor differences in the methods. The SSL methods that are used for comparison were developed for CNN architectures, which put them at a disadvantage. DINO is designed for transformers, but what about it makes it work better with transformers? One possible explanation is that transformers handle different resolutions of images better. Higher resolution images results in more image patches generated in the transformer. The computation also scales quadratically in the attention operations with respect to the number of patches. For ResNet, the computation increases linearly.</p> <p>The two main “emerging properties” they observe is that DINO ViT features are useful for dense predictions such as semantic segmentation. Another property is that k nearest neighbors on the output encodings, without any finetuning. This enables image retrieval applications.</p> <p>They observe the teacher outperforms the student in DINO training. This is not observed with other SSL methods. They cite “Polyak-Ruppert averaging” as an explantation of this. This means the teacher simulates an ensemble model with its momentum weights.</p> <p>The multi-crop strategy enforces that the inputs be rectangular. This makes this method compatible with CNNs in addition to ViTs. DINO shows that SSL is effective with vision transformers. However, it is designed in a way that makes the training method compatible with CNNs. This leads to some very interesting comparisons between the properties of SSL CNN and ViT models. The other works we will discuss take advantage of the flexibility of the transformer architecture, at the cost of CNN compatibility.</p> <p><a href="https://arxiv.org/abs/2304.07193">DINOv2: Learning Robust Visual Features without Supervision</a> scales DINO using a 1 billion parameter ViT model along with a larger proprietary dataset. They used an interesting data processing pipeline to combine curated and uncurated data, to get a large dataset of high quality and diverse images. This step is important because unprocessed uncurated data can be of low quality and dominated by certain modes of data and duplicated data.</p> <p>There are several architectural and training changes applied on top DINO v1 that allow it to scale effectively. Notably, in addition to DINO, they add an <a href="https://arxiv.org/abs/2111.07832">iBOT</a> loss. This method masks some of the input tokens of the student. In order to combine DINO and iBOT losses, they learn separate heads on the student and teacher for each loss. iBOT does BERT style pretraining of image transformers, which we will also cover in this post.</p> <h1 id="data2vec"><a href="https://arxiv.org/abs/2202.03555">data2vec</a></h1> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec-480.webp 480w,/assets/img/blog/ssl-vit/data2vec-800.webp 800w,/assets/img/blog/ssl-vit/data2vec-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/data2vec.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="data2vec architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2202.03555"> Source </a> </figcaption> </figure> <p>The teacher model predicts representations from unmasked input, while the student model predicts representations from masked input. The student aims to match the teacher’s output by predicting the representations of the masked tokens. To avoid collapse, the teacher’s weights are an exponential moving average of the student’s weights.</p> <p>Instead of training a multimodal model, independent models are trained on different modalities. Data2VecAudio, Data2VecText, and Data2VecVision are developed. The learning objective remains the same, but the generation of embeddings and masking strategies differ.</p> <ol> <li>Encoding of inputs into embeddings: <ol> <li>Text is tokenized, and learned embeddings for each token are retrieved.</li> <li>Images are divided into 16x16 patches and linearly projected into an embedding.</li> <li>Audio is encoded by a 1D convolutional neural network with multiple layers. A 16 kHz waveform is mapped to a 50 Hz representation. This means a sequence of 320 integers is mapped to a single representation. <ol> <li>Unlike images, a multiple-layer network is used for audio, likely due to the absence of a Fourier transform.</li> </ol> </li> </ol> </li> <li>Masking: <ol> <li>Some of the student input embeddings are replaced by the MASK token embedding. <ol> <li>Text: Random tokens are masked.</li> <li>Images: Embeddings corresponding to rectangular blocks are masked.</li> <li>Audio: Continuous spans of embeddings are masked.</li> </ol> </li> </ol> </li> <li>Addition of position encoding.</li> <li>Both the teacher and student transformer models receive the input.</li> <li>Representations at different layers are distilled from the teacher to the student. Outputs from the masked tokens of the top \(K\) transformer blocks are normalized and averaged into a single vector.</li> <li>A regression loss (Smooth L1) is applied to the averaged vectors of each network. <ol> <li>The loss transitions from a squared loss to an L2 loss when the error margin goes below the hyperparameter \(\beta\). The L2 loss is only applied when the student and teacher predictions are close. This loss is designed to be less sensitive to outliers.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec_loss-480.webp 480w,/assets/img/blog/ssl-vit/data2vec_loss-800.webp 800w,/assets/img/blog/ssl-vit/data2vec_loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/data2vec_loss" class="mx-auto d-block" width="500" height="auto" alt="data2vec loss" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2202.03555"> Source </a> </figcaption> </figure> <ol> <li>The students weights are updated with SGD. The teacher’s weights are updated as a EMA of the students weights: \(\Delta \leftarrow \tau \Delta + (1-\tau)\theta\) <ol> <li>\(\Delta\) represents the teacher’s parameters, while \(\theta\) represents the student’s parameters.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec_architecture-480.webp 480w,/assets/img/blog/ssl-vit/data2vec_architecture-800.webp 800w,/assets/img/blog/ssl-vit/data2vec_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/data2vec_architecture.png" class="mx-auto d-block" width="500" height="auto" alt="data2vec architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The position encoding and feature encoder weights are shared between the two models. However, the teacher’s transformer weights are specified through an exponential moving average.</p> <p><a href="https://arxiv.org/abs/2212.07525"><strong>data2vec 2.0</strong></a></p> <p>Data2Vec 2.0 introduces several architectural and loss function changes that lead to a significant speed up in training.</p> <p>They use target representations for multiple masked predictions of a sample. This is more computationally efficient because we only need to run the teacher model once to train with \(M\) different masks of the input instead of 1. Further efficiency gains are implemented through not processing the masked parts of the image with the student, and sharing the feature encoder output across all masks.</p> <p>They use a L2 loss instead of a smooth L1 loss. This is a simplification of the earlier loss. They also use a convolutional decoder to predict the masked representations rather than a transformer.</p> <p>They also introduce inverse block masking. Rather than masking blocks. Blocks are chosen to be unmasked areas. The representations outside of the block will be predicted. There are multiple blocks which may overlap. A mask consists of multiple blocks. Training includes multiple masks for each target.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec_2-480.webp 480w,/assets/img/blog/ssl-vit/data2vec_2-800.webp 800w,/assets/img/blog/ssl-vit/data2vec_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/data2vec_2.png" class="mx-auto d-block" width="100%" height="auto" alt="data2vec 2.0" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2212.07525"> Source </a> </figcaption> </figure> <p>They also add a linear attention bias (<a href="https://arxiv.org/abs/2108.12409">ALiBi</a>). This essentially modifies self attention to increase the bias for query key pairs that are far apart. This enables faster training by providing an inductive bias.</p> <h1 id="masked-autoencoders-are-scalable-vision-learners"><a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a></h1> <p>This paper uses a simple autoencoder architecture to learn image representations. Parts of the images are masked, and the model is tasked to predict what is in the masked regions. This model can be trained through this <a href="https://github.com/ariG23498/mae-scalable-vision-learners/blob/master/mae-pretraining.ipynb">notebook</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/mae-480.webp 480w,/assets/img/blog/ssl-vit/mae-800.webp 800w,/assets/img/blog/ssl-vit/mae-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/mae.png" class="mx-auto d-block" width="100%" height="auto" alt="Masked Autoencoder" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2111.06377"> Source </a> </figcaption> </figure> <ol> <li>The image is split into patches, as done in Vision Transformers.</li> <li>Using a mask ratio (75%–95%), patches are selected randomly without replacement.</li> <li>The unmasked patches are input into the encoder. Note that the mask tokens do not get processed by the encoder (difference from BERT). The encoder uses a vanilla ViT architecture, where the unmasked patches are linearly projected into token embeddings which get processed by transformer blocks. The output is a ViT-processed embedding for each unmasked patch. Each patch has an added position embedding.</li> <li>The encoded tokens and the masked tokens are combined as an input to the decoder. The mask tokens map to a learned embedding. This embedding will be the same at all positions because it is not transformed by the encoder. At this stage, position embeddings are added to the full set. <ol> <li>Note that for unmasked tokens, position embeddings are added twice, once before the encoder and once before the decoder.</li> </ol> </li> <li>The decoder reconstructs the unmasked image from the set of patch embeddings. The decoder is trained by a mean squared error loss with respect to the unmasked input image.</li> </ol> <p>This architecture builds on the vision transformer. An alternative is to use CNNs. This would involve directly setting pixels in the input image to zero, learning a vector representation, and then decoding it back to the image. The reason this fails is that it aims to globally decode an image. With transformers, you first predict representations of the masked patches, and then decode into the image patch. This breaks it down into two easier problems. Also, with CNNs, you can’t explicitly encode masked regions like you can with a ViT. Having a mask token more explicitly indicates the mask.</p> <p>They mask a very high percentage of patches (80%). This reduces spatial redundancy and forces the model to learn more higher-level and useful features. With a lower mask ratio, the model might learn to represent small local changes, like color and lighting variation. It doesn’t need to understand the higher-level structure of the image, because it’s mostly already there. This is a notable change from language models. BERT masks 15% of tokens. MAE and related works mask a majority of the image (75%+).</p> <p>The model uses the ImageNet-1K dataset for pretraining and evaluation. Evaluation is done by either finetuning the full encoder model or using a linear probe (training one MLP layer on the output of the encoder) on the task of classification.</p> <p>One interesting result is that the performance of finetuning and linear probing has different trends when ablating the masking ratio. Linear probing accuracy increases linearly with masking ratio until 75%. Finetuning has relatively consistent performance between 40% and 80%.</p> <p>Having a deep decoder allows for the representations to be more abstract, because the decoder has more capacity for reconstruction. A shallower decoder would lead to the encoder having to represent more of the details needed for reconstruction. This is less relevant for finetuning than it is for linear probing, as during finetuning, the encoder can shift from focusing on reconstruction to recognition. In my opinion, linear probing results are more interesting since the goal is to build useful representations that can be used for various tasks. Finetuning offers just a marginal improvement over just training on the classification task directly without pretraining at all. However, linear probing discourages learning nonlinear features in the representation. To address this, the authors evaluate “partial finetuning” in which the last few blocks of the transformer are finetuned.</p> <p>Excluding mask tokens from the input and using a lightweight decoder makes this model very efficient to train. Using mask tokens in the encoder also creates a domain shift between pretraining and downstream tasks, which hurts performance. This is because a large portion of the pretraining input will be mask tokens, which is significantly different from what the model will see downstream.</p> <h1 id="beit-bert-pre-training-of-image-transformers"><a href="https://arxiv.org/abs/2106.08254"><strong>BEiT: BERT Pre-Training of Image Transformers</strong></a></h1> <p>This approach is most similar to BERT / NLP SSL models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/beit-480.webp 480w,/assets/img/blog/ssl-vit/beit-800.webp 800w,/assets/img/blog/ssl-vit/beit-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/beit.png" class="mx-auto d-block" width="100%" height="auto" alt="beit" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2106.08254"> Source </a> </figcaption> </figure> <p>A fundamental difference in applying SSL to images compared to text is that images are continuous. Text has a finite number of tokens. You can use a softmax to get a probability distribution across all tokens. In ViTs, patches of an image are treated as tokens. However, you can’t get an explicit probability distribution over all possible image patches. BEiT addresses this problem by training a discrete variational autoencoder (dVAE) to learn discrete visual tokens. These discrete tokens are an approximation or compression of image patches.</p> <p>The main difference between this and a vanilla ViT architecture is the usage of discrete visual tokens.</p> <p>There are two steps to training:</p> <ol> <li>Tokenizer and Decoder are trained as a VAE to learn discrete visual tokens</li> <li>The discrete tokens from the learned tokenizer are used to pretrain a BEiT encoder.</li> </ol> <p>Why aren’t the tokens used as the input directly? The softmax distribution of tokens could be used as a soft label for the BEiT encoder.</p> <p>The transformer training task is named masked image modeling (MIM), as it is designed after BERT’s masked language modeling (MLM). 40% of the tokens are masked. Similar to other methods, BEiT masks a large portion of the image to make the pretraining task sufficiently difficult.</p> <h1 id="conclusion">Conclusion</h1> <p>The landscape of self-supervised learning for image processing is undergoing a significant transformation. While it originated with Convolutional Neural Networks (CNNs), a strong coupling with transformer-based architectures is emerging and may lead the way for further advancements.</p>]]></content><author><name></name></author><category term="self-supervised-learning"/><category term="transformer"/><category term="computer-vision"/><summary type="html"><![CDATA[In recent years, self-supervised learning (SSL) has emerged as a powerful paradigm in computer vision, allowing models to learn meaningful representations from unlabeled data. Prior work in this field focuses on using CNN architectures such as ResNet for this task. However, as evidenced by the success of self-supervised language models, transformers are a natural fit for self-supervised training. We will cover a set of recent papers that apply transformers for self-supervised visual learning.]]></summary></entry><entry><title type="html">Deep Dive into Yann LeCun’s JEPA</title><link href="https://rohitbandaru.github.io/blog/JEPA-Deep-Dive/" rel="alternate" type="text/html" title="Deep Dive into Yann LeCun’s JEPA"/><published>2024-07-31T00:00:00+00:00</published><updated>2024-07-31T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/JEPA-Deep-Dive</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/JEPA-Deep-Dive/"><![CDATA[<p>In the AI research community, Yann LeCun has a unique and often controversial perspective. As of 2024, LLMs and Generative AI are the main focus areas of the field of AI. We’ve all been impressed by the performance of LLMs in various contexts, and generative systems like OpenAI’s <a href="https://openai.com/sora">Sora</a>. However, it is not clear where these advances fit in the long term goal of achieving and surpassing human level intelligence, which many call AGI.</p> <p>In his position paper <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">A Path Towards Autonomous Machine Intelligence</a> and his many recent talks (linked below), Yann presents an alternative framework for achieving artificial intelligence. He also proposes a new architecture for a predictive world model: Joint Embedding Predictive Architecture (JEPA).</p> <p>This blog post will dive deep into Yann’s vision for AI, the JEPA architecture, current research, and energy-based models. We will go deep into the technical aspects of these ideas, as well as give my opinions, along with interesting references. I will also cover recent research advances such as <em>V-JEPA</em></p> <p>This is a long post, feel free to jump to the sections about JEPA, I-JEPA, and V-JEPA.</p> <h3 id="relevant-talks-by-yann-lecun">Relevant Talks by Yann LeCun</h3> <p><a href="https://drive.google.com/file/d/1RVYBVi_bWyz-4sZSsu4rSWzDwQBLsvHL/view"><em>From Machine Learning to Autonomous Intelligence</em></a></p> <div class="video"> <figure> <iframe width="560" height="315" src="https://www.youtube.com/embed/VRzvpV9DZ8Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </figure> </div> <p><a href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf"><em>Objective-Driven AI: Towards Machines that can Learn, Reason, and Plan”</em></a></p> <div class="video"> <figure> <iframe width="560" height="315" src="https://www.youtube.com/embed/d_bdU3LsLzE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </figure> </div> <h1 id="problems-with-current-ai">Problems with Current AI</h1> <p>The JEPA architecture aims to address current AI challenges. To contextualize these issues, we’ll examine Yann LeCun’s criticisms of popular AI trends as of 2024.</p> <p>Recent years have seen tremendous excitement around Large Language Models (LLMs) and Generative AI. LLMs are pretrained using autoregressive self-supervised learning, predicting the next token given preceding ones. They’re trained on vast datasets of text and code from the internet and books, often fine-tuned with supervised learning or reinforcement learning. Generative AI broadly refers to creation of multimodal media from inputs, such as text-to-image generation.</p> <p>However, these models face significant limitations:</p> <ol> <li>Factuality / Hallucinations: When uncertain, models often generate plausible-sounding but false information. They’re optimized for probabilistic likelihood, not factual accuracy.</li> <li>Limited Reasoning: While techniques like <a href="https://arxiv.org/abs/2201.11903">Chain of Thought</a> prompting improve LLM’s ability to reason, they’re restricted to solving the selected type of problem and approaches to solving them without improving generalized reasoning abilities.</li> <li>Lack of Planning: LLMs predict one step at a time, lacking effective long-term planning crucial for tasks requiring sustained goal-oriented behavior.</li> </ol> <p>Despite impressive advancements, the challenge of autonomous driving illustrates the gap between current AI and human-level intelligence. As LeCun notes, humans can learn driving basics in about 20 hours. In contrast, self-driving car development has consumed billions of dollars, extensive data collection, and decades of effort, yet still hasn’t achieved human-level performance.</p> <p>Even achieving Level 5 autonomy wouldn’t signify true human-level AI or Artificial General Intelligence (AGI). Such intelligence would involve learning to drive from scratch within a day, using only data collected during that experience, without relying on massive pre-existing datasets for finetuning. Realizing this level of adaptable intelligence might require several more decades of research.</p> <h2 id="common-sense">Common Sense</h2> <p>The limitations in AI models can often be attributed to a lack of common sense. Common sense can be defined as thinking and acting in a reasonable manner. Humans and many animals have this ability. This includes avoiding egregiously dangerous or incorrect actions. Expanding on the autonomous driving example, AV systems need to be trained to deal with new situations safely. When learning to drive, humans utilize their common sense to know to not do dangerous things like driving off the road or into other cars. This is not obvious to current AV systems, so they require a large amount of training data to avoid these actions.</p> <p>LLMs similarly demonstrate a lack of common sense through nonsensical or illogical outputs. Common sense is a vague term. One definition is that it is a lower bound on the types of errors an agent makes. For AI to be trustworthy, it needs this foundational level of understanding.</p> <p>Common sense can also be viewed as a collection of world models. These models enable quick learning of new skills, avoidance of dangerous mistakes in novel situations, and prediction of outcomes in unfamiliar scenarios. Essentially, we use world models to generalize our experiences.</p> <h3 id="how-humans-learn">How Humans Learn</h3> <p>Humans acquire a basic understanding of the world during early infancy, but we’re also born with some innate knowledge. The brain isn’t randomly initialized; it’s evolved, pre-trained, and fine-tuned throughout life. This differs significantly from artificial neural networks, which start with random initializations and have far weaker inductive biases than humans or animals. Life is generally pre-programmed to behave in a certain way from birth. More intelligent life is able to learn more and not purely rely on innate knowledge.</p> <p>Understanding the extent to which babies acquire common sense during infancy is crucial for AI development. If common sense is largely innate, the focus should be on massive datasets mimicking evolutionary timescales. If it’s primarily learned, priority should be given to models that excel at quick learning from limited data.</p> <p>A baby’s experience, while not comparable to evolutionary timescales, still represents a substantial dataset. If a baby is awake for <a href="https://intuitiveparentingdc.com/blog/2018/7/6/developmentally-appropriate-sleep-expectations-birth-to-age-5">8 hours</a> a day, in four months they have seen about 960 hours of data. This data is also augmented by other sensory signals and dense biological supervision (pain, hunger, emotions). This is around the same length as the <a href="https://arxiv.org/abs/1705.06950">Kinetics 400</a> video dataset. This is still dwarfed by the millions of hours of video that self driving cars are using.</p> <p>This Nature <a href="https://www.nature.com/articles/s42256-024-00802-0">paper</a> by Orhan and Lake explores learning from infant-perspective data. They demonstrate that computer vision models can be trained on noisy, less diverse datasets collected from infant headcams. These egocentric datasets are far noisier and less diverse than standard image/video datasets, but AI models without strong inductive biases can learn from them.</p> <p>Emmanuel Dupoux’s diagram, presented by Yann LeCun, suggests that babies often understand concepts like object permanence, solidity, and biological motion by around four months. While presented as quick learning, it’s important to note the significant amount of data processing that occurs during this time.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/dupoux-480.webp 480w,/assets/img/blog/jepa/dupoux-800.webp 800w,/assets/img/blog/jepa/dupoux-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/dupoux.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Dupoux diagram on cognitive development" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>We don’t yet know precisely how much data AI systems would need to learn the same concepts as babies. It’s likely that the data efficiency gap is relatively small for basic concepts that babies learn. For instance, object permanence could probably be learned from 960 hours of video data. However, it becomes evident that this gap grows substantially with age and with the complexity of the knowledge being assessed. The challenges in developing fully autonomous vehicles clearly demonstrate how large this data efficiency gap can become.</p> <p>In addition to the lack of common sense, we mention three other fundamental gaps in the ability of current AI: hallucinations, lack of planning, and lack of reasoning.</p> <h2 id="learning-to-think">Learning to Think</h2> <p>The question of whether Large Language Models (LLMs) can truly reason and plan is a contentious topic in the AI community. While these models exhibit behaviors that resemble <a href="https://arxiv.org/abs/2201.11903">reasoning</a> and planning, skeptics argue that they merely replicate patterns from their training data.</p> <p>To frame this discussion, let’s consider reasoning and planning as forms of “thinking”, which we will define as a variable length internal process that precedes any outputs.. Current deep learning models employ two primary mechanisms for this kind of processing:</p> <ol> <li>Depth: Each layer in a neural network can be viewed as a step in the thinking process. However, this depth is typically fixed, with some recent <a href="https://arxiv.org/abs/2404.02258">work</a> exploring dynamic depth adjustment based on input complexity. Despite these advances, maximum depth and other constraints still limit the model’s flexibility.</li> <li>Sequential Generation: Decoder-based LLMs, such as GPT, generate text one token at a time. Each step in this process involves some degree of computation that could be interpreted as thinking. Prompt engineering techniques leverage this sequential nature to guide the model towards desired outputs. A key limitation of this approach is that the model must produce a token at each step, preventing purely internal information processing.</li> </ol> <p>While these properties enable models to create the illusion of thought, significant advancements are necessary to achieve more effective reasoning and planning capabilities.</p> <p>Many researchers draw parallels between AI and the two-system model of thinking <a href="https://www.google.com/books/edition/Thinking_Fast_and_Slow/ZuKTvERuPG8C?hl=en&amp;gbpv=1&amp;printsec=frontcover">proposed</a> by Daniel Kahneman. System 1 thinking is fast and intuitive, providing immediate responses without conscious deliberation. System 2, in contrast, is slower and more deliberate, engaging in deeper cognitive processing. Current machine learning models, including LLMs, primarily operate in a System 1 mode by processing information in a single pass without the ability to plan ahead. While they excel at pattern recognition, they lack true reasoning or planning capabilities.</p> <p>This inability to plan contributes to factual errors in LLM outputs. Each generated word carries a risk of inaccuracy, with the probability of errors increasing exponentially as the output length grows. The sequential nature of token generation means that early mistakes can compound, potentially invalidating the entire output. This stands in stark contrast to human speech, where we typically plan our utterances at a higher level before vocalization, minimizing such errors. In this context, reasoning can be viewed as the planning of speech. Without the capacity to reason or plan effectively, LLMs essentially “speak without thinking.”</p> <p>In the JEPA paper, Yann LeCun proposes frameworks for models that can think. Learning to think may address the fundamental problems in current AI models and represent a crucial step towards achieving more human-like intelligence in AI.</p> <h1 id="modality">Modality</h1> <p>Recent advancements have expanded LLMs to include multimodal processing and outputs, but they remain primarily language-centric. This raises questions about the sufficiency of language alone for AI and the investment needed in visual understanding. Could visual comprehension help ground AI in reality, improving common sense and reducing hallucinations?</p> <p>Language serves as a compressed representation of the complex concepts humans experience. Its expressive power is vast, capable of describing intricate scientific theories and nuanced emotions. Yet, language alone may not suffice for complete understanding.</p> <p>Humans interpret language within the context of shared reality. It functions as a highly efficient medium for transmitting information through the relatively narrow bandwidth of speech. When we process language, our brains rely on prior knowledge and experiences. While some of this prior information can be acquired through text, a significant portion stems from visual and physical interactions with the world.</p> <p>Currently, it does seem that language models are more capable than vision models. Language models currently outperform visual models due to information density, data requirements, and data availability.</p> <p>In a given data point there is a certain amount of explicit information in the form of bits. But then there is relevant information that is useful. For example, if you take an image of the park, a lot of bits are used to represent the position of every blade of grass. But that is not useful in most scenarios. Language is very compressed. While there are some filler words that don’t add much <a href="https://www.youtube.com/watch?v=VvPaEsuz-tY&amp;ab_channel=Argonaut57">information</a>, the ratio of knowledge to bits is high. However, for images, most of the bits are not useful. This means you need orders of magnitude more bits of data to learn equivalent knowledge. Video models are further behind because you need another order of magnitude more bits since consecutive frames in video are mostly redundant.</p> <p>While language-based AI leads, scenarios exist where visual learning could catch up. One scenario in which visual learning could overtake language is that we will have a large number of robots / autonomous vehicles interacting with the world while collecting visual data. Language will be data constrained with the rate of new text generation limiting scaling. In a world with a lot of robots, the knowledge gained from the visual world and the size of the available datasets may exceed that of text. However, this is all very speculative. We don’t know how important vision or grounding is for intelligence.</p> <h1 id="a-framework-for-building-human-level-ai">A Framework for Building Human-Level AI</h1> <p>Yann proposes a high level architecture for building an AI system that is aimed at addressing the problems we outlined. This is a design for an intelligent agent that can perceive the world,</p> <p>We will then explore the various challenges that must be addressed to construct such an architecture. Currently, this is merely a theoretical architecture. Building certain components remains an open problem, and assembling all the modules will pose an additional challenge.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/jepa_brain-480.webp 480w,/assets/img/blog/jepa/jepa_brain-800.webp 800w,/assets/img/blog/jepa/jepa_brain-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/jepa_brain.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="High Level View of LeCun's Architecture for Intelligence" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> High Level View of LeCun's Architecture for Intelligence <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>This architecture contains different proposed components. We will explain these components and their relationships.</p> <p><strong>Configurator</strong>: Configures input from all other modules and configures them for the task at hand. It tells the perception module what information to extract.</p> <p><strong>Perception:</strong> Estimates the current state of the world from different sensory signals.</p> <p><strong>World module</strong>: Estimates missing information about the state of the world and predicts future states. It simulates the world and extracts relevant information as determined by the configurator.</p> <p><strong>Cost module</strong>: Measures the level of discomfort as energy. This energy is the sum of the intrinsic cost module and the trainable critic module.</p> <p><strong>Intrinsic cost</strong>: Computes a cost given the current state of the world and predicted future states. This cost can be imagined as hunger, pain, or general discomfort. This cost can be hardwired in AI agents, as done with rewards in RL.</p> <p><strong>Trainable Critic</strong>: Predicts future intrinsic energy. It has the same input as the intrinsic cost. This estimate is dependent on the intrinsic cost and cannot be hardwired. It is trained from past states and subsequent intrinsic cost, retrieved from memory.</p> <p><strong>Short term memory</strong>: Stores relevant information about past present and future states of the world along with intrinsic cost.</p> <p><strong>Actor</strong>: Proposes sequences of actions. These sequences are executed by the effectors. The world model predicts future states from the sequence which then generates a cost.</p> <h1 id="actor">Actor</h1> <p>The actor proposes an optimal action or sequence of actions.</p> <p>If the world model and cost are well behaved, gradient based optimization can be used to determine an optimal action sequence. If actions are discrete then dynamic programming methods such as beam search can be used.</p> <p>There are two different modes in the actor. These align with Kahneman’s System 1 and 2, which we mentioned earlier.</p> <p><strong>Mode 1 Reactive Behavior</strong>: A policy module that computes an action from the state generated by perception and short-term memory. This module acts fast and produces simple decisions. A world model is needed to estimate the cost of an action. Without a world model the agent would have to perturb their actions which is not feasible. The world model can be adjusted after observing the next state.</p> <p><strong>Mode 2 Reasoning and Planning</strong>: A sequence of actions along with predicted corresponding states is generated. From this sequence of states, a cost can be computed. Planning is done by optimizing the action sequence to minimize total cost. The action sequence is then sent to the effectors which execute at least the beginning of the sequence. The states and costs are stored in short-term memory. The sequence can be optimized through gradients since the cost and world model are differentiable. Dynamic programming can also be used. Planning in this setup is essentially inference time cost optimization.</p> <p>Agents may have multiple policy modules executing mode 1. In this design, the agent only has one world model, so mode 2 can only be run once. However, AIs could be designed to have multiple world models and mode 2 processes at the same time. This is similar to having multiple thoughts at the same time. However, this would be very complicated in that the different modules would have to coordinate with the effectors and other modules to avoid conflicts. Also, this may be why humans don’t think like this.</p> <p>Policy modules can be learned to approximate actions from mode 2 reasoning. This is the process of learning a new skill. In humans, system 2 thinking can be done through system 1 after enough learning. For example, in chess, inexperienced players plan steps explicitly and simulate outcomes. Experienced players can instantly recognize patterns and make optimal moves.</p> <h1 id="cost">Cost</h1> <p>Cost is the sum of an immutable intrinsic cost and a trainable cost or critic.</p> \[C(s) = \mathrm{IC}(s) + \mathrm{TC}(s)\] <p>Each of these costs are the sum of different sub-costs generated by submodules. The weights of the sub-cost at each state \(u\) and \(v\) are determined by the configurator. This allows the agent to focus on different goals at different times.</p> \[\mathrm{IC}(s) = \sum_{i=1}^ku_i\mathrm{IC_i}(s)\\ \mathrm{TC}(s) = \sum_{i=1}^kv_i\mathrm{TC_i}(s)\] <p>The IC being immutable prevents the agent from drifting towards bad behaviors. It constrains the behavior of the agent.</p> <p>\(\mathrm{TC}\) or the critic is trained to predict future intrinsic cost values. The intrinsic cost only considers the current state. The critic can be trained to predict the future cost so the agent can minimize cost in the future. The short term memory stores triplets of (time, state, intrinsic energy): \((\tau, s_{\tau}, IC(s_{\tau}))\). The critic can be trained to predict the cost of a future state or a discounted sum of future intrinsic costs. For example, the loss function of the critic could be \(\|\|\mathrm{IC}(s_{\tau+\delta}) - \mathrm{TC}(s_{\tau})\|\|^2\). This formulation trains the critic to predict the intrinsic cost of a state \(\delta\) steps in the future. \(\mathrm{IC}(s_{\tau+\delta})\) can be replaced with other targets that can be extracted from the sequence of triplets. However, it cannot depend on the future trainable cost itself.</p> <h1 id="configurator">Configurator</h1> <p>The configurator controls the other components of the system. If these components are implemented as transformers, they can be easily configured by adding tokens. The configurator would inject tokens to steer these components in certain directions. For example, it may influence certain types of actions from the actor, or for perception to focus on certain properties.</p> <p>The configurator is also responsible for setting the weights of the cost terms. This will allow for the agent to focus on different subgoals at different times. The unanswered question is how the configurator can learn to decompose a complex task into subgoals.</p> <h1 id="world-model">World Model</h1> <p>In JEPA, the purpose of the world model is to predict future representations of the state of the world. There are three main issues</p> <ol> <li>Diversity of the state sequences the model is able to observe during training</li> <li>The world isn’t fully predictable, so the model has to predict multiple plausible state representations following an action</li> <li>Predictions must be made at different time scales and abstractions</li> </ol> <h2 id="self-supervised-learning--energy-based-models">Self-Supervised Learning / Energy-Based Models</h2> <p>In order to train a world model, Yann LeCun proposes an SSL energy-based model (EBM).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/ebm-480.webp 480w,/assets/img/blog/jepa/ebm-800.webp 800w,/assets/img/blog/jepa/ebm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/ebm.png" class="image-fluid mx-auto d-block" width="300" height="auto" alt="ebm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>\(x\) and \(y\) can be considered videos, where \(y\) follows x. EBMs learn an energy function \(F(x,y)\) that take low values when \(x\) and \(y\) are compatible and high if not. Compatible in this context means that \(y\) is a plausible continuation of \(x\).</p> <p>This is different from generative models in that \(y\) is not directly predicted from \(x\). There is a large space of values of \(y\) that can follow \(x\). Predicting exactly what will happen is an intractable problem. However, it is feasible to understand what is possible and what is not. Being good at this task requires an understanding of the world and common sense. A value of \(y\) that defies the laws of physics should result in a high energy value.</p> <p>However, planning requires predictions of future states. Although \(y\) can’t be predicted directly, we can predict future representations of \(y\). We can get representations from an encoder: \(s_x = g_x(x)\), \(s_y = g_y(y)\)</p> <p>The encoder will be trained such that the representations are maximally informative about each other, and that \(s_y\) can easily be predicted from \(s_x\). We can make predictions on this representation to enable planning.</p> <p>A latent variable can be introduced to handle uncertainty. A latent variable is just an arbitrary random variable. It is the source of randomness that is transformed to a useful distribution. Here we want to map the latent variable to the large space of possible values \(s_y\) can take.</p> <p>A latent-variable EBM (LVEBM) is represented as \(E_w(x, y, z)\).</p> <p>The energy function can be determined by find the \(z\) value that minimizes the energy. \(F_w(x,y) = \min_{z \in \mathcal{Z} }E_w(x,y,z)\)</p> <p>The EBM collapses when all pairs have the same low energy. This can happen when the latent variable has too much information capacity. This happens because \(z\) can vary along a larger space. This means that the space for which the energy of \(y\) is low is correspondingly large. If it is too large then the energies of \(y\) collapse. If the \(z\) dimension is the same as the representation dimension, the model can ignore \(y\) entirely and set \(s_y\) to equal \(z\).</p> <p>The paper describes a high data density region. This refers to \((x, y)\) pairs that are commonly seen in the real data distribution. We want to lower energy in this region, but keep it high outside of it. Collapse is when the energy is low inside and outside of this region which makes the EBM useless.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/regularized_ebm-480.webp 480w,/assets/img/blog/jepa/regularized_ebm-800.webp 800w,/assets/img/blog/jepa/regularized_ebm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/regularized_ebm.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="regularized ebm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>There are two training methods used to prevent collapse.</p> <p>Contrastive methods: Collapse is avoided by increasing the energy with respect to negative examples. It requires some method to generate examples to contrast against. The number of contrastive examples needed grows exponentially with respect to the dimension of the representation.</p> <p>Regularized methods: In these methods, the loss is regularized to minimize the space in \(y\) where the energies are lowered. These are less likely to be affected by the curse of dimensionality. Contrastive architectures can be regularized. For example, the latent dimension can be constrained.</p> <h2 id="joint-embedding-predictive-architecture">Joint Embedding Predictive Architecture</h2> <p>JEPA is an EBM that performs predictions in the representation space. The energy is the error in predicting \(s_y\) from \(s_x\).</p> <p>JEPA needs multi-modality, which in this context means to represent multiple possible values of \(y\). There are two ways it can be achieved.</p> <p>Encoder invariance: This means that \(s_y\) will be the same for different values of \(y\). The encoder ignores aspects of the state that may vary.</p> <p>Latent variable predictor: Varying \(z\) will lead to different plausible predictions of \(s_y\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/lv_jepa-480.webp 480w,/assets/img/blog/jepa/lv_jepa-800.webp 800w,/assets/img/blog/jepa/lv_jepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/lv_jepa.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="JEPA with a latent variable" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>There are four criteria that can be used to train this architecture without contrastive loss:</p> <ol> <li>Maximize the information content of \(s_x\) about \(x\): \(-I(s_x)\)</li> <li>Maximize the information content of \(s_x\) about \(y\): \(-I(s_y)\)</li> <li>Make \(s_y\) predictable from \(s_x\): \(D(s_y, \tilde{s_y})\)</li> <li>Minimize the information content of the latent variable with a regularizer: \(R(z)\)</li> </ol> <h3 id="hierarchical-jepa-h-jepa">Hierarchical JEPA (H-JEPA)</h3> <p>There is a trade off between information loss in the encoding and the predictability of the encodings. If a representation contains most of the information of the input, it would be hard to predict. A more abstract and higher level representation would be lower in dimension and more predictable. Higher dimension representations are also more suitable for longer term predictions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/hjepa-480.webp 480w,/assets/img/blog/jepa/hjepa-800.webp 800w,/assets/img/blog/jepa/hjepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/hjepa.png" class="image-fluid mx-auto d-block" width="500" height="auto" alt="H-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>H-JEPA (Hierarchical JEPA) enhances JEPA’s abstraction capabilities by splitting the architecture into two parts. The first JEPA handles low-level representations for short-term predictions, while the second operates at a higher abstraction level for longer-term forecasts. This two-tier structure, though innovative, is arbitrary. True intelligence requires multiple levels of abstraction. However, it is not clear how many levels of abstraction are needed. We may even need variable levels of abstraction. Different situations have different levels of complexity.</p> <p>This architecture can enable higher level planning. In JEPA-2, we can sample from the latent variable for several time steps. Directed search / pruning can be employed in order to efficiently search. This search can be used to determine an optimal action.</p> <p>This kind of search would be different in JEPA-1 or without H-JEPA because the latent dimension would be too large to efficiently sample from. Abstraction is needed to enable this kind of planning.</p> <h2 id="world-model-architecture">World Model Architecture</h2> <p>The world is unpredictable but the agent itself is predictable to the agent. This may motivate a model of self (ego model) that does not have a latent variable.</p> <p>The state of the world varies only slightly between time steps. Rather than regenerating, it can be updated in memory. With this architecture, the world model will only output the change in the state. This can be implemented with an attention-like mechanism.</p> <ol> <li>The world model outputs query value pairs: \((q[i], v[i])\)</li> <li>The world model retrieves a value from memory using the query <ul> <li> \[\mathrm{Mem}(q) = \sum_jc_jv_j\] <ul> <li>The value retrieved from memory is a weighted sum of all values.</li> </ul> </li> <li> \[\tilde{c}_j = \mathrm{Match}(k_j,q)\] <ul> <li>Measures dissimilarity between the key and query.</li> </ul> </li> <li> \[c = \mathrm{Normalize}(\tilde{c})\] <ul> <li>This is often a softmax.</li> </ul> </li> <li> \[v_j = \mathrm{Update}(r,v_j,c_j)\] <ul> <li>Value is updated using the current value and new value.</li> <li>The update function can be \(cr+(1-c)v\)</li> </ul> </li> </ul> </li> </ol> <h1 id="data-streams">Data Streams</h1> <p>In building a world model, we have to consider the fundamental differences in the type of data that humans and AI models process. Yann lists 5 modes of information gathering that an agent can use to learn its world model.</p> <ol> <li>Passive observation: sensor stream without control</li> <li>Action foveation: The agent can direct attention within the data stream</li> <li>Passive agency: Observing another agent’s actions and causal effects</li> <li>Active Egomotion: The sensors can be configured, for example moving a camera</li> <li>Active Agency: Sensory streams that are influenced by the agent’s actions</li> </ol> <p>Current AI methods largely focus on passive observation. Other modes may be needed to reach intelligence.</p> <p>AI is trained on internet data. Internet data is not experienced by the agent. Humans train on data that they experience. This is a fundamental difference. This is also why autonomous cars need so much training data. The AI driving systems don’t have other datasets that they have experienced. For example, if they trained on a large dataset of just walking around, they would need less driving data.</p> <p>It is challenging to create large-scale datasets from the perspective of an agent, especially reaching the scale of internet datasets. A present-day example is autonomous car datasets. AV companies have large fleets of vehicles on the road collecting data. These are active data streams.</p> <h1 id="objective-driven-ai">Objective Driven AI</h1> <p>The components of this architecture can be put together to build an intelligent system that follows human defined objectives.</p> <p>Perception is used to generate an initial representation of the state of the world. The actor proposes a sequence of actions. The world model then predicts the state reached if the action sequence is executed. This state is then used in the objectives. The task objective defines what we want the system to do. This could be a task or particular problem. The guardrail objective makes sure the system accomplishes the task without any unwanted behavior. These guardrails would be designed for safety.</p> <p>The action sequence is optimized with respect to the objects. There will be a lot of flexibility in designing the objects to get the system to behave in the way we want.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/objective_driven_ai-480.webp 480w,/assets/img/blog/jepa/objective_driven_ai-800.webp 800w,/assets/img/blog/jepa/objective_driven_ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/objective_driven_ai.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Objective Driven AI" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf"> Source </a> </figcaption> </figure> <p>The system can also be extended to achieve hierarchal planning. The higher levels of planning produce a state that will serve as an objective for the lower level. This state can be considered as a subgoal that is necessary to achieve the higher level goal. We can have unique objectives and guardrails for each level of planning.</p> <p>Latent variables are also introduced to represent the uncertainty in predictions of future states. The latent variables at the higher levels can be thought as imaginary higher level actions. However, only the lower level actions can actually be directly executed.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/h_objective_driven_ai-480.webp 480w,/assets/img/blog/jepa/h_objective_driven_ai-800.webp 800w,/assets/img/blog/jepa/h_objective_driven_ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/h_objective_driven_ai.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Hierarchal Objective Driven AI" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf"> Source </a> </figcaption> </figure> <h1 id="towards-implementing-jepa">Towards Implementing JEPA</h1> <p>The JEPA paper is a position paper that describes a vision for AI that may take decades to materialize. However, since its publication in the summer of 2022, there have been a few steps in advancing the architecture. These papers particularly explore the training of JEPAs. They do not explore the other components such as planning. These JEPAs are the first steps to creating a world model.</p> <p>These are essentially self supervised pretraining methods. When comparing against other works, these papers cite training speed as their advantage. They can achieve strong downstream performance with fewer pretraining epochs.</p> <h2 id="i-jepa-self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture">I-JEPA: <a href="https://arxiv.org/abs/2301.08243">Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</a></h2> <p>Compared to other image SSL approaches, I-JEPA takes advantage of the flexibility of the transformer architecture. ViT is used because it can handle an arbitrary amount of patches in an image, without requiring a strict shape in the input like CNNs</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/ijepa-480.webp 480w,/assets/img/blog/jepa/ijepa-800.webp 800w,/assets/img/blog/jepa/ijepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/ijepa.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="I-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2301.08243"> Source </a> </figcaption> </figure> <p>The input image is split into \(N\) non-overlapping patches and fed into a target encoder \(f_{\theta}\) to compute patch representations. \(s_y = \{s_{y1} … s_{yN}\}\)</p> <p>\(M\) possibly overlapping blocks are sampled from these representations. These blocks are basically larger sections of the image that contain multiple patches.</p> <p>Context is generated by sampling a block (larger than the target blocks). When predicting a target from this context, the overlap with the target block is masked from the context. The network is trained to predict the representations of the target blocks given the context block, and position encodings for the target block. The position encodings are added to the input so that the model knows where the target is. It is just tasked with predicting representations at those positions.</p> <p>This architecture avoids collapse by having exponential moving average weights in the target encoder. This is the same approach used in data2vec and BYOL.</p> <p>The main hyperparameters introduced by this work is the scale and aspect ratio of the target and context blocks. Generally, a small context is used to make this task difficult, which would force the model to learn higher level and more useful features.</p> <h2 id="v-jepa-revisiting-feature-prediction-for-learning-visual-representations-from-video">V-JEPA: <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/">Revisiting Feature Prediction for Learning Visual Representations from Video</a></h2> <p>V-JEPA is an extension of I-JEPA to videos. This is done by treating videos are 3d images.</p> <ol> <li>A clip of 64 frames (~2.1 seconds of video at 30 frames per second) is extracted from the video and resized to 16 × 224 × 224 × 3.</li> <li>The clip is split into \(L\) spatiotemporal patches of size 16x16x2 (2 is the number of consecutive frames.</li> <li> <p>A random mask is calculated for the context. This is a 2D that is similar to the mask in I-JEPA. This mask is then repeated across the time dimension. This repetition is necessary because the videos are short and there would be too much redundancy for the same patch at different time steps. This redundancy would make the learning task too easy. This masking creates a context image, while the target is the original image.</p> <ol> <li>2 masks are sampled: one short range and one long range. The short range mask covers less area in the image and is more discontinuous. These masks are constructed by different configurations of overlapping blocks, as done in I-JEPA. The target encoder only needs to run once, even if there are multiple masks for the context. Having multiple masks leads to more efficient training.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/vjepa_masking-480.webp 480w,/assets/img/blog/jepa/vjepa_masking-800.webp 800w,/assets/img/blog/jepa/vjepa_masking-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/vjepa_masking.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="V-JEPA masking" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Short-range (left), long-range (right) <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/"> Source </a> </figcaption> </figure> <ol> <li>The tokens are processed by a transformer encoder (linear projection of patches + multiple transformer blocks). The masked out patches do not need to be processed. There is a separate encoder for the target and context. The target encoder is an EMA of the context encoder (same as I-JEPA).</li> <li>The predictor predicts the representations of the masked tokens by the unmasked tokens processed by the context encoder. The loss is the L1 distance between the representations of these masked tokens (from the target encoder, and the context encoder + predictor).</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/vjepa-480.webp 480w,/assets/img/blog/jepa/vjepa-800.webp 800w,/assets/img/blog/jepa/vjepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/vjepa.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="V-JEPA Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Very similar to I-JEPA but with an added temporal dimension. <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/"> Source </a> </figcaption> </figure> <p>This is predicting gaps in short videos. It does not predict across time. Human learning is across the time dimension.</p> <p>Attentive probing is used to evaluate this model on different finetuning tasks. This is needed in place of linear probing since the input size may vary. This just requires learning a query token specific to the task and a linear classifier on top of the pretrained encoder.</p> <p>V-JEPA processes small sequences of frames. These short videos are essentially images with a little animation. However, that is the current state of video self-supervised learning. To achieve a model that is closer to human or even animal-level intelligence, this approach needs to scale up significantly. The resolution of the video needs to be increased. Also, the model needs to process longer durations of video and make predictions across time. For example, you should be able to predict what happens in the next 1 minute, based on the previous ten minutes of video input. Such a model could be the basis for an intelligent agent’s world model.</p> <p>V-JEPA is a very interesting model that may be the start of a highly important line of research.</p> <h2 id="mc-jepa-a-joint-embedding-predictive-architecture-for-self-supervised-learning-of-motion-and-content-features">MC-JEPA: <a href="https://arxiv.org/abs/2307.12698">A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</a></h2> <p>This is an extension of JEPA to include motion information. It uses an optical flow objective to learn motion from videos and uses general SSL to learn about the content of the images/videos. Optical flow is estimating the direction in which pixels move between two consecutive frames of a video.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/mcjepa_architecture-480.webp 480w,/assets/img/blog/jepa/mcjepa_architecture-800.webp 800w,/assets/img/blog/jepa/mcjepa_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/mcjepa_architecture.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="MC-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2307.12698"> Source </a> </figcaption> </figure> <p>The details of this dense flow estimation are out of the scope of this blog post. Flow estimation and content feature learning are combined as a multitask learning objective. Images are sampled for content learning, while consecutive frames are sampled from videos for flow estimation. The encoder is shared for both tasks. This is a JEPA architecture because the representations from one frame are warped to match the representations from the next frame. The same encoder is used to process both frames.</p> <p>The architecture for flow estimation is hierarchal. This may be the first instantiation of an H-JEPA architecture. This architecture is based on <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_PWC-Net_CNNs_for_CVPR_2018_paper.pdf">PWC-Net</a>. Each level has a different resolution.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/mcjepa_full_architecture-480.webp 480w,/assets/img/blog/jepa/mcjepa_full_architecture-800.webp 800w,/assets/img/blog/jepa/mcjepa_full_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/mcjepa_full_architecture.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="MC JEPA full architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2307.12698"> Source </a> </figcaption> </figure> <p>The image features are sampled from ImageNet, while a video dataset is used for flow estimation. It is also possible to use frames from video as images for content learning.</p> <p>This work shows that the JEPA framework is generalizable. There are a lot of ways that we could design a world model and it could include many possible objectives.</p> <h2 id="whats-next">What’s Next?</h2> <p>The current research in JEPA represents a significant step towards Yann LeCun’s vision of building a world model capable of human-level AI. While the present focus is on creating effective representation learning models for visual data, the ultimate goal is far more ambitious. The holy grail of this research is a V-JEPA model that can predict across extended time horizons, potentially through a Hierarchical JEPA architecture capable of processing complex, lengthy videos like 10-minute YouTube clips.</p> <p>To realize this vision, several crucial advancements are necessary. Firstly, we need to embrace true multimodality, incorporating audio and other modalities that are often overlooked in current video models. Scaling up V-JEPA is also essential, requiring larger video datasets and more sophisticated model architectures that can handle higher resolutions. Additionally, the development of more challenging benchmarks for video understanding is critical, as current standards fall short of the complexity seen in image or language modeling tasks.</p> <p>Future iterations of V-JEPA must evolve beyond spatial masking to make predictions across various time horizons. This capability to forecast future representations based on present information is fundamental to understanding the temporal dynamics of video content. Achieving this may necessitate a hierarchical JEPA structure, where different levels handle predictions at various time scales and abstraction levels. Maybe the next JEPA paper will introduce a hierarchal video JEPA (HV-JEPA).</p>]]></content><author><name></name></author><category term="self-supervised-learning"/><category term="ai"/><category term="computer-vision"/><summary type="html"><![CDATA[In the AI research community, Yann LeCun has a unique and often controversial perspective. As of 2024, LLMs and Generative AI are the main focus areas of the field of AI. We’ve all been impressed by the performance of LLMs in various contexts, and generative systems like OpenAI’s Sora. However, it is not clear where these advances fit in the long term goal of achieving and surpassing human level intelligence, which many call AGI.]]></summary></entry><entry><title type="html">Scaling Deep Learning</title><link href="https://rohitbandaru.github.io/blog/Scaling-Deep-Learning/" rel="alternate" type="text/html" title="Scaling Deep Learning"/><published>2023-02-21T00:00:00+00:00</published><updated>2023-02-21T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Scaling-Deep-Learning</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Scaling-Deep-Learning/"><![CDATA[<p>Many of the state-of-the-art results in deep learning are achieved using multiple GPUs. For some of the largest and most data-intensive ML models, it can take months or even years to train on one CPU or GPU. Training is sped up by scaling to large numbers of GPUs/TPUs. Some neural networks are too large to even fit on one GPU. For example, training large language models like BERT can easily exceed the available memory on a single <a href="https://github.com/google-research/bert/blob/master/README.md#out-of-memory-issues">GPU</a>.</p> <p>If you have the resources it can be easy to speed up training by adding more GPUs. However, it is important to understand the impact this scaling will have on training. Machine learning acceleration is a huge and complex field. I intend to just cover the basic intuitions to keep in mind when training a typical model.</p> <aside> ✍️ We will use GPU and TPU interchangeably. We are treating ML accelerators as generic. </aside> <p>There are two types of machine learning training parallelization: data parallelism and model parallelism.</p> <h1 id="data-parallelism">Data Parallelism</h1> <p>Data parallelism splits a training batch into smaller batches for each GPU. Each GPU has its own copy of the model. Each GPU computes gradients with its own training batch. These gradients are then aggregated across all the GPUs. Each GPU can send its gradients to all other GPUs. For example, if you train with a batch size of 64 and 4 GPUs, each GPU will get a batch size of 16. It will compute gradients for this batch. Once all the GPUs are done with their computations, they can send their gradients to each other. The gradients are then averaged and applied to the model. This allows us to train a model with batch size 64 at the speed of batch size 16. However, there is additional latency in communicating the gradients and synchronizing the GPUs, but it is usually negligible compared to the gradient computations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/data_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/data_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/data_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/data_parallelism.png" width="100%" height="auto" alt="Data parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Another option is to skip the gradient aggregation and simply apply the updates to the model separately. This can be done by having an orchestrator take a lock on the model. Or in <a href="https://arxiv.org/abs/1106.5730">Hogwild</a>, you can just update the model without any lock. This will allow some GPU batches to be dropped due to race conditions but minimizes synchronization delays.</p> <p>Adding GPUs doesn’t make training steps faster. It allows you to have larger mini-batch sizes, which in turn trains models faster.</p> <p>There are three variables to consider: mini-batch size, GPU batch size, and number of GPUs. Since the number of GPUs is a function of the other two variables, we will only discuss the two types of batch size and how to optimize them.</p> \[\textrm{Mini batch size} = \textrm{GPU batch size} * \textrm{Number of GPUs}\] <p>The implementation of parallelism can vary between ML frameworks: <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch</a>, <a href="https://www.tensorflow.org/guide/distributed_training">TensorFlow</a>, <a href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html#way-batch-data-parallelism">Jax</a></p> <h2 id="optimal-gpu-batch-size">Optimal GPU Batch Size</h2> <p>With GPUs, we simply want to minimize the training step time. If we operate the GPUs in the optimal GPU batch size range, we can then just set the number of GPUs to get the optimal mini-batch size. Also, note the GPU batch size has an upper limit from the memory available.</p> <p>To see how GPU performance relates to speed, I timed the training steps of a ResNet50 model against ImageNet-sized batches of different sizes. I tested batch sizes of every power of two until the GPU ran out of memory.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/step_speed_vs_batch_size-480.webp 480w,/assets/img/blog/scaling_ml/step_speed_vs_batch_size-800.webp 800w,/assets/img/blog/scaling_ml/step_speed_vs_batch_size-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/step_speed_vs_batch_size.jpg" width="100%" height="auto" alt="step speed vs batch size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We see that the throughput (examples per ms) is maximized at the largest possible batch size. We also see that for batch sizes of less than 2^4 or 16, the throughput is lower. GPUs are inefficient with small batches due to overhead. CPUs perform better in some settings. The takeaway is that we want to maximize the GPU utilization by fitting the largest possible batch. Some libraries have the functionality to search for the largest possible batch size given a GPU and dataset. In the flat region, GPU step time increases linearly with batch size.</p> <h2 id="optimal-mini-batch-size">Optimal Mini Batch Size</h2> <p>To optimize the mini-batch size, we will ignore accelerators and just focus on mini-batch gradient descent. Mini batch size is less hardware-dependent and more problem dependent. We will use ImageNet as an example, but the effects of batch size on training should be considered for every new problem.</p> <p>Assuming maximize GPU usage/batch size, optimizing the mini-batch size means selecting the number of GPUs to use. The assumption is that with more GPUs, we can train a model faster. Training on more GPUs means faster training epochs. However, we are interested in the test accuracy of the model, not just completing epochs.</p> <p>Consider this plot from the paper <a href="https://arxiv.org/abs/1811.03600">Measuring the Effects of Data Parallelism on Neural Network Training</a> by Shallue et al.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/effects_of_dp-480.webp 480w,/assets/img/blog/scaling_ml/effects_of_dp-800.webp 800w,/assets/img/blog/scaling_ml/effects_of_dp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/effects_of_dp.png" width="500" height="auto" alt="Plot of training speed vs batch size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1811.03600"> Source </a> </figcaption> </figure> <p>For points on the dashed line, the number of training steps is halved whenever the batch size is doubled. This means that doubling the GPUs/TPUs would have the training time. This is ideal. In this region, you can happily speed up your model training by adding more GPUs that you may have available. However, this tradeoff changes at batch size 2^13 or 8192. From here, doubling the GPUs still speeds up model training, but the speed will be more than half. This is the point of diminishing returns. If you have the GPUs, you might as well use them but those additional GPUs are not as effective.</p> <p>The paper goes into great detail on this relationship and the effects of other factors such as model architecture, optimizers, and datasets. The takeaway for this blog is that if you set the maximum GPU batch size, up to a point, adding additional GPUs will linearly speed up the training of your model.</p> <h1 id="model-parallelism">Model Parallelism</h1> <p>This type of parallelism is much less commonly used. It can be used along with data parallelism. Model parallelism is when an ML model is too large to fit in the memory of one device. It is partitioned across multiple devices. This has enabled us to train larger and larger networks. For example, the GPT-3 model is about <a href="https://www.reddit.com/r/MachineLearning/comments/gzb5uv/comment/fti44lv/?utm_source=share&amp;utm_medium=web2x&amp;context=3">350 GB</a>. No single GPU can store the whole model in memory.</p> <p>There are different ways of achieving model parallelism. You can split the model vertically by layer, or horizontally by splitting the tensors.</p> <h2 id="pipeline-parallelism">Pipeline Parallelism</h2> <p>The simplest solution is to process different layers of a neural network on different accelerations. A simple illustration of this:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipeline_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/pipeline_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/pipeline_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/pipeline_parallelism.png" width="100%" height="auto" alt="pipeline parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In <a href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html#speed-up-by-pipelining-inputs">PyTorch</a>, the layers are bucketed into groups of roughly equal memory so that the computations are evenly distributed across the accelerators.</p> <p>A major issue with this approach is that after Layer 0 has a forward pass, it has to wait for the other layers to compute forward and backward passes. The GPU is idle for about 75% of the time. The following diagrams are from the <a href="https://arxiv.org/abs/1811.06965">GPipe</a> paper by Huang et al.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism.png" width="100%" height="auto" alt="batches without pipeline parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1811.06965"> Source </a> </figcaption> </figure> <p>The solution is to have the layer compute the next graph while it is waiting for the gradient of the current batch. This essentially combines data parallelism with model parallelism. I explained above that to maximize training speed, we want to maximize the utilization of accelerators. For large models that require model parallelism, we have an additional problem of GPU waiting time. Pipelining GPU batches helps reduce this gap.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipeline_parallelism_batches-480.webp 480w,/assets/img/blog/scaling_ml/pipeline_parallelism_batches-800.webp 800w,/assets/img/blog/scaling_ml/pipeline_parallelism_batches-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/pipeline_parallelism_batches.png" width="100%" height="auto" alt="Pipeline parallelism batches" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1811.06965"> Source </a> </figcaption> </figure> <p>We see that with 4 GPU batches, each GPU is idle for about 6/16 of the time. The variables here are the number of GPUs and number of GPU batches per GPU. With 1 GPU batch (no pipelining), the utilization is: \(\frac{2}{n_{GPU}* 2} = \frac{1}{n_{GPU}}\). With pipelining, we get \(\frac{n_{batches}*2}{n_{batches}*2 + 2* (n_{GPU}-1)}\). This simplifies to the following:</p> \[utilization = \frac{n_{batches}}{n_{batches} + n_{GPU}-1}\] <p>This equation explains the tradeoff. Increasing the number of GPU batches drives the utilization closer to 1, while increasing the number of GPUs reduces the utilization.</p> <p>In an optimal setup, we split the model among as few GPUs as possible, but increase the number of batches that they process in a step. Pipeline parallelism has the added benefit of the speedups of data parallelism. This makes it a very effective solution.</p> <p>In the <a href="https://arxiv.org/abs/1806.03377">PipeDream</a> paper, Harlap et al. show that we can further reduce idle time by interleaving forward and backward operations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipedream-480.webp 480w,/assets/img/blog/scaling_ml/pipedream-800.webp 800w,/assets/img/blog/scaling_ml/pipedream-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/pipedream.png" width="100%" height="auto" alt="pipedream" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1806.03377"> Source </a> </figcaption> </figure> <p>However, this eliminates gradient synchronization. Even eliminating batches above 4, we get the same utilization as GPipe parallelism, just in a different order. For many of the backward passes, a stale version of the model parameters is used. Gradient synchronization is an important tradeoff in all types of ML parallelism.</p> <p>In analyzing utilization, we have been assuming that forward and backward computations are equivalent. Backward passes tend to take more time. If we interleave operations as to always prioritize backward passes, we can get a utilization gain. From AWS Sagemaker <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html">documentation</a>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipedream1-480.webp 480w,/assets/img/blog/scaling_ml/pipedream1-800.webp 800w,/assets/img/blog/scaling_ml/pipedream1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/pipedream1.png" width="100%" height="auto" alt="without backward prioritization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html"> Source </a> </figcaption> </figure> <p>The idle time here is 1 forward pass and 1 backward pass.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipedream2-480.webp 480w,/assets/img/blog/scaling_ml/pipedream2-800.webp 800w,/assets/img/blog/scaling_ml/pipedream2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/pipedream2.png" width="100%" height="auto" alt="with backward prioritization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html"> Source </a> </figcaption> </figure> <p>With backward prioritization, the idle time for GPU 0 is 3 forward passes. This effect will be increased with more GPUs. There are many tradeoffs in parallel ML, such as communication between model layers, the memory overhead of forward and backward passes, different model splits, staleness, etc. We are only covering the high-level intuitions to achieve fast and effective training of large models.</p> <p>What if we want to use more GPUs for data parallelism, but without splitting up the model further? We can simply run pipelines in parallel. For example, we can split the model among four GPUs but duplicate each model split twice. We can then aggregate the gradients of both pipelines in the update. This is often called hybrid model and data parallelism.</p> <h2 id="tensor-parallelism">Tensor Parallelism</h2> <p>Instead of splitting the model into layers, we can split the layers themselves. From the <a href="https://arxiv.org/abs/1909.08053">Megatron-LM paper</a> by Shoeybi et al.:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/tensor_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/tensor_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/tensor_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/tensor_parallelism.png" width="100%" height="auto" alt="tensor parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1909.08053"> Source </a> </figcaption> </figure> <p>The input X has to be completely copied for each split of the model. The layer is split into two halves. The splits of the model are then aggregated in the last layers of the model. Splitting the tensors themselves offers some benefits. The latency is reduced since you can fit more layers on a GPU. This is parallel computation instead of serialized computation. You don’t have to worry about scheduling to minimize idle time.</p> <p>An issue with this approach is that the activations are also separated, so you are learning a different model architecture. There is an additional cost in concatenating \(Y_1\) and \(Y_2\) for both GPUs. The Megatron-LM architecture is designed to reduce the cost of communicating between GPUs.</p> <h1 id="conclusion">Conclusion</h1> <p>We touched the surface on the many tradeoffs, optimizations, and considerations needed for distributed and large scale ML. As models grow larger, it will become more import to understand and keep up to date with this field.</p> <h1 id="additional-resources">Additional Resources</h1> <p><a href="https://www.youtube.com/watch?v=3XUG7cjte2U">https://www.youtube.com/watch?v=3XUG7cjte2U</a></p> <p><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">https://lilianweng.github.io/posts/2021-09-25-train-large/</a></p> <p><a href="https://openai.com/blog/techniques-for-training-large-neural-networks/">https://openai.com/blog/techniques-for-training-large-neural-networks/</a></p> <p><a href="https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/">https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/</a></p> <p><a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">https://huggingface.co/docs/transformers/v4.15.0/parallelism</a></p>]]></content><author><name></name></author><category term="applied-ml"/><summary type="html"><![CDATA[Many of the state-of-the-art results in deep learning are achieved using multiple GPUs. For some of the largest and most data-intensive ML models, it can take months or even years to train on one CPU or GPU. Training is sped up by scaling to large numbers of GPUs/TPUs. Some neural networks are too large to even fit on one GPU. For example, training large language models like BERT can easily exceed the available memory on a single GPU.]]></summary></entry><entry><title type="html">Knowledge Distillation as Self-Supervised Learning</title><link href="https://rohitbandaru.github.io/blog/knowledge-distillation-ssl/" rel="alternate" type="text/html" title="Knowledge Distillation as Self-Supervised Learning"/><published>2022-01-11T00:00:00+00:00</published><updated>2022-01-11T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/knowledge-distillation-ssl</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/knowledge-distillation-ssl/"><![CDATA[<p>Self-supervised learning (SSL) methods have been shown to effectively train large neural networks with unlabeled data. These networks can produce useful image representations that can exceed the performance of supervised pretraining on downstream tasks. However, SSL is not effective with smaller models. This limits applications where computational power is limited, such as edge devices. Knowledge distillation (KD) is a popular method to train a smaller student network from a larger and more powerful teacher network. The <a href="https://arxiv.org/abs/2101.04731">SEED</a> paper by Fang et al., published in ICLR 2021, applies knowledge distillation to self-supervised learning to pretrain smaller neural networks without supervision. In this post, we will discuss self-supervised learning and knowledge distillation and how they are unified in SEED.</p> <h1 id="self-supervised-learning">Self-supervised Learning</h1> <p>Self-supervised learning is a form of unsupervised learning. Self-supervision refers to labels that are generated from the data itself rather than manual annotations (ex: images vs class labels). Different SSL methods have different tasks that are used for the self-supervision.</p> <p>In computer vision, it is very common to pretrain a neural network on ImageNet classification. This is an example of supervised pretraining. This network can then be fine-tuned for various downstream tasks, such as semantic segmentation, object detection, or even medical image classification. Supervised pretraining has been a standard practice in computer vision.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/sl_vs_ssl-480.webp 480w,/assets/img/blog/distillation_ssl/sl_vs_ssl-800.webp 800w,/assets/img/blog/distillation_ssl/sl_vs_ssl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/sl_vs_ssl.png" width="400" height="auto" alt="Self-supervised vs Supervised Pretraining" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Self-supervised vs Supervised Pretraining </figcaption> </figure> <p>Self-supervised learning provides an alternative to supervised pretraining with two main benefits:</p> <ol> <li> <p>Generalizability: A supervised objective like classification can limit what a model learns about data. This is because not all the information in an image is needed for classification. For example, you can train a network to classify cats and dogs. The color of the animal’s fur is not relevant to this objective. Therefore, representations from this network may not be useful for a downstream task of fur color classification.</p> </li> <li> <p>Unlabeled data: The amount of available unlabeled data dwarfs labeled datasets. SSL is a form of unsupervised learning. It can leverage datasets of billions of images rather than be limited to supervised datasets, such as ImageNet which has about one million images.</p> </li> </ol> <p>There are many methods of SSL. Most of the recent state of art methods implement a form of contrastive learning. This includes <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, <a href="https://arxiv.org/abs/2006.09882">SwAV</a>, and <a href="https://arxiv.org/abs/1911.05722">MoCo</a>. In contrastive learning, representations are pushed towards positive examples and away from negative examples. In SSL, the positive examples are variations of the original image and the negative examples are from other images in the dataset. Contrastive SSL methods share some common steps:</p> <ol> <li> <p>Image augmentation: In supervised learning, augmentations, such as random cropping, flipping, and color distortions are used to generate more training data. In SSL, augmentation is used to produce positive examples. It is needed to avoid the trivial solution of encoding raw pixel values without learning anything about the content of the image.</p> </li> <li> <p>Contrastive loss: The goal of contrastive learning is to push positive examples closer together and negatives apart. It is most common to see a version of the <a href="https://arxiv.org/abs/1807.03748">InfoNCE</a> loss. This loss (defined below) is meant to maximize the similarity of a data point with one positive example, and minimize the similarity with many negative examples. The similarity function \(s\) is usually just the dot product.</p> </li> <li> <p>Negative samples: SSL needs a large amount of negative examples for the best performance. We want to push an image representation away from all other possible image representations from the dataset. This can be accomplished by having a large batch size (SimCLR). All the other images in the batch will be negative examples. An alternative is to keep negative examples in memory through multiple training batches. MoCo does this by keeping a queue of the most recent image representations. It is preferred to keep recent image representations, since the network changes gradually over time. Recent representations are more similar to representations that would be generated from the current network. The queue essentially approximates a large training batch.</p> </li> </ol> \[\begin{equation} \mathcal{L}_{\mathrm{InfoNCE}} = -\mathbb{E} \left[ \mathrm{log} \frac{ \exp(s(x, y)) } { \sum_{y_j} \exp(s(x,y_j)) } \right] \end{equation}\] <h2 id="moco">MoCo</h2> <p><a href="https://arxiv.org/abs/1911.05722">MoCo</a> (momentum contrast) by He et al. implements contrastive SSL by keeping a queue of examples. The queue allows for a large number of negative examples to be used in the contrastive loss The momentum encoder is trained at the same time as the encoder in a bootstrapped fashion. They must have identical architectures for the momentum update to occur.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/moco-480.webp 480w,/assets/img/blog/distillation_ssl/moco-800.webp 800w,/assets/img/blog/distillation_ssl/moco-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/moco.png" width="700" height="auto" alt="MoCo training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>With a queue of representations encoded by the momentum encoder, the main encoder is trained to contrast the representations. \(q\) is the query or the representation from the encoder. \(k_+\) is the corresponding representation from the momentum encoder. The loss aims to push \(q\) towards \(k_+\) and away from all other representations \(k\) in the queue which serve as negative examples.</p> \[\begin{equation} \mathcal{L}_i = -\log\frac{\exp(q*k_+/\tau)}{\sum_{i=0}^K\exp(q*k_i/\tau)}\\ \end{equation}\] <p>MoCo is very effective in pretraining large neural networks for many downstream tasks. SEED aims to extend this for smaller networks.</p> <h1 id="knowledge-distillation">Knowledge Distillation</h1> <p>In knowledge distillation (<a href="https://arxiv.org/abs/1503.02531">Hinton et al.</a>, <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">Buciluǎ et al.</a>), a large teacher model is used to train a smaller and more efficient student model. It is useful in the case that a large neural network can perform well on a task, but a small network cannot be directly trained to high accuracy. This makes it relevant to SSL, where only large neural networks have strong performance.</p> <p>In supervised learning for classification, the labels are hard targets or one-hot encoded vectors. All of the probability is assigned to one class, and all other classes have a value of zero. The teacher model will have a softmax layer which will return a soft target. The soft target will assign some probability to other classes. Knowledge distillation uses the teacher network to produce these soft targets and uses them to train the student model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/soft_vs_hard-480.webp 480w,/assets/img/blog/distillation_ssl/soft_vs_hard-800.webp 800w,/assets/img/blog/distillation_ssl/soft_vs_hard-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/soft_vs_hard.png" width="600" height="auto" alt="dark knowledge" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Shiba Inu dogs are known to have cat-like characteristics, soft labels can encode this by assigning some probability to the cat class. </figcaption> </figure> <p>The soft targets encode more information than hard targets. Hinton describes this as “dark knowledge”. For example, from a soft target you can tell which class is the second most likely or the relative probabilities between two classes. This information is not available in a hard target.</p> \[\begin{equation} p_i = \frac{\exp(\frac{z_i}{T})}{ \sum_{j} \exp(\frac{z_j}{T})}\\ \end{equation}\] <p>The soft targets can be made softer by increasing the temperature of the softmax. The temperature \(T\) is typically set to 1. However, in knowledge distillation higher temperatures can yield better results as it increases the magnitude of the non-max values.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/kd-480.webp 480w,/assets/img/blog/distillation_ssl/kd-800.webp 800w,/assets/img/blog/distillation_ssl/kd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/kd.png" width="700" height="auto" alt="Knowledge Distillation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Knowledge Distillation </figcaption> </figure> <ol> <li> <p>A teacher model is trained for high accuracy. This can be a large neural network or an ensemble.</p> </li> <li> <p>The teacher model generates soft labels for a dataset. This dataset can be the same or different from the hard labeled dataset.</p> </li> <li> <p>The student network is trained to predict the soft labels. It can also be simultaneously trained with hard labels in a separate loss term.</p> </li> </ol> <p>Distillation can use unlabeled data. Once a model is trained, it can be used to produce soft labels for a large unsupervised dataset. This can be larger than the initial labeled dataset and effectively train the student network on a much larger dataset.</p> <h1 id="knowledge-distillation-for-ssl">Knowledge Distillation for SSL</h1> <p>Knowledge distillation aims to transfer dark knowledge between models. Self-supervised learning aims to increase the dark knowledge learned by a model. When training a model on a supervised classification objective, it will not need to learn information that does not help with classification. The objective limits what the model learns. Self-supervised learning methods are designed to be general and not task specific.</p> <p>SSL does not perform well with smaller models which limits its applicability. Also, the downstream task is likely less complex than the SSL task and can be achieved more efficiently with a smaller model. Knowledge distillation offers a way to reduce the size of the model while maintaining accuracy and relevant knowledge.</p> <p>One way to apply KD to SSL is to train the teacher on a SSL objective and then apply KD to train the student on a downstream task. This would require first fine-tuning the teacher network on the downstream test, with a new output layer. It would be more efficient to distill knowledge to a smaller network before training on the downstream task.</p> <p>Although it is simple to apply knowledge distillation on a supervised downstream task, you cannot directly apply it to the self-supervised training objective. This is because SSL models do not output classification predictions. SSL models output feature representations of the input. Training a student network to match these feature representations would not be effective. Self-supervised training involves optimizing with an objective on top of the representations.</p> <h1 id="seed">SEED</h1> <p>In the <a href="https://arxiv.org/abs/2101.04731">SEED</a> paper, the authors propose a self-supervised approach to knowledge distillation. It uses a contrastive objective on the representations.</p> <p>This will allow knowledge distillation to occur before the downstream task. The method produces an SSL trained student network that can be efficiently fine-tuned on downstream tasks. SEED extends self-supervision to smaller models allowing us to compress SSL models to use in more applications.</p> <h2 id="method">Method</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/seed-480.webp 480w,/assets/img/blog/distillation_ssl/seed-800.webp 800w,/assets/img/blog/distillation_ssl/seed-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/seed.png" width="100%" height="auto" alt="SEED" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> SEED <a href="https://arxiv.org/abs/2101.04731"> Source </a> </figcaption> </figure> <ol> <li> <p>Train the teacher, independent of the student network. Any of the recent state-of-the-art SSL methods or even supervised models (ResNet trained on ImageNet classification) can be used here. The only requirement is that the model must produce image representations. The teacher networks weights are then frozen.</p> </li> <li> <p>Apply an augmentation to the input image. The same augmentation of the image is used for both the student and the teacher networks. In most other SSL methods, different augmentations would be used. SEED reports better performance when using the same augmentation. This may be because trivial solutions are avoided by the pretraining of the teacher network.</p> </li> <li> <p>Input the image to both the student and teacher networks to get two vector representations: \(Z^S\) and \(Z^T\).</p> </li> <li> <p>Add teacher vector \(Z^T\) to the instance queue \(D\) which is a fixed size FIFO queue that persists between training batches. Self-supervised learning in general benefits from a large number of negative examples.</p> </li> <li> <p>Apply the self-supervised SEED loss, using the student and teacher vectors, and the instance queue. The student and teacher vectors are each compared to every embedding in the queue, to produce two similarity vectors. A cross-entropy loss is applied between the similarity vectors The student network is trained to produce vectors that have the same similarities as the teacher. We will further explain the loss used in SEED.</p> </li> </ol> <h2 id="loss">Loss</h2> <p>In self-supervised learning, a supervised objective is formed from the input rather than human annotations. In this case, the supervised objective is predicting the current image representation from a queue containing the current representation and negative examples. Knowledge distillation is applied with respect to this objective. The scores from applying the softmax to the teacher similarity vector form the soft label.</p> <p>The cross-entropy loss is used like the contrastive InfoNCE loss in SSL. The student vector is pushed towards the teacher vector and away from the vectors in the queue. However, some of the negative vectors are closer than others. The student network is also trained to match this information. This is where the dark knowledge of KD is applied.</p> <p>Unlike the InfoNCE loss, there are no hard positive and negative examples in this objective. The teacher network creates a soft probability distribution. Each example is assigned a continuous score between 0 and 1 that indicates how positive the example is. SEED can be viewed as a <em>soft contrastive learning</em> method.</p> \[\begin{align} \mathcal{L}_{SEED} &amp;= - \sum_i^N \textbf{p}^T(\textbf{x}_i; \theta_T, \textbf{D}^+) * \log \textbf{p}^S(\textbf{x}_i; \theta_S, \textbf{D}^+) \\ &amp;= - \sum_i^N \sum_j^{K + 1} \frac{\exp(\textbf{z}_i^T * \textbf{d}_j / \tau^T)}{\sum_{d\sim\textbf{D}^+}\exp(\textbf{z}_i^T * \textbf{d} / \tau^T)} * \log \frac{\exp(\textbf{z}_i^S * \textbf{d}_j / \tau^S)}{\sum_{d\sim\textbf{D}^+}\exp(\textbf{z}_i^S * \textbf{d} / \tau^S)} \end{align}\] <p>For each example in the batch (size \(N\)), two similarity functions are applied: one using the teacher network \(p^T\) and one using the student network \(p^S\). The similarity function is applying an inner product and softmax with the vectors to the instance queues. This produces a probability distribution with more probability on examples in the queue that are close to the input. Since we want these probability distributions to match, a cross entropy loss is applied between the two probability distributions.</p> \[\mathcal{L}_{cross-entropy} = \sum_{i} y_i * \log(\hat{y}_i)\] <p>Referring to the formula for cross-entropy. \(\textbf{p}^T(..)\) corresponds to the label \(y_i\). In classification, \(y_i\) would be binary or a one-hot encoded vector. In this case, \(\textbf{p}^T(..)\) is a score between 0 and 1. As in KD, this is a soft label. With hard labels and standard contrastive learning, the scores would be binary with a 1 for the current datapoint. \(\textbf{p}^S(..)\) corresponds to the prediction \(\hat{y}_i\). Here the prediction is the student similarity score. We want the student to produce similarity scores matching the teacher.</p> <h2 id="seed-vs-moco">SEED vs MoCo</h2> <p>SEED is trained very similarly to MoCo. The differences are the lack of momentum weight updates and the soft contrastive loss.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/seed2-480.webp 480w,/assets/img/blog/distillation_ssl/seed2-800.webp 800w,/assets/img/blog/distillation_ssl/seed2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/seed2.png" width="700" height="auto" alt="SEED training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> SEED training </figcaption> </figure> <h1 id="self-supervised-vs-supervised-knowledge-distillation">Self-supervised vs Supervised Knowledge Distillation</h1> <p>SEED or self-supervised distillation in general does not aim to replace supervised knowledge distillation. The authors report their best results when training models with both self-supervised and supervised knowledge distillation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/s_vs_sl_kd-480.webp 480w,/assets/img/blog/distillation_ssl/s_vs_sl_kd-800.webp 800w,/assets/img/blog/distillation_ssl/s_vs_sl_kd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/s_vs_sl_kd.png" width="900" height="auto" alt="Self-supervised KD with Supervised KD" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Self-supervised KD with Supervised KD </figcaption> </figure> <p>SEED allows for more effective self-supervised training of smaller models. It is better to train a large model with SSL and distill it to a small model than to train the small model directly. After SEED pretraining, the model can be fine-tuned with supervised knowledge distillation with the downstream task. In this step, the student is initialized from the self-supervised KD trained model, instead of initializing from scratch.</p> <h1 id="conclusion">Conclusion</h1> <p>Self-supervised knowledge distillation allows the impressive gains of large SSL models to be transferred to smaller neural networks. This allows for more applications of these models. We can even view knowledge distillation as a form of self-supervised learning. Hard labels are not used in SSL. The soft labels provide self-supervision since they are produced from the data.</p> <p>SEED essentially adapts momentum contrast to be used as knowledge distillation. An interesting future direction would be adapting other SSL methods such as SimCLR to be used as knowledge distillation. Nearly every contrastive SSL method can be adapted in this way.</p>]]></content><author><name></name></author><category term="paper-review"/><category term="self-supervised-learning"/><category term="knowledge-distillation"/><category term="computer-vision"/><summary type="html"><![CDATA[Self-supervised learning (SSL) methods have been shown to effectively train large neural networks with unlabeled data. These networks can produce useful image representations that can exceed the performance of supervised pretraining on downstream tasks. However, SSL is not effective with smaller models. This limits applications where computational power is limited, such as edge devices. Knowledge distillation (KD) is a popular method to train a smaller student network from a larger and more powerful teacher network. The SEED paper by Fang et al., published in ICLR 2021, applies knowledge distillation to self-supervised learning to pretrain smaller neural networks without supervision. In this post, we will discuss self-supervised learning and knowledge distillation and how they are unified in SEED.]]></summary></entry><entry><title type="html">Self-Supervised Learning  -  Getting more out of data</title><link href="https://rohitbandaru.github.io/blog/Self-Supervised-Learning/" rel="alternate" type="text/html" title="Self-Supervised Learning  -  Getting more out of data"/><published>2021-08-14T00:00:00+00:00</published><updated>2021-08-14T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Self-Supervised-Learning</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Self-Supervised-Learning/"><![CDATA[<p>Yann LeCun <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">describes</a> self-supervised learning as the next big challenge in the field of AI. How does it work? Self-supervised learning (SSL) is a specific type of unsupervised learning. It aims to learn from large datasets of unlabeled data to enable building more robust models in different domains such as vision and NLP.</p> <p>For many computer vision problems, it is a common practice to pretrain the model on a supervised learning task. For example, there are <a href="https://keras.io/api/applications/">many</a> neural networks that are pretrained to do image classification on ImageNet. However, self-supervised learning has recently been shown to outperform supervised pretraining learning on certain tasks. SSL is an active area of research with heavy involvement from top AI labs in Google, Facebook, Deepmind, and academia. Rather than focusing on the details of SSL architectures, we will explore the intuitions on why it works and what is needed.</p> <h1 id="code">Code</h1> <p>In order to make it easy to directly interact with SSL, I wrote a Colab notebook showing a few algorithms. This notebook demonstrates transfer learning on CPC, SwAV, and SimCLR pretrained models on the CIFAR10 classification task. This uses PyTorch Lightning’s implementations of these algorithms.</p> <p><a href="https://colab.research.google.com/drive/1PDCTe5dIQgYyiuLQw3WGyCuT03gX1qZR?usp=sharing"> <img src="/blog/assets/img/colab.svg" alt="Open In Colab"/> </a></p> <p>We are experimenting with the simple example of pretraining on ImageNet and evaluating on CIFAR10 classification. SSL can be effective on other datasets and learning tasks (object detection, segmentation, etc.), but these won’t be the focus of this post.</p> <h1 id="motivations">Motivations</h1> <h2 id="data">Data</h2> <p>Self-supervised learning does not need labels. The amount of unlabeled data generally far exceeds the amount of labeled data. SSL can leverage large amounts of unlabeled data to build powerful models. Although most research does not use datasets larger than ImageNet, there are real world applications of using larger unlabeled datasets. For example, Facebook/Meta can train the <a href="https://ai.facebook.com/blog/seer-the-start-of-a-more-powerful-flexible-and-accessible-era-for-computer-vision/">SEER</a> model on billions of Instagram images.</p> <h2 id="generalizability">Generalizability</h2> <p>If you train a model on image classification, it may not perform as well on non-classification tasks. This is because only part of the image’s information is needed to classify it. A self-supervised learning algorithm may be able to use more of the information in the data.</p> <p>The reason for the generalization gap is that the classification task does not always require a strong understanding of the object. For example, if you trained a supervised model to classify dog breeds, it may only look at the texture and color of the dog’s fur. In order to classify the breeds, the network may not need to understand other characteristics of the dog, such as size and facial features. This model would then not generalize well if you want to add a new dog breed with an indistinctive skin pattern. It will also not generalize well to new tasks like classifying the size or shape of the dog.</p> <h2 id="better-performance">Better Performance</h2> <p>It is common to think that unsupervised / self-supervised learning is only useful when you lack labels to do supervised learning. However, these approaches can actually increase performance compared to a fully supervised approach. The ability to learn more accurate and robust models is what gives self-supervised learning the potential to shift the field of AI.</p> <p>In research, there are comparisons between training on ImageNet images and labels with supervised learning and ImageNet with only images for self-supervised learning. Although the motivation for SSL is often framed as being able to use more data, in this case, the size of the dataset is the same. The ability to use larger unlabeled datasets is just a side benefit of SSL.</p> <h1 id="vision-vs-nlp">Vision vs NLP</h1> <p>Self-supervised learning has been long applied in NLP, but as Yann LeCun and Ishan Misra point <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">out</a>, it is much harder to apply to vision. In NLP, language models are often trained with self supervision. Given a some text, you can mask a word and try to predict it given the rest of the text. There is a limited vocabulary, so you can assign a probability to each word. This is the basis of many popular NLP methods.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/nlp-480.webp 480w,/assets/img/blog/self_supervised_learning/nlp-800.webp 800w,/assets/img/blog/self_supervised_learning/nlp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/nlp.png" class="img-fluid mx-auto d-block" width="400" height="auto" alt="Predicting masked words in NLP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Predicting masked words in NLP </figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/image_patch-480.webp 480w,/assets/img/blog/self_supervised_learning/image_patch-800.webp 800w,/assets/img/blog/self_supervised_learning/image_patch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/image_patch.png" class="img-fluid mx-auto d-block" width="300" height="auto" alt="Image SSL with patches" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Predicting patches of an image is much harder. </figcaption> </figure> <p>The analogue for vision is to mask a patch of an image and try to fill it in. However, because there is an intractable number of possible ways to fill in an image, you can’t compute a probability for each one. There can also be a large number of possible solutions. For example, in the image above, there is many facial expressions the dog could have. The NLP approach is straight forward but cannot be directly applied to vision.</p> <h1 id="pretext-task">Pretext Task</h1> <p>The earlier approaches to self-supervised learning focused on training the network on a pretext task. This task would not require labels in the label. The labels will be made up through the task. In <a href="https://arxiv.org/abs/2012.01985">RotNet</a>, each image is rotated by 0, 90, 180, or 270 degrees, and a network is trained to predict the rotation. In <a href="https://arxiv.org/abs/1603.09246">Jigsaw</a>, the image is split up into patches and scrambled like a jigsaw puzzle. A network is then trained to solve the puzzle by predicting the permutation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/rotnet-480.webp 480w,/assets/img/blog/self_supervised_learning/rotnet-800.webp 800w,/assets/img/blog/self_supervised_learning/rotnet-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/rotnet.png" width="100%" height="auto" alt="RotNet, SSL by predicting rotations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1803.07728"> Source </a> </figcaption> </figure> <p>The problem with pretext task-based SSL is the same as supervised learning. There can be shortcuts to achieve high accuracy on the task. There have been attempts to avoid this. For example, in Jigsaw, each path is randomly cropped, so the task can’t be solved by simply lining up edges. However, the limitations still exist regardless, so more recent research has focused on contrastive learning.</p> <h1 id="contrastive-learning">Contrastive Learning</h1> <p>A neural network outputs a vector representation for every image. The goal of contrastive learning is to push these vectors closer for similar images and pull them apart unrelated images. This in different ways in different research papers.</p> <h2 id="cpc"><a href="https://arxiv.org/abs/1807.03748">CPC</a></h2> <p>Contrastive Predictive Coding is a method developed by Deepmind. It is a generic approach that can be applied to any data modality. In the paper, it is applied to images, audio, and text. It is a very general framework with two main components: an encoder, and an autoregressive model. These can be anything and are designed to fit the domain.</p> <p>The encoder simply encodes the data into a lower-dimensional vector \(z_t\). This can be any model. For images, this can be a convolutional neural network.</p> <p>Autoregressive models the variables in the data are given an order. In images, the pixels can be ordered from left to right and top to bottom. We can imagine unrolling each datapoint (ex: image, audio clip) into a list. We can call each element of this list an observation. CPC encodes a sequence of observations \(X\) into a sequence of encodings \(Z\).</p> \[X = [x_1, x_2, x_3, x_4 ... x_N]\\ Z = [z_1, z_2, z_3, z_4 ... z_N]\\ z_t = g_{enc}(x_t)\] <p>The prediction of an observation in the sequence depends only on the previous observations. This similar to predicting the future from the past in a time series. In CPC, the autoregressive model is used to generate context vectors from the encodings \(z_t\). Context vector \(c_t\) is a function of encodings \(z_{\leq t}\), but not any encoding after \(z_t\). Note that the autoregressive model is trying to predict the encodings of the observations, but not the observations themselves. The architecture of this autoregressive model depends on the application.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/cpc-480.webp 480w,/assets/img/blog/self_supervised_learning/cpc-800.webp 800w,/assets/img/blog/self_supervised_learning/cpc-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/cpc.png" width="100%" height="auto" alt="CPC" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> CPC applied to audio, \(g_{enc}\) is the encoder, \(g_{ar}\) is the autoregressive model <a href="https://arxiv.org/abs/1807.03748"> Source </a> </figcaption> </figure> <p>With these two models, we can generate an encoding of the data and context vectors. These vectors can be used as representations of the data. But how are these models trained? The self-supervised task is essentially predicting the input from the context. For example, given \(c_t\), we want to be able to go backwards and identify that it was generated from \(x_t/z_t\). The models are trained on a contrastive InfoNCE loss.</p> \[\begin{equation} \mathcal{L}_{\mathrm{InfoNCE}} = -\mathbb{E} \left[ \mathrm{log} \frac{ s(x, y) } { \sum_{y_j} s(x,y_j) } \right] \end{equation}\] <p>\(x\) is the sample we are trying to predict. \(c\) is the correct context. \(c_j\) are the context vectors for the negative samples. The negative samples come from other observations of the same datapoint and other datapoints in the batch. We want to maximize \(s(x,c)\) and minimize the sum of \(s(x, y_j)\). This is contrastive in that we are pushing \(y\) to be close to \(x\), and all other \(y_j\) to be far from \(x\).</p> \[\begin{equation} f_k(x_{t+k},c_t) = \mathrm{exp} \left( (g_{enc}(x_{t+k}))^TW_kc_t \right) = \mathrm{exp} \left( z_{t+k}^TW_kc_t \right) \end{equation}\] <p>The \(s\) function is modeled by \(f_k\) a log bilinear model. \(W_k\) is linear transforms the context vector, which can then be compared with the encoding \(z\).</p> <p>To apply this to vision, the image is split up into 7x7 patches (with 50% overlap) which will be considered the observations. Each patch is encoded by a CNN (ResNet without pretraining). If the encoding returns at 1024 dimensional vector, the encoded image will have a size of 7x7x1024. An autoregressive model (<a href="https://arxiv.org/abs/1606.05328">PixelCNN</a> or <a href="https://arxiv.org/abs/1601.06759">PixelRNN</a>) is applied to the encodings of the patches. For 1D data like audio, an RNN/LSTM scan be used. The self-supervised task in this case is predicting which patch generated each context vector. Refer to the PixelRNN paper for more information on autoregressive models and PixelCNN. The final representation is computed by mean pooling the encodings into a single 1024 dimensional vector. This can then be used for downstream tasks, like image classification.</p> <p>Why do we need the autoregressive model? We could optimize the InfoNCE loss using the 7x7 encodings. The self supervised task here is predicting the next context vector given a sequence of context vectors. This is similar to predicting the next patch of an image given all the previous patches. But rather predict the patch, which as we discussed is too difficult, we just predict a lower dimensional vector. Without this autoregressive constraint, we are just optimizing for generating unique embeddings for each patch and ignoring the relation between the patches. The InfoNCE loss is just ensuring that the predictions are correct.</p> <p>Why not just mask out the current context / observation? The architecture for this may be a masked fully connected layer that learns the context vector for each observation, while excluding the connection to the observation itself. Or there could be two PixelCNNs, one from the left and one from the right. We can then concatenate these two context vectors and possibly add additional neural network layers on top of it. Both methods would be more computationally expensive and complex, but likely still feasible. This would be bidirectional model for images similar to <a href="https://arxiv.org/abs/1810.04805">BERT</a>. This idea may be explored in other research papers or, it may be an open idea to try.</p> <h2 id="simclr"><a href="https://arxiv.org/abs/2002.05709">SimCLR</a></h2> <p>SimCLR is a method from Google Brain which takes a different approach for self-supervised learning of image representations. The basis of SimCLR is image augmentations. Image augmentation has long been used in supervised learning. The augmentations are transformations applied to the image and cropping, color change, and rotation. The idea is that these transformations do not change the content of the image and the network will learn to ignore and be invariant to these transformations. In supervised learning, data augmentation is used to just increase the size of the dataset for a supervised task like classification. Many SSL methods including SimCLR make invariance to the augmentation the actual learning objective. The augmented images are fed into an encoder to get the representation. These representations are then learned to be close of augmentations of the same image.</p> <p>However, the problem with just comparing within the same image is collapse. The network would learn the trivial solution of a constant vector for all representations (ex: a vector of all zeros). This would maximize the similarity between augmentations but obviously not contain any useful images. We need negative samples to minimize similarity with. In SimCLR, the negative samples are augmentations of other images from the same training batch. The assumption made here is that the other images are unrelated to the current image and the representations should be far apart.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/simclr_arch-480.webp 480w,/assets/img/blog/self_supervised_learning/simclr_arch-800.webp 800w,/assets/img/blog/self_supervised_learning/simclr_arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/simclr_arch.png" width="500" height="auto" alt="SimCLR architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> The architecture of SimCLR. Diagram by Author, but dog images from SimCLR paper </figcaption> </figure> <p>Why do we need a projection head? It would not change the architecture much by optimizing the similarity losses on the representations themselves. The encoder may even include the same fully connected layers that would have been in the projection head. The projection head allows for a more complex and nonlinear similarity relationship between the encodings. Without it, the representations would have to have a high cosine similarity. This may restrict the expressivity of the vectors. The projection head can also ignore some information in the representations. For example, SimCLR may train to make the representations invariant to rotations. The rotation angle may be encoded in the representation but ignored by the projection head. If the rotation is encoded in the first 5 values of the vector, the projection MLP may have zero weights for those values. This may be desirable in a variant of the architecture in which the self-supervised learning happens simultaneously with a downstream task. The SimCLR architecture itself has no reason to include unnecessary information in the representation. It is unclear whether having “extra” information in the representation is desirable or not.</p> <p>Projection heads are very common in self-supervised learning. The autoregressive model in CPC can be viewed as a projection head.</p> <p>Aggressive augmentation yields the best results. This means applying multiple augmentations at a time. This makes the contrastive learning more challenging and forces the network to learn more about the image. The augmentations also avoid trivial solution to the contrastive objective. Without cropping, the network can match two augmented images by their local features (edges in the same place), instead of learning global features. Without color distortion, images can be matched by their color distribution. These augmentations can be composed with others, such as rotation and blur.</p> \[\begin{equation} \ell_{i,j} = \log{\frac{\exp(\mathrm{sim}(z_i,z_j)/\tau))}{\sum_{k=1}^{2N}\mathbb{1}_{[k\neq i]}\exp(\mathrm{sim}(z_i,z_k)/\tau)}} \end{equation}\] <p>The loss is referred to as NT-Xent (the normalized temperature-scaled cross entropy loss). The similarity function \(s\) can simply be cosine similarity (\(\frac{u^\top v}{\|u\|\|v\|}\)). This loss is similar to the InfoNCE loss. The main difference is the temperature \(\tau\). The temperature essentially controls how strongly should attract and repel the other vectors in the loss.</p> <h2 id="scaling">Scaling</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/scaling-480.webp 480w,/assets/img/blog/self_supervised_learning/scaling-800.webp 800w,/assets/img/blog/self_supervised_learning/scaling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/scaling.png" width="500" height="auto" alt="SSL scaling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2002.05709"> Source </a> </figcaption> </figure> <p>Self-supervised learning is often evaluated on ImageNet classification. The projection head is replaced with a linear layer. The network is then trained to classify ImageNet with the encoder weights frozen. The encodings learned with self-supervision must be useful enough for a linear layer to classify them.</p> <p>An interesting property of self-supervised trained encoders, is how the scale in terms of depth and width. We see that only SimCLR(4x) is able to match the accuracy of fully supervised learning. “4x” means the network is 4 times as wide and as deep. It is not necessarily a bad thing that SSL requires a much larger network for ImageNet classification. This likely means the network is learning more information from the data than what is needed for supervised learning. Although this doesn’t help with ImageNet classification, the vectors may be more effective in other downstream tasks.</p> <p>One issue with SimCLR is its reliance on huge batch sizes. The best results come from a batch size of 4096. It needs many negative samples to be effective. This makes the network inefficient to train. Other approaches attempt to address this problem.</p> <h2 id="byol"><a href="https://arxiv.org/abs/2006.07733">BYOL</a></h2> <p>BYOL is a paper from Deepmind that aims to remove the need for negative samples. There are two networks: a target network and an online network. The target network’s weights are an exponential moving average of the online encoder. Similar to SimCLR, augmented versions of an image are passed through the encoders. Unlike SimCLR, the loss does not use negative examples so there is no need for large batch sizes. There is a projection head on top of the online encoder. The online encoder is used for downstream tasks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/byol-480.webp 480w,/assets/img/blog/self_supervised_learning/byol-800.webp 800w,/assets/img/blog/self_supervised_learning/byol-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/byol.png" width="100%" height="auto" alt="BYOL architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> BYOL architecture <a href="https://arxiv.org/abs/2006.07733"> Source </a> </figcaption> </figure> <p>Bootstrapping is a poorly defined word used in machine learning. It can mean simultaneously optimizing two objectives that depend on each. In BYOL, that refers to the two encoders.</p> <p>BYOL is able to learn useful representations without collapse because only the parameters of the online encoder are optimized. The online encoder can’t learn to output a constant because it is following the representations of the target encoder. The bootstrapping ensures that the trivial solution is avoided.</p> <p>BYOL is a non-contrastive method of SSL. However one criticism of BYOL is that batch normalization causes implicit contrastive learning by leaking information between batch elements. However, in a <a href="https://arxiv.org/abs/2010.10241">follow up paper</a>, the authors show that replacing batch normalization with group normalization and weight standardization leads to comparable performance.</p> <h1 id="clustering">Clustering</h1> <p>Clustering is an important class of unsupervised learning algorithms. Although more often used outside of deep learning, clustering can be applied to self supervised learning. Feature vectors can be clustered. Clusters can indicate a group of related images. In this sense clusters are similar to classes and can be used as labels in SSL.</p> <h2 id="deepcluster"><a href="https://arxiv.org/abs/1807.05520">DeepCluster</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/deepcluster-480.webp 480w,/assets/img/blog/self_supervised_learning/deepcluster-800.webp 800w,/assets/img/blog/self_supervised_learning/deepcluster-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/deepcluster.png" width="100%" height="auto" alt="DeepCluster Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> DeepCluster architecture <a href="https://arxiv.org/abs/1807.05520"> Source </a> </figcaption> </figure> <p>DeepCluster trains a neural network in two alternating steps: clustering and classification. In the clustering step, each image is assigned a cluster as a pseudolabel by clustering the feature vectors from the network. K-means is used for clustering. There are \(k\) clusters of the same dimension as the feature vectors. The network is then trained to predict the clusters from the images. After training on this classification objective, the features improve. The dataset is reclustered with better clusters. This iterative training procedure improves the clusters and the representations.</p> <p>The main problem with DeepCluster is that it requires periodically clustering the entire dataset. This limits this method in scaling to extremely large datasets. This is addressed by SwAV with an online approach to clustering based SSL.</p> <h2 id="swav"><a href="https://arxiv.org/abs/2006.09882">SwAV</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/swav-480.webp 480w,/assets/img/blog/self_supervised_learning/swav-800.webp 800w,/assets/img/blog/self_supervised_learning/swav-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/swav.png" width="500" height="auto" alt="SwAV" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> SwAV <a href="https://arxiv.org/abs/2006.09882"> Source </a> </figcaption> </figure> <p>SwAV extends on DeepCluster to be online, while also taking inspiration from contrastive SSL methods. Two augmentations of an image are passed to an encoder. These representations are then assigned prototypes. There are K prototypes, which are vectors of the same representation as the encoding.</p> <h1 id="conclusion">Conclusion</h1> <p>There are many approaches to self supervised learning, however there are common elements. There are contrastive losses, data augmentation, bootstrapping, projection heads, and sometimes negative samples.</p>]]></content><author><name></name></author><category term="computer-vision"/><category term="self-supervised-learning"/><summary type="html"><![CDATA[Yann LeCun describes self-supervised learning as the next big challenge in the field of AI. How does it work? Self-supervised learning (SSL) is a specific type of unsupervised learning. It aims to learn from large datasets of unlabeled data to enable building more robust models in different domains such as vision and NLP.]]></summary></entry><entry><title type="html">Domain Adaptation</title><link href="https://rohitbandaru.github.io/blog/Domain-Adaptation/" rel="alternate" type="text/html" title="Domain Adaptation"/><published>2021-08-09T00:00:00+00:00</published><updated>2021-08-09T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Domain-Adaptation</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Domain-Adaptation/"><![CDATA[<p>Machine learning performance depends on the dataset that it is trained on. Datasets are imperfect, so problems in the data affect the models. One type of problem is domain shift. This means that a model trained to learn a task on one dataset, may not be able to perform the same task on a slightly different dataset.</p> <p>Say you train a model to detect dogs in outdoor settings like public parks. It may perform very well on test images of dogs in outdoor places. However, that model may not function well when trying to detect dogs indoors, although the task itself is identical. This is a problem because the background of the image should not matter since you are just trying to detect dogs. We will explore four different research papers that address this problem.</p> <h1 id="vocabulary">Vocabulary</h1> <p>There are two datasets: a source dataset and a target dataset. The dataset that the model is trained on is the source dataset. The target dataset is the one that it will be tested on.</p> <p>For domain generalization, a similar problem, the target dataset is not available during training. The network is trained on the source dataset to not overfit to the domain-specific features.</p> <p>In domain adaptation, both the source and target datasets are available during training, but labels for the target dataset are not always available. For unsupervised domain adaptation, there are no labels available for the target dataset during training time. Semi-supervised domain adaptation involves a few labeled examples from the target dataset. With supervised domain adaptation, all the data from both the source and target datasets have labels.</p> <p>Unsupervised domain adaptation is the most commonly studied problem, as it has the most applications. Supervised DA can be useful when you have a labeled dataset, but it is too small to directly train on.</p> <p>These methods can be applied to many ML problems. However, a common application is image classification. I will focus on image classification on two common benchmark datasets: MNIST and SVHN. A model trained on handwritten digits (MNIST) often performs poorly on printed house number digits (SVHN).</p> <h1 id="adversarial-methods">Adversarial Methods</h1> <p>The most common approaches to the domain adaptation method follow an adversarial approach. For some context, I would suggest reading about <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29">Generative Adversarial Networks (GANs)</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/framework-480.webp 480w,/assets/img/blog/domain_adaptation/framework-800.webp 800w,/assets/img/blog/domain_adaptation/framework-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/framework.png" width="100%" height="auto" alt="Framework for Adversarial Domain Adaptation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Framework for Adversarial Domain Adaptation </figcaption> </figure> <p>There are two encoders, which learn to produce a vector representation of each input. There is a classifier to classify the inputs and a discriminator that is trained to differentiate between the datasets. The goal is to eliminate differences in the domain from the encodings. This is similar to the GAN objective in that we want the encoders to fool the discriminator by generating encodings that are difficult to differentiate. However, this needs to be done such that the classifier is also effective for both datasets. The same classifier can then be applied to both datasets.</p> <p>There are many approaches to this with different training methods, architectures, and losses. The high-level goal is consistent. We want the encoders to generate encodings that contain the useful information needed for classification but remove the shift in domains.</p> <p>The key difference between the many algorithms is what the discriminator is and how it is trained. In simple cases, it is just an additional loss term. For example, maximum mean discrepancy (MMD) measures the difference between the encodings of the source and target datasets. Training the networks while minimizing the discrepancy can reduce domain shift. This may be useful for simple DA problems but does not work well for larger disparities.</p> <h2 id="adda"><a href="https://arxiv.org/abs/1702.05464">ADDA</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/adda-480.webp 480w,/assets/img/blog/domain_adaptation/adda-800.webp 800w,/assets/img/blog/domain_adaptation/adda-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/adda.png" width="100%" height="auto" alt="The steps of ADDA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> The steps of ADDA <a href="https://arxiv.org/abs/1702.05464"> Source </a> </figcaption> </figure> <p>Adversarial Discriminative Domain Adaptation (ADDA) applies a simple approach to discriminative DA. There is only one encoder shared between the source and target datasets. The networks are trained in two steps.</p> <ol> <li> <p>The encoder and classifier are first trained to achieve high classification accuracy on the source dataset.</p> </li> <li> <p>The encoder is trained with the discriminator to lose domain discriminability. The discriminator is trained to classify the two domains with an adversarial loss. The encoder is trained with the negation of this loss since it is adversarial with respect to the discriminator. This negative is done through gradient reversal, which means in backpropagation, the gradients are negated before going to the encoder.</p> </li> </ol> <p>One major shortcoming of this approach is that the classification performance can be lost or forgotten in the adaptation step. This is because the labels are not used in this step.</p> <h2 id="dann"><a href="https://arxiv.org/abs/1505.07818">DANN</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/dann-480.webp 480w,/assets/img/blog/domain_adaptation/dann-800.webp 800w,/assets/img/blog/domain_adaptation/dann-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/dann.png" width="100%" height="auto" alt="DANN" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> DANN <a href="https://arxiv.org/abs/1505.07818"> Source </a> </figcaption> </figure> <p>Domain-Adversarial Training of Neural Networks (DANN) is very similar to ADDA. Rather than have a separate adaptation step, the domain discriminator is trained alongside the classier. A gradient reversal layer is used because the domain discriminator and the classier have adversarial loss functions. This allows classification and discrimination to be trained together and avoid the network from forgetting the task.</p> <h1 id="image-translation">Image Translation</h1> <p>Another approach to addressing the domain gap is to convert examples from one domain to another. An example of this is transforming street-view digits (SVHN) to look like handwritten MNIST (digits). After this translation, you can apply an MNIST trained image classifier. The architectures are more complex because, in addition to the main task (image classification), the networks must translate images to and from the source and target domains.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/translation-480.webp 480w,/assets/img/blog/domain_adaptation/translation-800.webp 800w,/assets/img/blog/domain_adaptation/translation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/translation.png" width="100%" height="auto" alt="SVHN DA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="image-to-image-translation"><a href="https://arxiv.org/abs/1712.00479">Image to Image translation</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/i2i-480.webp 480w,/assets/img/blog/domain_adaptation/i2i-800.webp 800w,/assets/img/blog/domain_adaptation/i2i-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/i2i.png" width="100%" height="auto" alt="I2I" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1712.00479"> Source </a> </figcaption> </figure> <p>Like the adversarial methods Image to Image Translation (I2I) aims to learn a domain invariant encoding (Z) for the images. There are six networks in this architecture: the source encoder, source decoder, target encoder, target decoder, domain discriminator, and task network (ex: classifier). The decoders aim to reconstruct the images from the encoding. This also includes adversarial learning with the domain discriminator.</p> <p>The network is trained on a weighted combination of six different losses. The paper studies which combination of losses yields the best performance.</p> <ol> <li> <p>Qc is the classification loss on the source domain. We cannot get this loss for the target domain since there are no labels. However, the loss can be extended to include the target domain if labels exist.</p> </li> <li> <p>Qid is the loss of encoding an image and decoding it back into the same domain. Encoding an image into Z and decoding it back to the original domain should ideally return the same image. This loss can be the L1 norm of the difference between the original and decoded image.</p> </li> <li> <p>Qz is the domain discriminator’s loss. This is similar to ADDA in that it is trying to determine the domain of the encoding. We want this loss to increase as the encodings improve.</p> </li> <li> <p>Qtr is another discrimination loss in which the image is translated into the other domain before going to the domain discriminator.</p> </li> <li> <p>Qcyc is the cycle consistency loss. This loss is similar to Qid. The difference is that the images are decoded in the other domain before being encoding and decoded in the original domain. The image from the source domain is encoded into Z. This is decoded into the target domain and encoded back to Z. This is then decoded into the source domain and compared with the original image. A loss with the source and target switched is also applied. This aims to ensure encodings from similar images in different domains have similar encodings.</p> </li> <li> <p>Qtrc is similar to Qcyc, but instead of decoding back into the original domain, the encoding is classified. Unlike Qcyc, this is not symmetric since it involves labels. An image from the source domain is translated into the target domain and then classified.</p> </li> </ol> <h2 id="cycada"><a href="https://arxiv.org/abs/1711.03213">CyCADA</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/cycada-480.webp 480w,/assets/img/blog/domain_adaptation/cycada-800.webp 800w,/assets/img/blog/domain_adaptation/cycada-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/cycada.png" width="100%" height="auto" alt="CyCADA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1711.03213"> Source </a> </figcaption> </figure> <p>CyCADA is similar to I2I. Many of the I2I losses and networks have counterparts here. The main difference is that the target images are not translated to the source domain. Also, the GAN losses can be applied to both the images and the encodings.</p> <p>The source images are translated into the target domain. They are translated back into the source domain to apply the cycle consistency loss (L1 difference with the original image).</p> <p>The fs network is trained on the supervised learning task in the source domain. The semantic consistency loss ensures that the features from this network remain close before and after translation into the target domain. This ensures that the images retain the semantic information after translation.</p> <p>A GAN loss is then applied to the images and features (from fT) for the translated images and the target images. This loss is needed to train the translations to be similar to the target domain. There are two GAN losses to ensure that both the images and the features are similar.</p> <p>Finally, a task loss is applied to the translated images. This applies the task to the original target images.</p> <h1 id="other-domains">Other Domains</h1> <p>Image classification is the primary problem used to benchmark domain adaptation methods. However, domain adaptation can also be applied to other computer vision problems, such as image segmentation. It can also be applied in different research areas, such as natural language processing (NLP).</p> <p>One particularly interesting application of domain adaptation is self-driving cars and robotics. It is a common practice to train deep neural networks for these applications using data from simulated environments. It is much easier to collect large amounts of data in a simulation rather than in the real world. However, in order for a model trained on simulation data to function in a real-world environment, domain adaptation is often required to achieve good performance.</p> <p>There are also many variants to the problem, including few-shot domain adaptation, domain generalization, and multiclass domain adaptation.</p> <h1 id="conclusion">Conclusion</h1> <p>There are several approaches to domain adaptation but they often share some common characteristics. Adversarial learning with a domain discrimination network is common. There is also a lot of work using image to image translation with a cycle consistency loss. Apply domain adaptation to new problems will likely involve some combination of these components.</p> <h1 id="references">References</h1> <p>[1] Long, Mingsheng, et al. “Learning transferable features with deep adaptation networks.” International conference on machine learning. PMLR, 2015.</p> <p>[2] Eric Tzeng et al. “Adversarial discriminative domain adaptation”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017, pp. 7167-7176.</p> <p>[3] Yaroslav Ganin and Victor Lempitsky. “Unsupervised domain adaptation by backpropagation”. In: arXiv preprint arXiv:1409.7495 (2014).</p> <p>[4] Zak Murez et al. “Image to image translation for domain adaptation”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018, pp. 4500-4509.</p> <p>[5] Hoffman, Judy, et al. “Cycada: Cycle-consistent adversarial domain adaptation”. In: arXiv preprint arXiv:1711.03213 (2017).</p>]]></content><author><name></name></author><category term="computer-vision"/><summary type="html"><![CDATA[Machine learning performance depends on the dataset that it is trained on. Datasets are imperfect, so problems in the data affect the models. One type of problem is domain shift. This means that a model trained to learn a task on one dataset, may not be able to perform the same task on a slightly different dataset.]]></summary></entry><entry><title type="html">Pruning Neural Networks</title><link href="https://rohitbandaru.github.io/blog/Neural-Network-Pruning/" rel="alternate" type="text/html" title="Pruning Neural Networks"/><published>2020-09-01T00:00:00+00:00</published><updated>2020-09-01T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Neural-Network-Pruning</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Neural-Network-Pruning/"><![CDATA[<p>Much of the success of deep learning has come from building larger and larger neural networks. This allows these models to perform better on various tasks, but also makes them more expensive to use. Larger models take more storage space which makes them harder to distribute. Larger models also take more time to run and can require more expensive hardware. This is especially a concern if you are productionizing a model for a real-world application.</p> <p>Model compression aims to reduce the size of models while minimizing loss in accuracy or performance. Neural network pruning is a method of compression that involves removing weights from a trained model. In agriculture, pruning is cutting off unnecessary branches or stems of a plant. In machine learning, pruning is removing unnecessary neurons or weights. We will go over some basic concepts and methods of neural network pruning.</p> <h1 id="remove-weights-or-neurons">Remove weights or neurons?</h1> <p>There are different ways to prune a neural network.</p> <ol> <li>You can prune weights.</li> </ol> <p>This is done by setting individual parameters to zero and making the network sparse. This would lower the number of parameters in the model while keeping the architecture the same.</p> <ol> <li>You can remove entire nodes from the network.</li> </ol> <p>This would make the network architecture itself smaller, while aiming to keep the accuracy of the initial larger network.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/pruning/weights_vs_neurons-480.webp 480w,/assets/img/blog/pruning/weights_vs_neurons-800.webp 800w,/assets/img/blog/pruning/weights_vs_neurons-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pruning/weights_vs_neurons.png" width="100%" height="auto" alt="pruning weights vs nodes" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Visualization of pruning weights/synapses vs nodes/neurons <a href="https://arxiv.org/abs/1506.02626"> Source </a> </figcaption> </figure> <p>Weight-based pruning is more popular as it is easier to do without hurting the performance of the network. However, it requires sparse computations to be effective. This requires hardware support and a certain amount of sparsity to be efficient. Pruning nodes will allow dense computation which is more optimized. This allows the network to be run normally without sparse computation. This dense computation is more often better supported on hardware. However, removing entire neurons can more easily hurt the accuracy of the neural network.</p> <h1 id="what-to-prune">What to prune?</h1> <p>A major challenge in pruning is determining what to prune. If you are removing weights or nodes from a model, you want the parameters you remove to be less useful. There are different heuristics and methods of determining which nodes are less important and can be removed with minimal effect on accuracy. You can use heuristics based on the weights or activations of a neuron to determine how important it is for the model’s performance. The goal is to remove more of the less important parameters.</p> <p>One of the simplest ways to prune is based on the magnitude of the weight. Removing a weight is essentially setting it to zero. You can minimize the effect on the network by removing weights that are already close to zero, meaning low in magnitude. This can be implemented by removing all weights below a certain threshold. To prune a neuron based on weight magnitude you can use the L2 norm of the neuron’s weights.</p> <p>Rather than just weights, activations on training data can be used as a criteria for pruning. When running a dataset through a network, certain statistics of the activations can be observed. You may observe that some neurons always outputs near-zero values. Those neurons can likely be removed with little impact on the model. The intuition is that if a neuron rarely activates with a high value, then it is rarely used in the model’s task.</p> <p>In addition to the magnitude of weights or activations, redundancy of parameters can mean a neuron can be removed. If two neurons in a layer have very similar weights or activations, it can mean they are doing the same thing. By this intuition, we can remove one of the neurons and preserve the same functionality.</p> <p>Ideally in a neural network, all the neurons have unique parameters and output activations that are significant in magnitude and not redundant. We want all the neurons are doing something unique, and remove those that are not.</p> <h1 id="when-to-prune">When to prune?</h1> <p>A major consideration in pruning is where to put it in the training/testing machine learning timeline. If you are using a weight magnitude-based pruning approach, as described in the previous section, you would want to prune after training. However, after pruning, you may observe that the model performance has suffered. This can be fixed by fine-tuning, meaning retraining the model after pruning to restore accuracy.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/pruning/pruning_flow-480.webp 480w,/assets/img/blog/pruning/pruning_flow-800.webp 800w,/assets/img/blog/pruning/pruning_flow-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pruning/pruning_flow.png" width="300" height="auto" alt="flow of iterative pruning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1611.06440"> Source </a> </figcaption> </figure> <p>The usage of pruning can change depending on the application and methods used. Sometimes fine-tuning or multiple iterations of pruning are not necessary. This depends on how much of the network is pruned.</p> <h1 id="how-to-evaluate-pruning">How to evaluate pruning?</h1> <p>There multiple metrics to consider when evaluating a pruning method: accuracy, size, and computation time. Accuracy is needed to determine how the model performs on its task. Model size is how much bytes of storage the model takes. To determine computation time, you can use FLOPs (Floating point operations) as a metric. This is more consistent to measure than inference time and it does not depend on what system the model runs on.</p> <p>With pruning, there is a tradeoff between model performance and efficiency. You can prune heavily and have a smaller more efficient network, but also less accurate. Or you could prune lightly and have a highly performant network, that is also large and expensive to operate. This trade-off needs to be considered for different applications of the neural network.</p> <h1 id="conclusion">Conclusion</h1> <p>Pruning is an effective method of making neural networks more efficient. There are plenty of choices and areas of research in this area. We want to continue to make advances in deep learning while also keeping our models energy, time, and space-efficient.</p> <h1 id="references">References</h1> <p>[1] Blalock, Davis, et al. “What is the state of neural network pruning?.” arXiv preprint arXiv:2003.03033 (2020).</p> <p>[2] Han, Song, et al. “Learning both weights and connections for efficient neural network.” Advances in neural information processing systems. 2015.</p> <p>[3] PyTorch Pruning Tutorial <a href="https://pytorch.org/tutorials/intermediate/pruning_tutorial.html">https://pytorch.org/tutorials/intermediate/pruning_tutorial.html</a></p> <p>[4] Keras / Tensorflow Pruning Tutorial <a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras">https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras</a></p> <p>[5] Molchanov, Pavlo, et al. “Pruning convolutional neural networks for resource efficient inference.” arXiv preprint arXiv:1611.06440 (2016).</p> <p>[6] Babaeizadeh, Mohammad, Paris Smaragdis, and Roy H. Campbell. “Noiseout: A simple way to prune neural networks.” arXiv preprint arXiv:1611.06211 (2016).</p>]]></content><author><name></name></author><category term="applied-ml"/><summary type="html"><![CDATA[Much of the success of deep learning has come from building larger and larger neural networks. This allows these models to perform better on various tasks, but also makes them more expensive to use. Larger models take more storage space which makes them harder to distribute. Larger models also take more time to run and can require more expensive hardware. This is especially a concern if you are productionizing a model for a real-world application.]]></summary></entry></feed>