<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.0">Jekyll</generator><link href="https://rohitbandaru.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rohitbandaru.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-29T07:07:41+00:00</updated><id>https://rohitbandaru.github.io/feed.xml</id><title type="html">blank</title><subtitle>ML blog.</subtitle><entry><title type="html">Transformer Design Guide (Part 2: Modern Architecture)</title><link href="https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt2/" rel="alternate" type="text/html" title="Transformer Design Guide (Part 2: Modern Architecture)"/><published>2025-01-11T00:00:00+00:00</published><updated>2025-01-11T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt2</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt2/"><![CDATA[<p>While the core transformer architecture introduced in “Attention is All You Need” remains effective and widely used today, numerous architectural improvements have emerged since its inception. Unlike the dramatic evolution seen in Convolutional Neural Networks (CNNs) from AlexNet onwards, transformer modifications tend to be more incremental and optional - the original architecture still serves as a strong baseline. This speaks to the elegant design choices made by the original authors, while leaving room for ongoing optimizations in efficiency, context length, and multimodal capabilities.</p> <p>This post will focus on changes to the Transformer architecture that have been popular in the last couple of years (as of 2024). Time will tell which of these will be relevant in the future. For a deep dive on the original transformer architecture from <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> (2017), see part 1 of this blog post. ML researchers and engineers use a lot of jargon when discussing transformers. This blog post seeks to elucidate it.</p> <p>The original transformer architecture is very powerful and can handle a very wide range of applications. However, there are some limitations that more recent research has sought to address:</p> <ul> <li>Long context <ul> <li>Transformers are limited by their \(N^2\) computational complexity. This limits the sequence length a transformer can process, especially under resource and latency constraints.</li> </ul> </li> <li>Multimodality <ul> <li>While transformers were originally developed for language processing, which remains their primary application, they have since expanded to handle nearly all data modalities and power large multimodal models.</li> </ul> </li> <li>Efficiency <ul> <li>Scaling is attributed to the recent successes of LLMs. To enable this scaling, the model architecture needs to be designed efficiently.</li> <li>There have also been some research into more efficient computation. The same operations can be rewritten to run faster on hardware.</li> </ul> </li> </ul> <p>This blog post will cover the modifications to the architecture that are popular in the ML community and are applied in cutting edge industry applications such as LLMs (GPT, Claude, Gemini). This will give the prerequisite knowledge to understand these SOTA models. Many of these optimizations are only popularly used in LLMs. I find that transformers in other modalities, such as vision transformers, tend to stick to more vanilla architectures and training procedures.</p> <p>While these architectural improvements are significant, they aren’t the main driver of AI’s remarkable progress in recent years. The real paradigm shift has come from post-training techniques. Specifically, fine-tuning and instruction tuning methods have enabled us to leverage language models more effectively for a variety of use cases. I initially planned to cover these techniques here, but as this post grew longer, I realized they deserve a part 3 to do them justice. For now, we’ll focus purely on the architectural changes to the model itself.</p> <p>Many of the topics covered in this blog are large and active areas of research. These may warrant their own blog posts to give the research justice. However, in this post, we will take a myopic view and not cover this breadth of research. This will allow us to focus on more aspects of modern transformer applications and how they relate to each other.</p> <p>To understand the current state of the art, we will look at technical reports for foundational LLMs: <a href="https://arxiv.org/abs/2303.08774">GPT</a>, <a href="https://arxiv.org/abs/2403.05530">Gemini</a>, <a href="https://arxiv.org/abs/2302.13971">LLaMA</a>, <a href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf">Claude</a>. We will also look at recent papers in other modalities to see how the transformer is adapted or evolved for these use cases. However, we find that there are generally fewer architectural changes in other domains.</p> <p>Unfortunately many of the most used transformer models are opaque in that the public has access to very limited details of their architecture. Even open source models usually only make the inference code open, while keeping the training code closed. However, the following models can still help us understand the current state of the art:</p> <ul> <li>Open Source LLMs <ul> <li>LLaMA (2023): <a href="https://arxiv.org/abs/2302.13971">LLaMA</a>, <a href="https://arxiv.org/abs/2307.09288">LLaMA 2</a>, <a href="https://arxiv.org/abs/2407.21783">LLaMA 3</a></li> <li>Gemma (2024): <a href="https://arxiv.org/abs/2403.08295">Gemma</a>, <a href="https://arxiv.org/abs/2408.00118">Gemma 2</a></li> <li>Mistral (2023): <a href="https://arxiv.org/abs/2310.06825">Mistral 7B</a>, <a href="https://arxiv.org/abs/2401.04088">Mixtral of Experts</a></li> <li>Qwen (2023): <a href="https://arxiv.org/abs/2309.16609">Qwen</a></li> <li>Phi (2024): <a href="https://arxiv.org/abs/2404.14219">Phi-3</a>, <a href="https://arxiv.org/abs/2412.08905">Phi-4</a></li> <li>DeepSeek (2024): <a href="https://arxiv.org/abs/2401.02954">DeepSeek LLM</a> <a href="https://arxiv.org/abs/2405.04434">DeepSeek V2</a> <a href="https://arxiv.org/abs/2412.19437">DeepSeek V3</a></li> <li>OLMo (2024): <a href="https://arxiv.org/abs/2402.00838">OLMo</a>, <a href="https://arxiv.org/abs/2501.00656">OLMo 2</a> <ul> <li>Open training code!</li> </ul> </li> </ul> </li> <li>Text encoder LLMs <ul> <li><a href="https://arxiv.org/abs/2412.13663">ModernBERT</a> (2024)</li> </ul> </li> <li>Open Source Vision Language Models <ul> <li>PaliGemma (2024): <a href="https://arxiv.org/abs/2407.07726">PaliGemma</a>, <a href="https://arxiv.org/abs/2412.03555">PaliGemma 2</a></li> <li><a href="https://arxiv.org/abs/2412.10302">DeepSeek-VL2</a> (2024)</li> </ul> </li> <li>Computer Vision Models <ul> <li><a href="https://arxiv.org/abs/2302.05442">ViT-22B</a> (2023)</li> </ul> </li> <li>Many more!</li> </ul> <p>Through looking at these technical reports, you find that they share many of the same architectural improvements. This blog seeks to explain these improvements. I believe it is likely that the closed source transformers (GPT 3+, Gemini, Claude) aren’t that different.</p> <h1 id="normalization">Normalization</h1> <p>In part 1, we covered <a href="https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt1/#normalization">Pre-LayerNorm</a>. We also see in architectures such as GPT that layer norm is added before and after the feed-forward and attention layers. This change was introduced in <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf#page=4.84">GPT-2</a>. Generally, adding more normalization is inexpensive and has little downside, but can significantly improve and speed up training.</p> <p>There are also new types of normalization being used. <a href="https://arxiv.org/abs/1910.07467">RMSNorm</a> is currently a popular variant, used in <a href="https://arxiv.org/pdf/2302.13971">LLaMA</a>.</p> <p>To refresh, layer normalization is defined as follows:</p> \[\mathrm{LN}(x) = \frac{x-\mu(x)}{\sqrt{\sigma(x)^2+\epsilon}} *\gamma +\beta\] <p>RMSNorm is defined as:</p> \[\mathrm{RMSNorm(x)} = \frac{x}{\mathrm{RMS}(x)} \gamma, \quad \text{where} \quad \mathrm{RMS}(x) = \sqrt{\frac{1}{N} \sum_{i=1}^N x_i^2+\epsilon}\] <p>The notation we use for RMSNorm is changed to be more aligned to the LayerNorm notation. \(x\) and \(\gamma\) are both vectors, and the division by \(\mathrm{RMS}(x)\) is broadcasted to each element of \(x\). \(\gamma\) is a vector or learned scaling parameters for each value of \(x\). It is written as \(g\) in the RMSNorm paper.</p> <p>The difference is that RMSNorm rescales the features, while LayerNorm rescales and recenters the features. RMSNorm is designed to be shift invariant and not depend on the mean or recenter the values in any way. The mean subtraction is dropped from the numerator. \(\beta\) is also dropped to avoid a learned recentering. In LN, variance \(\sigma(x)^2\) depends on the mean (\(\frac{\sum_{i=1}^{N}(x_i - \mu)^2}{N}\)). RMS is used in place of variance to be completely independent of the mean. RMSNorm is completely shift invariant.</p> <p>RMSNorm is more efficient than LayerNorm. The memory usage is halved because there is only one learned parameter \(\gamma\), since \(\beta\) is dropped. There is also less computation since there is no mean subtraction. These small efficiency gains become significant when applied to transformers with hundreds of billions of parameters. Another possible advantage is that it also constrains the model less, while still keeping the advantages of normalization.</p> <h1 id="activation-functions">Activation Functions</h1> <p>The original transformer used ReLU activation functions, which appear only between the two layers of the feed-forward block. Since then, two other activation functions have gained popularity: GeLU and SwiGLU.</p> <p>Research into activation functions remains largely empirical, with limited theoretical understanding of why certain functions outperform others. The research community has spent years optimizing every component of the transformer architecture, including the ReLU activation function. These functions should be considered for any new transformer application, but one should only expect a modest improvement in performance. Nevertheless, their mathematical formulations are quite interesting and worth understanding.</p> <blockquote> <p>“We offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence.” - Noam Shazeer in “<a href="https://arxiv.org/abs/2002.05202">GLU Variants Improve Transformer</a>”</p> </blockquote> <h2 id="gaussian-error-linear-units-gelus"><a href="https://arxiv.org/abs/1606.08415"><strong>Gaussian Error Linear Units (GELUs)</strong></a></h2> <p>GeLU is formally defined as follows:</p> \[\mathrm{GeLU}(x)=xP(X \leq x)=x\Phi(x)\] <p>\(\Phi(x)\) or \(P(X \leq x)\) is the standard Gaussian cumulative distribution function (CDF). It is the probability of values less than \(x\) occurring in a standard normal distribution: \(\Phi(x)=P(X \leq x); X\sim\mathcal{N}(0,1)\). GeLU multiples the input by this probability.</p> <p>The CDF is defined as follows:</p> \[Φ(x) = (1/2) * [1 + \mathrm{erf}(x / √2)]\] <p>The error function is defined as:</p> \[\text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt\] <p>The integral in \(\mathrm{erf}\) needs to be approximated for practical use. There are two ways to approximate GeLU:</p> \[\mathrm{GeLU}(x)\approx0.5x(1 + \tanh[\sqrt{2/π}(x + 0.044715x^3)])\] \[\mathrm{GeLU(x)} \approx x\sigma(1.702x)\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/gelu-480.webp 480w,/assets/img/blog/transformer_pt2/gelu-800.webp 800w,/assets/img/blog/transformer_pt2/gelu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/gelu.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="GeLU" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1606.08415"> Source </a> </figcaption> </figure> <p>GeLU can be interpreted as a smooth version of ReLU with the added property of non-zero gradients for negative inputs.</p> <p>The GPT-2 <a href="https://github.com/openai/gpt-2/blob/master/src/model.py#L26">code</a> uses the longer and more exact approximation. The shorter approximation is much more computationally efficient. Nowadays, this is more popular because the difference in model performance is negligible.</p> <p>GeLU is used in BERT and GPT models. It is also popular for vision transformers such as ViT-22B.</p> <h2 id="swiglu"><a href="https://arxiv.org/abs/2002.05202">SwiGLU</a></h2> <p>SwiGLU builds on the <a href="https://arxiv.org/abs/1710.05941v1">Swish</a> activation function. This is also referred to as SiLU (Sigmoid Linear Unit) in the GeLU paper.</p> \[\mathrm{Swish}(x) = x\sigma(x) = \frac{x}{1+e^{-x}}\] <p>SwiGLU is a gated linear unit (GLU) version of this. GLUs were introduced in <a href="https://arxiv.org/abs/1612.08083">2016</a>. GLUs modify the feed-forward network by introducing a gating mechanism that controls information flow, allowing the network to selectively emphasize or suppress different parts of the input. This works by adding another linear transformation of the input \(Vx\) that acts as the gating function. The gating function performs element-wise multiplication with the output of the first feedforward layer and activation function. SwiGLU is a GLU that uses Swish as the activation function.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/swiglu-480.webp 480w,/assets/img/blog/transformer_pt2/swiglu-800.webp 800w,/assets/img/blog/transformer_pt2/swiglu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/swiglu.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="SwiGLU" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> </figcaption> </figure> \[\mathrm{SwiGLU}(x) = \mathrm{Swish}(xW_1+b)⊗(Vx+c)\] <p>GLUs are more expensive since \(V\) is essentially a third linear layer in the feed-forward block, the same size as \(W_1\). To ameliorate this cost, the SwiGLU paper sets \(d_{ff}\) to \(\frac{2}{3}4d_{model}\), instead of \(4d_{model}\). \(\frac{2}{3}\)is chosen to keep the total number of parameters in the feed-forward block constant, since we have three weight matrices of the same parameter count instead of one. LLaMA also uses this ratio. However, this additional matrix multiplication is in parallel to the first linear layer. Depending on the hardware, it may have negligible training speed implications. The gating mechanism improves the performance of the model enough to warrant a reduction in the width of the feed-forward block.</p> <p>GLU also increases the expressivity of the linear layer. The first linear layer is able to represent second order polynomials like \(x^2\), due to the element-wise multiplication. This may compensate for reduced width.</p> <p>SwiGLU is used by many recent LLMs such as Mixtral/Mistral, LLaMA, and Qwen. Gemma uses GeGLU, which was introduced in the same paper as SwiGLU (<a href="https://arxiv.org/abs/2002.05202">GLU Variants Improve Transformer)</a>. GeGLU is a gated linear unit (GLU) variant of GeLU (GELU in place of Swish).</p> <h1 id="position-embedding">Position Embedding</h1> <p>In <a href="https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt1/#position-embedding">part 1</a>, we covered two basic implementations of position embeddings tested in the “Attention Is All You Need” paper: learned position embeddings and sinusoidal position embeddings. These absolute position embeddings are functions of each token’s position in the sequence. While these methods are straightforward, they have notable limitations.</p> <p>One key challenge is generalizing position embeddings to longer context lengths than those used in training. LLMs are typically pretrained with a fixed context length (e.g., 4096), but we often want to use them with longer sequences during inference. Learned position embeddings can’t generalize beyond their training sequence lengths since they lack embeddings for later positions. This is why sinusoidal position embeddings, which can naturally extend to any length, have become more popular.</p> <p>Another limitation is that absolute position embeddings are permutation invariant, which isn’t ideal for many applications. This has led to research into relative position embeddings, which encode the positional relationship between pairs of tokens rather than their absolute positions in the sequence.</p> <h2 id="relative-position-encoding">Relative Position Encoding</h2> <p>Relative position encoding (RPE) uses the distance between tokens to define an embedding. Absolute position embeddings are simple in that they only require adding an embedding to the input token embeddings of the model. Each token maps to one embedding. With relative position embeddings, there is an embedding for different pairs of tokens. This requires computing the embeddings in the attention operation.</p> <p>For a sequence of length n, there are \(2n-1\) different pairwise distances ranging from \(-n/2\) to \(n/2\), including 0. The distance can also be clamped by a max distance to reduce the number of parameters. These distances are mapped to learned embeddings.</p> <p>RPE was introduced by <a href="https://arxiv.org/abs/1803.02155">Shaw et al. 2018.</a> This particular implementation can be referred to as the Shaw relative position embedding. Rather than using matrices, the paper describes how individual attention scores, for query token \(i\) and key token \(j\).</p> <p>The following equation describes how relative position is encoded into attention:</p> \[\begin{equation}e_{ij} = \frac{(x_i W^Q)(x_j W^K+a_{ij}^K)^T}{\sqrt{d_z}}\end{equation}\] \[\begin{equation}z_i = \sum_{j=1}^{n} \alpha_{ij}(x_jW^V + a_{ij}^V)\end{equation}\] <p>Equation (1) describes how the attention scores are calculated. Equation (2) shows how relative position is encoded into the values. It essentially reweighs the values based on relative positions. \(\alpha\) is just \(e\) after softmax along the key dimension. The change from standard attention is the addition of \(a_{ij}^K\) and \(a_{ij}^V\). These are learned embeddings that bias the keys and values based on the relative position between \(i\) and \(j\). The bias on the key is different based on which query it will be in a dot product with. The embedding lookup is based on the difference between \(i\) and \(j\) and a max distance parameter \(k\). There are separate embedding tables for keys and values: \(w^K\), \(w^V\).</p> \[\begin{align*} a_{ij}^K &amp;= w^K_{\text{clip}(j-i, k)} \\ a_{ij}^V &amp;= w^V_{\text{clip}(j-i, k)} \\ \text{clip}(x, k) &amp;= \max(-k, \min(k, x)) \end{align*}\] <p>Relative position is encoded in the keys so that the attention matrix can capture positional relationships between tokens. The position can also be encoded in the values, so that positional information can be encoded in the embedding themselves. This makes sense since the attention mechanism is an information bottleneck since the scores are scalars, whereas values are embeddings. However, the paper reports that they get the best results from only adding relative position the keys. Relative position on the values does not add any additional performance but comes with an efficiency cost. We see subsequent works drop this term and focus on incorporating relative position in the attention matrix.</p> <p>Now that we have defined how we can incorporate relative position within the attention operation, we must understand how to efficiently implement this. The notation above doesn’t use matrix multiplies and we cannot naively iterate over \(i\) and \(j\) with for loops. To implement the Shaw relative position embedding, \(a^K\) and \(a^V\) are computed as two matrices of size \((N, N, h, d_k)\) and \((N, N, h, d_v)\), result in \(O(N^2hd)\) memory utilization. The memory usage is reduced to \(O(N^2d)\) by sharing these embeddings between each attention head (this is also in line with absolute position embedding). The notation above disregards the attention heads dimension for simplicity. We can formulate attention with relative position as follows:</p> \[\text{RelativeAttention} = \text{Softmax}\left(\frac{QK^T + S^{rel}}{\sqrt{d_k}}\right)V\] <p>\(S^{rel}\) represents the relative attention bias which is calculated by initializing a matrix of size \([N,N,h,d_k]\). This contains embeddings of size \(d_k\) from each pair of \(N\) positions. This matrix adds significant memory complexity and contains repeated embeddings.</p> <p>Huang et al. 2018 introduce in their <a href="https://arxiv.org/abs/1809.04281">Music Transformer</a> paper another optimization to RPE to bring the memory complexity down to \(O(Nd)\). The key intuition is that there are only \(2n-1\) or \(O(N)\) possible relative positions possible. The Shaw RPE matrices include many duplicate embeddings because the same pairwise distances repeat many times in the model. We can restructure the computation so that these repeated embeddings aren’t stored in memory at the same time. This implementation does not include relative position in the values.</p> <p>Music Transformer optimizes this by multiplying each query directly with the relative position embedding matrix: \(QE^{r\top}\). This produces a matrix of size \([N,N]\), matching the expected size of the attention matrix bias. \(E^r\) represents the embedding table as a matrix storing representations for every relative position (pairwise position difference). The matrix is then transformed into the correct attention bias \(S^{rel}\) through a series of operations which they refer to as a skewing procedure. While these transformations are confusing, the key point is that we avoid creating any \(O(N^2d)\) matrices. Instead, \(E^r\) only requires \(O(Nd)\) additional space, with all other matrices matching the attention matrix size of \(O(N^2)\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/musictransformer_rpe-480.webp 480w,/assets/img/blog/transformer_pt2/musictransformer_rpe-800.webp 800w,/assets/img/blog/transformer_pt2/musictransformer_rpe-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/musictransformer_rpe.png" class="image-fluid mx-auto d-block" width="600" height="auto" alt="Music Transformer RPE" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1809.04281"> Source </a> </figcaption> </figure> <ol> <li>Mask the top left triangle, this will be shifted to result in a causal mask</li> <li>Pad an empty “dummy” column on the left side</li> <li>Reshape the matrix from \([L,L+1]\) to \([L+1,L]\) <ol> <li>Row-major ordering would cause this to result in the values to be shifted such that the relative positions are aligned to the diagonals.</li> </ol> </li> <li>Remove the empty first row through slicing</li> </ol> <p>Since relative position is encoded into the attention layers, it possible to use it alongside absolute position embeddings. Relative position embeddings are only required to be used in the first attention layer, since the information can propagate to subsequent layers. However, it is more common to use relative position attention in all of the transformer blocks. This may ensure that the position information is not disregarded by the model. It is much less common to add absolute position embeddings in every block.</p> <p>Relative position embeddings add an inductive bias of translation invariance to the models. Given the scale of the datasets modern transformers are trained on, it seems like this bias would limit the model’s ability to learn. However, as with many results in this post, relative position embedding has empirical advantages in language modeling.</p> <h2 id="attention-biasing">Attention Biasing</h2> <p>Relative position embeddings initialize embeddings for each relative position. However this can be simplified by mapping each relative distance to a scalar bias to the attention scores.</p> <h3 id="alibi">ALiBi</h3> <p>Attention with Linear Biases (ALiBi) is another method of position embedding introduced by <a href="https://arxiv.org/abs/2108.12409">Press et al. 2021</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/alibi-480.webp 480w,/assets/img/blog/transformer_pt2/alibi-800.webp 800w,/assets/img/blog/transformer_pt2/alibi-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/alibi.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="ALiBi" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2108.12409"> Source </a> </figcaption> </figure> <p>\(m\) is a hyperparameter that can be set per attention head. No absolute position embeddings are added. This method adds a strong inductive bias for locality. The attention scores decay with relative distance. Since there are no learned parameters for position this bias can generalize to longer sequence lengths during inference. The biases can be extended indefinitely.</p> <h3 id="t5">T5</h3> <p>The T5 models introduced by <a href="https://arxiv.org/abs/1910.10683">Raffel et al. 2019</a>, also utilize relative biases, but make these learned parameters that are shared among all the layers. Rather than setting a single hyperparameter \(m\), each relative distance maps to a learnable scalar bias value. This is more parameter efficient when compared to RoPE since the biases are shared across all layers, and they are scalars rather than embeddings.</p> <h2 id="rope"><a href="https://arxiv.org/abs/2104.09864">RoPE</a></h2> <p>While relative position embeddings improve model performance, they require modifying the attention operation with position bias, adding computational complexity and memory overhead. Rotary Position Embedding (RoPE), introduced in the RoFormer paper by <a href="https://arxiv.org/abs/2104.09864">Su et al. 2021</a>, offers an elegant solution by encoding relative position directly into the queries and keys. This approach keeps the attention operation unchanged by leveraging mathematical properties to achieve the same benefits more efficiently.</p> <p>The attention matrix contains dot products between query and key embeddings: \(\langle q_m,k_n \rangle\) . The key property of relative position embeddings is that this dot product does not depend on the absolute positions \(m\) and \(n\), but rather the relative position \(m-n\). This is translation invariant, meaning that the output is the same if you shift \(m\) and \(n\) by any constant amount. RoPE exploits the fact that we do not care what information is in the queries and keys, we only care about the dot product.</p> \[\langle f_q(x_m,m), f_k(x_n,n) \rangle = g(x_n, x_n, m-n)\] <p>RoPE groups the embedding dimensions into groups of two adjacent indices. These two values can be considered to be defining a complex number. The first number represents the real part, and the second is the imaginary dimension.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/rope-480.webp 480w,/assets/img/blog/transformer_pt2/rope-800.webp 800w,/assets/img/blog/transformer_pt2/rope-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/rope.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="RoPE" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2104.09864"> Source </a> </figcaption> </figure> <p>If the embeddings are of size \(d\), \(d/2\) of these size two blocks / complex numbers are present. The absolute position of the embedding in the input sequence is denoted \(m\).</p> <p>Each of these groups representing a vector will be rotated. There is a different angle \(\theta\) of rotation for each position \(m\) from 0 to \(d/2\).</p> \[\Theta=\{\theta_i=10000^{-2(i-1)/d},i\in[1,2,...,d/2]\}\] <p>The value 10000 follows the sinusoidal position embedding in Attention Is All You Need and is equivalent to \(\theta_1\). Each group is rotated by the angle \(m\theta_i\). The rotation matrix is defined as:</p> \[\begin{bmatrix} \cos(m\theta_i) &amp; -\sin(m\theta_i) \\ \sin(m\theta_i) &amp; \cos(m\theta_i) \end{bmatrix}\] <p>Given that \(m\) is a variable index. The \(\theta\) values define frequencies. Each group’s vector is rotated at a different frequency. This helps the model learn short and long range relationships between tokens.</p> <p>We want to multiply a vector containing \(d/2\) groups by different rotation matrices. To accomplish this, the different rotation matrices are placed on a diagonal. This matrix is sparse and the rotation can be efficiently computed.</p> \[R_{\Theta,m}^d = \begin{pmatrix}\cos m \theta_1 &amp; -\sin m \theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\\sin m \theta_1 &amp; \cos m \theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\0 &amp; 0 &amp; \cos m \theta_2 &amp; -\sin m \theta_2 &amp; \cdots &amp; 0 &amp; 0 \\0 &amp; 0 &amp; \sin m \theta_2 &amp; \cos m \theta_2 &amp; \cdots &amp; 0 &amp; 0 \\\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos m \theta_{d/2} &amp; -\sin m \theta_{d/2} \\0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin m \theta_{d/2} &amp; \cos m \theta_{d/2}\end{pmatrix}\] <p>We can multiply this matrix by the query \(Q\) and key \(K\) matrices to encode the positions. The standard attention formula can be applied to these rotated queries and keys. We have defined the rotation and the implementation of RoPE but now we’ll show how this actually encodes position.</p> <p>Let’s take a arbitrary query embedding \(q_m\) and a key embedding \(k_m\). We can assume an embedding dimension of 2 so that the query and key map to only one complex number. The multiplication with the 2d rotation matrix is equivalent with multiplying with \(e^{im\theta}\) and \(e^{in\theta}\). Multiplying a complex number \(z = x + iy\) by \(e^{imθ}\) effectively rotates \(z\) counterclockwise by an angle \(m\theta\). \(x\) and \(y\) are the two components in the embedding group. Expanding the product:</p> \[\begin{aligned}e^{imθ} * z &amp;= (\cos(mθ) + i*\sin(mθ))*(x + iy)\\ &amp;=(x*\cos(mθ)-y*\sin(mθ)) + i*(x*\sin(mθ) + y*\cos(mθ))\end{aligned}\] <p>This is equivalent to multiplying with the 2d rotation matrix, assuming the first value \(x\) in the group represents the real component and \(y\) is the second and imaginary component.</p> \[\begin{aligned}R(θ) * [x, y] &amp;= \begin{bmatrix} \cos(m\theta_i) &amp; -\sin(m\theta_i) \\ \sin(m\theta_i) &amp; \cos(m\theta_i) \end{bmatrix} * [x, y] \\ &amp;= [x*cos(θ) - y*sin(θ), x*sin(θ) + y*cos(θ)]\end{aligned}\] <p>The query embedding is rotated by \(m\theta\) and the key is rotated by \(n\theta\).</p> \[q_m^r = q_m*e^{im\theta} \\ k_n^r = k_n*e^{in\theta}\] <p>In attention we are interested in the dot product of these rotated vectors. The dot product of the vectors is equivalent to the inner product of the complex numbers the 2d vector represents. The complex inner product is defined as \(Re[ab^*]\). We take the conjugate of the rotated key embedding:</p> \[k_n^{r*} = k_n^**e^{-in\theta}\] <p>We take the conjugate value of \(b\) and take the real part of the product. This can be applied to the query key product:</p> \[q_m^r*k_n^r = Re[q_m*e^{im\theta}*k_n^{r*}e^{-in\theta}] = Re[q_m^r*k_n^{r*}e^{i(m-n)\theta}]\] <p>This shows that we have reached our desired property that the dot product depends on the relative position \(m-n\), but not the absolute positions \(m\) and \(n\). This is also translation invariant in that you can shift \(m\) and \(n\) by a constant amount and not change this dot product.</p> <p>Unlike RPE, RoPE doesn’t require modifying the attention operation. It just requires transforming the input query and key matrices. This makes RoPE easier to efficiently implement. RoPE also has no learned embeddings. Attention is All You Need shows that sinusoidal position embeddings, with no learned parameters, performs just as well as learned position embeddings. RoPE can be thought of as an extension of sinusoidal embeddings that prefers relative position information.</p> <p>We’ll now revisit the \(\theta\) parameters to understand how they affect long context inference. \(10000\) can be considered the base frequency \(b\). This is an important hyperparameter that determines the lowest and highest frequency rotations. Given the embedding dimension \(d\), the lowest frequency rotation is \(\theta_{d/2} = b^{2/d-1}\). With the settings \(b=10000\) and \(d=768\), this corresponds to a wavelength of about 9763 tokens. This means for long context LLM inference (&gt;32k tokens), this rotation would become periodic. ****This directly affects the sequence lengths that RoPE can handle effectively. If \(b\) is set too low, RoPE becomes periodic during longer context inference and performance suffers.</p> <p>RoPE is still periodic like the sinusoidal absolute position embedding in the original transformer. However the difference is that it is periodic in the relative positions, not the absolute positions. The translation invariance significantly improves performance, but there is still room for improvement in the periodicity of the relative positions. Several approaches can be used to help RoPE perform better at long context inference.</p> <h3 id="positional-interpolation">Positional Interpolation</h3> <p><a href="https://arxiv.org/abs/2306.15595">Tian et al. 2023</a> introduces a simple method to improve RoPE’s extrapolation capabilities. For models trained with context length \(L\) that need to handle longer contexts \(L'\) during inference, RoPE can be rescaled using:</p> \[\text{f}'(x,m)=\text{f}'(x,\frac{mL}{L'})\] <p>This rescaling reduces the rotation applied to each token embedding. Though this approach requires finetuning for the new context length, the authors show that just 200 finetuning steps yield strong results.</p> <p>Another solution is to pretrain with scaled RoPE from the start. This approach was explored in <a href="https://arxiv.org/abs/2309.16039">Zhang et al. 2023</a> and implemented in <a href="https://arxiv.org/abs/2407.21783">Llama 3</a>. By setting the base frequency \(b\) to 500,000, they achieved effective inference with context lengths of 32k. While this offers a simple solution, it may limit the model’s ability to learn proper processing of higher relative positions during training.</p> <h3 id="yarn">YaRN</h3> <p><a href="https://arxiv.org/abs/2309.00071">Peng et al. 2023</a> improves on positional interpolation (PI) by scaling different RoPE frequencies differently. They hypothesize that PI hurts high frequency dimensions by changing the wavelengths by a significant portion. They adapt to this by scaling high frequencies less, and low frequencies more. As we previously discusses, the low frequency components are the most important to avoid periodicity.</p> <p>This method in used in <a href="https://arxiv.org/abs/2412.19437">DeepSeek-V3</a> and <a href="https://arxiv.org/abs/2407.10671v1">Qwen2</a>.</p> <p><strong>Additional Resources</strong></p> <ul> <li><a href="https://www.youtube.com/watch?v=DwaBQbqh5aE">Self-Attention with Relative Position Representations – Paper explained - AI Coffee Break with Letitia</a></li> <li><a href="https://jaketae.github.io/study/relative-positional-encoding/">Relative Positional Encoding - Jake Tae</a></li> <li><a href="https://gudgud96.github.io/2020/04/01/annotated-music-transformer/">Understanding Music Transformer - gudgud96</a></li> <li><a href="https://blog.eleuther.ai/rotary-embeddings/">Rotary Embeddings: A Relative Revolution - EleutherAI</a></li> <li><a href="https://fleetwood.dev/posts/you-could-have-designed-SOTA-positional-encoding">You could have designed state of the art Positional Encoding - fleetwood.dev</a></li> <li><a href="https://nn.labml.ai/transformers/rope/index.html">Rotary Positional Embeddings (RoPE) - labml.ai</a></li> <li><a href="https://github.com/AliHaiderAhmad001/T5-Relative-Position">Relative position embeddings according to T5 paper - AliHaiderAhmad001</a></li> <li><a href="https://blog.eleuther.ai/yarn/">Extending the RoPE - EleuterAI</a></li> <li><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo">LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU - Umar Jamil</a></li> </ul> <h1 id="efficient-attention">Efficient Attention</h1> <p>The \(N^2\) complexity of self-attention has long been considered a major bottleneck. There has been a lot of research into sparse attention methods, such as <a href="https://arxiv.org/abs/2007.14062">BigBird</a> and <a href="https://arxiv.org/abs/1904.10509">Sparse Transformers</a>. While these methods enhance efficiency through sparse attention patterns, they come at the cost of model performance. Because of this, we don’t see these methods adopted in frontier LLMs.</p> <p>Sliding Window Attention (SWA) is another sparse attention method introduced in <a href="https://arxiv.org/abs/2004.05150">Longformer</a>. Instead of having a query token attend to all prior key tokens, it attends only to \(w\) (window length) prior tokens. This reduces attention complexity from \(N^2\) to \(Nw\). However, effective performance requires a substantial window length. For example, Mistral uses a window length of half the context length. This is implemented through an attention mask during training. During inference, the KV cache (explained later) size can be reduced from \(N\) to \(w\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/swa-480.webp 480w,/assets/img/blog/transformer_pt2/swa-800.webp 800w,/assets/img/blog/transformer_pt2/swa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/swa.png" class="image-fluid mx-auto d-block" width="600" height="auto" alt="SWA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2310.06825"> Source </a> </figcaption> </figure> <p>SWA resembles convolution in that tokens attend only to nearby tokens. Longformer employs smaller window sizes in earlier transformer blocks and larger ones in later blocks. This enables both local and global information flow, allowing learning at different scales—similar to CNN architecture.</p> <p>A key limitation is that distant tokens cannot communicate directly. Information must flow through multiple transformer layers to connect distant tokens. To address this, models like <a href="https://arxiv.org/abs/2408.00118">Gemma 2</a> and <a href="https://arxiv.org/abs/2412.13663">ModernBERT</a> alternate between SWA and global attention blocks. By maintaining global attention blocks throughout the network, distant token can still communicate quickly. We can adjust the number of SWA blocks and window size to manage this efficiency tradeoff.</p> <p>SWA has emerged as a powerful and practical method for improving transformer efficiency. It has been gaining more traction in LLMs than other sparse attention approaches. SWA typically uses a large window size - usually half the global context length. This makes it more similar to global attention than sparse/local attention. However, it still offers substantial benefits. It halves both the KV cache size during inference. Additionally, it reduces the attention computation during both training and inference. SWA also avoids the issues of position embedding extrapolation by capping the maximum relative positions in the attention operation.</p> <h2 id="flashattention">FlashAttention</h2> <p>FlashAttention (<a href="https://arxiv.org/abs/2205.14135">Dao et al. 2022</a>) is an optimization to the computation of the attention operation. They are designed to be completely accurate in their results, but more efficiently run on GPUs. We will focus on <a href="https://arxiv.org/abs/2307.08691">FlashAttention-2</a>, which has several significant improvements. <a href="https://arxiv.org/abs/2407.08608">FlashAttention-3</a> is another optimization, however, it is designed specifically for NVIDIA GPUs.</p> <p>When designing algorithms like attention, we often only consider the number of floating point operations (FLOPs). However, we need to more carefully consider the hardware. Specifically, GPUs are very fast in computing highly parallelized floating point operations, but have slower memory access. We can speed up attention by reducing the amount of memory reads and writes, even if it comes at the cost of additional computation.</p> <p>GPUs have two types of memory: slow but large HBM (high bandwidth memory) and small but fast SRAM (static random access memory). When working with large matrices, we load blocks (parts of the matrices) from HBM into SRAM. The computation takes place and then the output is written back to HBM.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/gpu_memory-480.webp 480w,/assets/img/blog/transformer_pt2/gpu_memory-800.webp 800w,/assets/img/blog/transformer_pt2/gpu_memory-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/gpu_memory.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="GPU Memory Hierarchy" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2205.14135"> Source </a> </figcaption> </figure> <p>We will review two prerequisite topics before covering FlashAttention.</p> <h3 id="block-matrix-multiplication-bmm">Block Matrix Multiplication (BMM)</h3> <p><a href="https://mathworld.wolfram.com/BlockMatrix.html">Block matrix multiplication</a> enables matrix multiplications to be parallelized by splitting the inputs into blocks and concatenating output blocks. If we are trying to compute \(C=AB\), a certain output block requires a subset of rows from \(A\) and subset of columns from \(B\). We can just take the input blocks containing these rows and columns, rather than reading the entire matrix into memory.</p> \[A = \begin{bmatrix} A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{bmatrix}, \quad B = \begin{bmatrix} B_{11} &amp; B_{12} \\ B_{21} &amp; B_{22} \end{bmatrix}\] <p>Each output block \(C_{ij}\) is computed using only the required blocks: \(C_{11} = A_{11}B_{11} + A_{12}B_{21}\). The shapes of these blocks need to be carefully defined to be compatible for the matrix multiplication, and be efficient on hardware. The number of rows in the blocks of \(A\) must match the columns of blocks of \(B\).</p> <p>Computing an output block doesn’t require reading all the required input blocks at once. They can be read iteratively since it is a sum. When viewing matrix multiplication as dot products between rows and columns, the summation works by splitting these vectors into blocks, then summing the dot products of these blocks.</p> <p>Parallel GPU cores can compute different output blocks while only reading the required input blocks. Attention is just two matrix multiplications \(S = QK^\top\) and \(O = P V\) and a softmax. FlashAttention takes advantage of the BMM algorithm to compute this more efficiently.</p> <h3 id="online-softmax">Online Softmax</h3> <p>With BMM, we can compute matrix multiplications block by block without needing any global information. However, there is a challenge in integrating softmax into this framework since it requires normalizations which need to be calculated globally from all blocks.</p> <p>FlashAttention uses an online softmax to minimize the memory access. Recall the softmax operation:</p> \[\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}\] <p>The denominator \(l\) is the normalization factor. Also, softmax function is numerically unstable because exponents can reach extremely high values when \(x_i\) is a large positive number. To achieve numerical stability, we subtract the maximum value before computing the exponentials:</p> \[\text{softmax}(x_i) = \frac{e^{x_i - \max_j x_j}}{\sum_{j=1}^n e^{x_j - \max_j x_j}}\] <p>This subtraction keeps each exponent between 0 and 1, ensuring numerical stability. The result remains unchanged since this operation is equivalent to multiplying both numerator and denominator by the same constant. However, this maximum value is another global value.</p> <p>A naive implementation would require 3 passes, one to find the max value, one to compute the numerator and accumulate the normalization factor, and one to apply the normalization factor. This requires a lot of memory access. However, we can optimize by fusing the first two steps into a single \(O(N)\) pass using this algorithm:</p> <ol> <li>Assume the first element is the max \(m_j=s_{0j}\) and initialize an accumulator for the normalization constant as \(l_j := e^{s_{0j}-m_j}\)</li> <li>Iterate through the rest of the list <ol> <li>If the value \(s_{ij}\) is less than or equal to the current max, add it to the accumulator: \(l_j := l_j+e^{s_{ij}-m_j}\).</li> <li>If the value \(s_{ij}\) is greater than the current max, adjust the normalization factor (which was computed using the wrong max) by multiplying: \(l_j := l_j * e^{(s_{ij}-m_j)}\). This correction factor ensures \(s_{ij}-m_j\) is added to each incorrect exponent in the sum. Then update the max value: \(m_j:=s_{ij}\).</li> <li>This entire update can be expressed without conditional logic: \(m_i := \max(m_{i-1}, s_{ij});\quad l_j := l_j * e^{(m_{i-1}-m_{i})} + e^{(s_{ij}-m_{i})}\)</li> </ol> </li> </ol> <p>Through this optimization, we’ve reduced the softmax computation from three linear passes to two. The correctness of this algorithm can be simply proved by induction. The method is defined as iterating over individual items in \(x\), however, it easily extends to iterating over blocks in \(x\). We compute the local softmax within the blocks and then accumulate a global normalization factor.</p> <hr/> <p>There are two main changes to attention implemented in FlashAttention: tiling and recomputation.</p> <h3 id="tiling">Tiling</h3> <p>We can reduce the number of memory reads and writes by fusing operations together. With two separate operations, the data will be read twice and written twice. If the operations can be fused together, there will be only one read and write. However, we have to consider that we are working with very large matrices and using highly parallelized computation. Block matrix multiplication along with online softmax enables us to compute attention in pass without writing the full matrix to memory.</p> <p>With tiling, we restructure the attention operation to load data block by block from HBM to SRAM. The full attention matrix of size \((N,N)\) is never materialized at once or written to HBM. This is done by looping through the blocks multiple times to avoid the need to write to memory. This is increasing computation to reduce memory access.</p> <p>With fused operations you are trading accessibility of the code for efficiency. Simple matrix operations for attention (like multiplications and softmax) are easy to understand and modify. However, when running attention at scale, implementing an optimized fused version makes practical sense. Currently, billions of dollars are being spent running attention. It is important that we make this operation as efficient as possible.</p> <h3 id="recomputation">Recomputation</h3> <p>Typically, the attention matrix is stored in HBM during the forward pass for use in the backward pass. FlashAttention takes a different approach. It recomputes the attention matrix during the backward pass instead of storing it. While this requires computing the attention matrix twice, it eliminates the need to write the large matrix to memory.</p> <p>The softmax normalization parameters from the forward pass are still stored, enabling the backward pass attention computation to be completed in a single pass.</p> <p><strong>logsumexp</strong></p> <p>For each example in the batch, we can save only one value for the softmax normalization instead of two. We need to save both the max value \(m_i\) and the normalization sum \(l_i\). We can store the logsumexp \(L_i = m_i+\log(l_i)\). We’ll show how we can compute a correct softmax with this.</p> \[\begin{aligned} \text{softmax}(x^{(j)}) &amp;= \exp(x^{(j)}-L^{(j)}) \\ &amp;=\exp(x^{(j)}-m^{(j)}-\log l^{(j)}) \\ &amp;=\frac{\exp(x^{(j)}-m^{(j)})}{\exp(\log l^{(j)})} \\ &amp;=\frac{\exp(x^{(j)}-m^{(j)})}{ l^{(j)}} \end{aligned}\] <p>This small trick reduces the memory writes from the forward pass by half.</p> <h3 id="algorithm">Algorithm</h3> <p><strong>Forward Pass</strong></p> <ol> <li>Iterate through query blocks to get \(\text{Q}_i\) (this step runs in parallel)</li> <li>For each query block, iterate through key and value blocks to obtain \(\text{K}_j\) and \(\text{V}_j\) <ol> <li>Compute the attention block \(\text{S}_i^{(j)}=\text{Q}_i\text{K}_j^T\)</li> <li>Update the maximum value \(m_i^{(j)} = \max(m_i^{j-1}, \text{rowmax}(\text{S}_i^{(j)}))\), where rowmax finds the maximum in the current block</li> <li>Compute the unnormalized attention weights for the current block using the current maximum: \(\tilde{\text{P}}_i^{(j)}=\exp(\text{S}_i^{(j)}-m_i^{(j)})\)</li> <li>Update the block output: \(\text{O}_i^{(j)} = \text{diag}(e^{m_i^{(j-1)}} - m_i^{(j)})^{-1} \text{O}_i^{(j-1)} + \tilde{\text{P}}_i^{(j)} \text{V}_j\) <ol> <li>The first term adjusts the exponents from previous blocks</li> <li>This output remains unnormalized and contains only the softmax numerator</li> </ol> </li> </ol> </li> <li>Normalize the output block and write to HBM: \(\text{O}_i=\text{diag}(l_i^{(T_c)})^{-1}\text{O}_i^{T_c}\) <ol> <li>\(T_c\) represents the final block index after completing the loop</li> </ol> </li> <li>Compute logexpsum and save to HBM for the backward pass: \(L_i = m_i^{(T_c)} + \log(l_i ^{(T_c)})\)</li> </ol> <p><strong>Backward Pass</strong></p> <ol> <li>Initialize gradient buffers in HBM for \(\text{dQ}\), \(\text{dK}\), \(\text{dV}\) <ol> <li>Initialize \(D=\text{rowsum}(\text{dO}\circ O)\) in HBM</li> </ol> </li> <li>Iterate through key and value blocks to obtain \(K_j\) and \(V_j\) <ol> <li>Initialize \(\text{dK}_j\) and \(\text{dV}_j\) in SRAM</li> <li>Iterate through the output blocks and load \(\text{Q}_i\), \(\text{O}_i\), \(\text{dO}_i\), \(\text{dQ}_i\), \(L_i\), \(D_i\) to SRAM. <ol> <li>Recompute the attention block \(\text{S}_i^{(j)}=\text{Q}_i\text{K}_j^T\)</li> <li>Normalize the attention using the saved logsumexp \(\text{P}_i^{(j)}=\exp(\text{S}_i^{(j)}-L_i)\)</li> <li>Calculate the following gradients <ol> <li> \[\text{d}\text{V}_j \leftarrow \text{d}\text{V}_j + (\text{P}_i^{(j)})^\top \text{dO}_i\] </li> <li> \[\text{d}\text{P}_i^{(j)} = \text{dO}_i \text{V}_j^\top\] </li> <li> \[\text{d}\text{S}_i^{(j)} = \text{P}_i^{(j)} \circ (\text{d}\text{P}_i^{(j)} - D_i)\] <ol> <li>This is derived from the Jacobian of the softmax</li> </ol> </li> </ol> </li> <li>Update \(\text{dQ}_i \leftarrow \text{dQ}_i + \text{dS}_i^{(j)}\text{K}_j\) <ol> <li>This read from HBM and written back. This memory expensive step is necessary since we need to accumulate gradients for all query blocks before writing the final result</li> </ol> </li> <li>Update in SRAM: \(\text{dK}_j \leftarrow \text{dK}_j + \text{dS}_i^{(j)T}\text{Q}_j\)</li> </ol> </li> <li>Write \(\text{dK}_j\) and \(\text{dV}_j\) to HBM</li> </ol> </li> </ol> <hr/> <p>FlashAttention is a kernel fusion that torch.compile cannot find because it is a rewrite of the computation. NxN attention matrix is not materialized. This also allows larger values of N which is important for long context. It is now implemented in most deep learning frameworks.</p> <p>FlashAttention achieves its performance improvements through careful memory management and tiling of the attention computation. Rather than computing the full attention matrix at once, it processes smaller blocks of queries and keys, reducing the peak memory usage. This optimization is particularly important for training and inference with long sequences, where the quadratic memory scaling of attention would otherwise be prohibitive.</p> <p><strong>Additional Resources</strong></p> <ul> <li><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&amp;t=7s">Flash Attention derived and coded from first principles with Triton (Python) - Umar Jamil</a></li> <li><a href="https://www.youtube.com/watch?v=gMOAud7hZg4">FlashAttention - Tri Dao - Stanford MLSys #67</a></li> <li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/slides/cs224n-2024-lecture18-deployment-and-efficiency.pdf#page=6.00">Slides</a></li> </ul> <h1 id="mixture-of-experts-moe">Mixture of Experts (MOE)</h1> <p>Mixture of Experts (MoE) is transformer architectural change applied to all of the feed-forward blocks. Rather than having one feed-forward module in each transformer block, there are multiple parallel feed-forward blocks. A gating network selects among these blocks. These blocks are considered experts that specialize in different things. Multiple blocks are selected and the output embeddings are added together, creating a mixture.</p> <p>MoEs are considered to be sparse models. MoE models can have very high numbers of trainable parameters, but have only a fraction used for a given example during training and inference.</p> <p>MoE is generally structured as follows:</p> <ol> <li>Create \(n\) experts, which are separate versions of the feed-forward MLP block</li> <li>A gating network / router processes the token embedding \(x\) and outputs a score for each embedding. This is a vector of size \(n\). <ol> <li>The gating network is typically implemented as a single learned MLP layer: \(W_gx\)</li> </ol> </li> <li>We select the top k experts and route the token embedding to these experts.</li> <li>The output is a weighted sum between the outputs of the k selected experts <ol> <li>A softmax is used on the scores of the top k experts to normalize the weights</li> </ol> </li> </ol> <p>The number of selected experts \(k\) is chosen to be small (typically 2) to achieve this sparsity. With this approach and parameter setting, we achieve the goal of a sparse transformer architecture.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/moelayer-480.webp 480w,/assets/img/blog/transformer_pt2/moelayer-800.webp 800w,/assets/img/blog/transformer_pt2/moelayer-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/moelayer.png" class="image-fluid mx-auto d-block" width="500" height="auto" alt="MoE Layer" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2401.04088"> Source </a> </figcaption> </figure> <p>We want all \(n\) experts to be utilized with roughly equal frequency. In the worst case, the same \(k\) experts are selected for every input, and the remaining experts are just a waste of space. While each input should activate only a few experts, we want to ensure that all experts are activated with roughly equal frequency over the entire training set. We want to encourage diversity in expert selection. Different MoE implements achieve this differently.</p> <h2 id="sparsely-gated-moe-2017">Sparsely Gated MoE (2017)</h2> <p><a href="https://arxiv.org/abs/1701.06538">Shazeer et al. 2017</a> introduces mixture of experts for language modeling, however, using LSTMs instead of transformers.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/lstm_moe-480.webp 480w,/assets/img/blog/transformer_pt2/lstm_moe-800.webp 800w,/assets/img/blog/transformer_pt2/lstm_moe-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/lstm_moe.png" class="image-fluid mx-auto d-block" width="600" height="auto" alt="MoE on LSTM" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/pdf/1701.06538"> Source </a> </figcaption> </figure> <p>In order to enforce diversity, they add tunable Gaussian noise to the expert score \((xW_g)_i\) of each expert in the function \(H()\).</p> \[H(x)_i = (x \cdot W_g)_i + \text{StandardNormal()}\cdot \text{Softplus}((x \cdot W_{noise})_i)\] <p>The noise for each component is learned through \(W_{noise}\). A load balancing loss is optimizes the magnitude of the loss for different experts. Without a proper loss term, the noise would collapse to 0. This noise term is important for the load balancing loss.</p> <p>Softplus is similar to ReLU but smooth and always non-negative: \(\text{Softplus}(x)=\log(1+e^x)\). This is just to make sure the noise added is non-negative.</p> <p>After adding the noise, we apply the \(\text{KeepTopK}\) function. This sets the values to \(-\infty\) for experts outside of the top \(k\) to ignore them in the softmax. After the softmax, only two experts have weights, which add up to 1. The final weights on the experts is calculate as:</p> \[G(x) = Softmax(KeepTopK(H(x), k))\] <p>The softmax normalizes the weights of the two experts, which can be used to compute a weighted sum of the outputs of the selected experts.</p> <p>For diversity, an additional importance loss term is used. Given an expert \(i\) and a training batch \(X\), we define importance as follows:</p> \[Importance(i,X)=\sum_{x\in X}G_i(x)\] <p>We then sum this across experts to define an auxiliary loss, which is minimized when all the experts are activated equally on the batch:</p> \[L_{importance,i}(X) = w_{importance} \cdot CV(Importance(i,X))^2\] <p>However, one shortcoming of the importance loss term is that it uses the weights. It is possible for an expert to have a high average weight, but never be selected in \(KeepTopK\). They define another loss \(L_{load}\) weighted by \(w_{load}\) to address this. We want a smooth estimation of the number of examples assigned to each expert. The definition of this is out of scope for this post, but it is designed to be smooth operation on a discrete operator.</p> <p>A key challenge with MoE is that each expert only sees a fraction of the training batch, limiting its learning effectiveness. To address this, one solution is to use data parallelism with very large batch sizes.</p> <p>MoE also requires careful handling in multi-device training setups. Through expert parallelism—a form of model parallelism—different devices manage different experts. After the gating function determines expert assignments, tokens are shuffled between GPUs in a ring pattern before returning to their original GPU. This allows us to use MoE to scale the number of parameters in a model, but there is an added communication cost with expert parallelism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/expert_parallelism-480.webp 480w,/assets/img/blog/transformer_pt2/expert_parallelism-800.webp 800w,/assets/img/blog/transformer_pt2/expert_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/expert_parallelism.png" class="image-fluid mx-auto d-block" width="500" height="auto" alt="Expert Parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://pytorch.org/blog/training-moes/"> Source </a> </figcaption> </figure> <p>The auxiliary loss is calculated separately for each MoE layer in the transformer.</p> <h2 id="gshard">GShard</h2> <p><a href="https://arxiv.org/abs/2006.16668">GShard</a> (Lepikhin et al. 2020) applies MoE to transformer, also with k=2 experts. Each expert implements the same two-layer MLP architecture used in standard transformers. GShard uses a simpler auxiliary loss term to encourage expert diversity. It is simply \(\frac{c_e}{S}m_e\) where \(e\) is a particular expert. This loss is averaged across all experts. \(c_e/S\) is the fraction of the \(S\) tokens routed to expert \(e\). This is not differentiable because \(c_e\) is determined by the top k operator. This ratio acts as a weight on the mean weight of the expert \(m_e\) which is differentiable. The authors were able to train very large MoE transformer models while using this loss to balance tokens between experts.</p> <p>The expert with the highest score is selected first. For the second expert, rather than choosing the next highest score, the system randomly samples from the remaining experts. This random selection promotes diversity, functioning similarly to the Gaussian noise term.</p> <h2 id="switch-transformer">Switch Transformer</h2> <p><a href="https://arxiv.org/abs/2101.03961">Fedus et al. 2022</a> applies MoE to transformers with some simplifications compared to the 2017 paper. They only use one expert by setting \(k\) to 1. Instead of having separate loading balancing and importance losses, they use one auxiliary loss term (same loss as GShard):</p> \[\mathcal{L}_{aux} = \alpha N \sum_{i=1}^{N} f_i P_i\] <p>\(f_i\) is the fraction of tokens routed to expert \(i\) and \(P_i\) is the fraction of router probability routed to expert \(i\). These are among all tokens in a batch. \(f_i\) is analogous to the load balancing loss and \(P_i\) is analogous to the importance loss. \(P\) is differentiable, while \(f_i\) is not. The loss works because \(f_i\) is treated as a weight on a differentiable loss. The multiplication with \(N\) ensures that the optimal loss is the same with different numbers of experts.</p> <h2 id="frontier-llms">Frontier LLMs</h2> <h3 id="mixtral">Mixtral</h3> <p>MoEs gained significant popularity in LLMs with the <a href="https://arxiv.org/pdf/2401.04088.pdf">Mixtral</a> paper from Mistral AI. The model uses 8 experts but selects only 2 for each computation, with SwiGLU serving as the expert function. The MoE block operates as follows, where \(x\) represents input token embeddings and \(y\) represents output token embeddings:</p> \[y = \sum_{i=0}^{n-1} \text{Softmax}(\text{Top2}(x \cdot W_g))_i \cdot \text{SwiGLU}_i(x)\] <p>While the paper doesn’t detail the exact auxiliary loss, the Hugging Face <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/mixtral/modeling_mixtral.py#L886">implementation</a> extends the Switch Transformer loss to work with any number of selected experts.</p> <h3 id="deepseek">DeepSeek</h3> <p>DeepSeek shares more <a href="https://arxiv.org/abs/2401.06066">details</a> in how they train their MoE models. They do not use an auxiliary loss. Instead, they add a bias term \(b_i\) to the scores of each expert. At the end of each training step, they update the biases by \(\gamma\), which they term as the bias update speed. If the expert is overloaded, the bias is subtracted. If it is underloaded, it is added. This is heuristic method that avoids the challenges of optimizing an auxiliary loss.</p> <p>Although they don’t use an auxiliary loss for load balancing between experts, they do use one to encourage expert diversity within sequences. Their goal is for tokens within a single example to use different experts. This contrasts with other methods and challenges the intuition that experts should specialize in specific topics. They find empirically that this reduces expert specialization. They implement this similarly to the Switch Transformer loss but average the probabilities within a sequence. However, a recent paper by <a href="https://arxiv.org/abs/2408.15664">Wang et al. 2024</a> suggests this loss isn’t necessary for achieving expert diversity across sequences.</p> <p>They also introduce the notion of shared experts. They have routed experts which are selected by a gating function. The shared experts are always used. These are meant to store more general knowledge, while the routed experts are more specialized. This requires having more experts activated if you want to have more than 1 routed expert. DeepSeek-V3 uses high numbers of experts (ex: 2 shared, 64 routed). To implement this efficiently, they implement each expert as a 2 layer MLP with a small hidden dimension.</p> <p>MoE represents one of the most significant improvements to the transformer architecture. However, the research community has yet to converge on a standard implementation. Different papers approach expert diversity in varying ways. Understanding these documented methods provides a foundation for designing your own MoE model. The technique’s success has led to applications beyond language models into other domains, such as the <a href="https://arxiv.org/abs/2106.05974">Vision MoE</a> (V-MoE) paper.</p> <p><strong>Additional Resources</strong></p> <ul> <li><a href="https://www.youtube.com/watch?v=RcJ1YXHLv5o&amp;list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&amp;index=29&amp;ab_channel=StanfordOnline">Mixtral AI Talk</a></li> <li><a href="https://huggingface.co/blog/moe">Mixture of Experts Explained - Hugging Face</a></li> <li><a href="https://pytorch.org/blog/training-moes/">Training MoEs at Scale with PyTorch</a></li> <li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1c7h4wq/why_llama_3_is_not_a_moe/">Why Llama 3 is not a MoE? - /r/LocalLLaMA</a></li> <li><a href="https://github.com/huggingface/transformers/issues/31464">Bug</a> from the Hugging Face Mixtral code explaining the auxiliary loss</li> <li>Mixtral MoE inference <a href="https://github.com/mistralai/mistral-inference/blob/main/src/mistral_inference/moe.py">code</a></li> <li>DeepSeek-V3 MoE inference <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/model.py#L331">code</a></li> </ul> <h1 id="kv-cache">KV Cache</h1> <p>Modern transformer architectures have introduced several innovations to improve decoding efficiency, especially for handling long sequences and reducing memory usage during inference. These improvements are essential when deploying large language models in production environments with limited computational resources.</p> <p>Decoding with an LLM involves two main steps:</p> <ol> <li>Prefill: Processing the input prompt and prior context</li> <li>Decoding: Generating new tokens</li> </ol> <p><a href="https://arxiv.org/abs/2211.05102">Efficiently Scaling Transformer Inference</a> (Pope et al., 2022) explores how these decoding steps can be optimized. In language model decoding, both the context and previously generated tokens are needed to predict the next token. However, reprocessing these tokens from scratch would be inefficient. Instead, we can save the intermediate outputs of prior tokens to avoid recomputation.</p> <p>Only the keys and values from the attention layers need to be preserved. Since transformer decoders use causal attention, new tokens don’t influence the embeddings of previous tokens. We can safely discard the query and output embeddings for prior tokens as they’re no longer needed for computation. The key and value embeddings are kept in the KV cache, allowing newly decoded tokens to attend to previous ones.</p> <p>During the prefill step, a KV cache is generated, containing the attention key and value tensors from each layer. This cache grows as each new token is decoded.</p> <p>The quadratic growth of KV cache with context length presents a significant scaling constraint. When we say a model uses a KV cache, it means the keys and values from previous tokens are preserved during the decoding process. The KV cache can have significant memory utilization as the number of tokens increases. However, the KV caching mechanism means that for the same model and hardware, more tokens can be processed during inference than during training.</p> <h1 id="mqagqa">MQA/GQA</h1> <p>Multi Query Attention (MQA) and Group Query Attention (GQA) are architectural changes to the attention layers of the model. These methods slightly hurt the model’s performance, but enable more efficient decoding.</p> <h2 id="multi-query-attention-mqa">Multi Query Attention (MQA)</h2> <p>Multi-Query Attention was introduced in the paper <a href="https://arxiv.org/abs/1911.02150">Fast Transformer Decoding: One Write-Head is All You Need</a>.</p> <p>In standard multi-head attention, each attention head has its own set of query (Q), key (K), and value (V) projections. This means that for \(h\) heads, each token will have \(h\) query, key, and value embeddings.</p> <p>MQA modifies this by using a single shared key and value head across all query heads. There are \(h\) query embeddings but only one key and value embedding. All of these embeddings are of size \(d_{model}/h\). This significantly reduces the memory footprint of the model. This is especially impactful when decoding with a long context length.</p> <p>MQA trades a minor decrease in model performance for substantial memory efficiency. The size KV cache size by a factor equal to the number of attention heads. For example, with 32 attention heads, MQA requires only 1/32 of the KV cache memory compared to standard multi-head attention. This enables decoding with longer contexts.</p> <p>Multi query attention maybe more aptly named single key and value attention, since MHA already has multiple queries. The difference is that MQA doesn’t have multiple keys and values.</p> <h2 id="group-query-attention-gqa">Group Query Attention (GQA)</h2> <p>Group-Query Attention was introduced in the paper <a href="https://arxiv.org/abs/2305.13245">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt2/mqa_gqa-480.webp 480w,/assets/img/blog/transformer_pt2/mqa_gqa-800.webp 800w,/assets/img/blog/transformer_pt2/mqa_gqa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt2/mqa_gqa.png" class="image-fluid mx-auto d-block" width="500" height="auto" alt="MHA, MQA, GQA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/pdf/2305.13245"> Source </a> </figcaption> </figure> <p>GQA is a middle ground between standard multi-head attention and Multi-Query Attention (MQA). Instead of sharing a single key-value head across all query heads (as in MQA), GQA shares key-value heads among groups of query heads.</p> <p>For example, if we have 8 query heads (\(h=8\)) and 2 key-value heads (\(g=2\)), each key-value head would be shared by 4 query heads. This provides a better balance between computational efficiency and model quality compared to MQA.</p> <p>There is reduced computational cost in that \(g\) keys and values need to be computed instead of \(h\). However, the main motivation is memory saving. <a href="https://arxiv.org/abs/2211.05102">Pope et al., 2022</a> shows that the memory savings of GQA enable longer context lengths. The computation of MHA might be slower. But the memory requirements can make longer context lengths impossible, due to out of memory errors. With more heads, the resource savings are further multiplied.</p> <p>GQA is currently more popular as it offers a tunable tradeoff between efficiency and quality. MQA is a special case of GQA where the number of groups is 1. Both MQA and GQA take advantage of the fact that in attention the number of keys/values and queries are not required to be the same.</p> <h1 id="conclusion">Conclusion</h1> <p>This blog post is meant to bridge the gap between “Attention Is All You Need” and frontier LLMs. While the transformer architecture has evolved significantly over the years, the original design remains remarkably relevant and far from obsolete. Very few architectural changes have been universally adopted. Individual transformer applications make different design decisions, but the transformer itself has remained universal. There have also been challengers to the transformer architecture, such as <a href="https://arxiv.org/abs/2312.00752">Mamba</a> . For the time being, I expect that the transformer is here to stay.</p>]]></content><author><name></name></author><category term="transformer"/><summary type="html"><![CDATA[While the core transformer architecture introduced in “Attention is All You Need” remains effective and widely used today, numerous architectural improvements have emerged since its inception. Unlike the dramatic evolution seen in Convolutional Neural Networks (CNNs) from AlexNet onwards, transformer modifications tend to be more incremental and optional - the original architecture still serves as a strong baseline. This speaks to the elegant design choices made by the original authors, while leaving room for ongoing optimizations in efficiency, context length, and multimodal capabilities.]]></summary></entry><entry><title type="html">Transformer Design Guide (Part 1: Vanilla)</title><link href="https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt1/" rel="alternate" type="text/html" title="Transformer Design Guide (Part 1: Vanilla)"/><published>2024-11-03T00:00:00+00:00</published><updated>2024-11-03T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt1</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Transformer-Design-Guide-Pt1/"><![CDATA[<p>The Transformer architecture has emerged as the cornerstone of deep learning and artificial intelligence. Despite its conceptual simplicity, the specific details of the architecture can be difficult to understand and reason about. This two-part blog series aims to provide a thorough examination of the Transformer, demystifying its core components and recent advancements. The goal is to cover the fundamental and cutting-edge concepts needed to design transformer-based models for any application in any modality.</p> <p>This blog post will be in two parts:</p> <p><strong>Part 1 will be a deep dive of the standard Transformer architecture.</strong> It is a highly modular architecture, so we will explain each component in detail and how they integrate. This will also cover how to design the components for different use cases. It was introduced by the famous paper <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. There is no shortage of resources to learn about transformers, but I hope to offer some new perspectives.</p> <p><strong>Part 2 will cover recent advancements that have further advanced the capabilities of transformers.</strong> The original transformer architecture is robust and versatile and has led to many successful applications. However, in recent years with the surge in investment into transformers / LLMs, we have seen many useful advances. These impart new capabilities such as longer context length, faster training, and more efficient inference. This is a guide to designing modern transformer architectures for any use case.</p> <h1 id="transformer-architecture">Transformer Architecture</h1> <p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> introduced the transformer architecture in 2017 specifically for machine translation. Since then, architectures derived from this have been used not only for various NLP tasks but also for other modalities such as vision, audio, and time series. We’ll take a modality-agnostic approach. As we explore each component, we’ll focus on how to design it for different modalities and use cases. For instance, position embeddings might be designed differently for text than for images.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/transformer-480.webp 480w,/assets/img/blog/transformer_pt1/transformer-800.webp 800w,/assets/img/blog/transformer_pt1/transformer-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/transformer.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="Transformer diagram" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Annotated from source: <a href="https://arxiv.org/abs/1706.03762"> Source </a> </figcaption> </figure> <p>Transformers are so generalizable because they have relatively few inductive biases. Unlike CNNs, which require Euclidean input, Transformers do not enforce a data structure. It is up to the designer to incorporate domain-specific inductive biases into the model. These design decisions are important for the model to be effective at a given task.</p> <hr/> <p>The transformer architecture can be understood as 3 steps.</p> <ol> <li>Input processing (Generation of a set of embeddings to input into the transformer)</li> <li>Transformer blocks (Bulk of the computation)</li> <li>Output processing (Using the output embeddings of the transformer to perform a task and train the model)</li> </ol> <h1 id="input-processing">Input Processing</h1> <p>The input to the transformer is an unordered set of embeddings. These embeddings are high dimension vectors of floating point values that represent a part of the input. We refer to input processing as the steps taken to compute these embeddings. Input processing changes the most between modalities.</p> <p>The general pattern for input processing is as follows</p> <ol> <li>Split up the input into pieces</li> <li>Map each piece to an embedding</li> </ol> <p>The output of these two steps is a set of embeddings that represent the raw input in a way the transformer architecture can process.</p> <h2 id="image-processing">Image Processing</h2> <p>Transformers for images were introduced in the <a href="https://arxiv.org/abs/2010.11929">ViT</a> paper. The input image is processed in two steps:</p> <ol> <li>The image is split into patches of 16x16.</li> <li>The pixel values of each patch are flattened to vector and fed through a learned linear projection, resulting in patch embeddings.</li> </ol> <p>The result is a set of embeddings that can be processed by the transformer.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/vit_input-480.webp 480w,/assets/img/blog/transformer_pt1/vit_input-800.webp 800w,/assets/img/blog/transformer_pt1/vit_input-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/vit_input.png" width="100%" height="auto" alt="Image Tokenization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> </figcaption> </figure> <h2 id="text-tokenizer">Text Tokenizer</h2> <p>Text is represented as a sequence of characters or bytes. Unlike images, text isn’t inherently numerical data that can be directly transformed into embeddings. Text is processed by tokenization, which is mapping it to a sequence of discrete tokens. Tokenizers create a vocabulary, which is mapping of all possible tokens to vocab indices. These indices are used to retrieve a learned embedding from a table. Text input processing involves two steps: tokenization and embedding lookup.</p> <p>Let’s consider two basic options for tokenization:</p> <ul> <li><strong>Character-Level Tokenization:</strong> Every character in the text becomes a separate token. This creates a very long sequence but has a very small vocabulary.</li> <li><strong>Word-Level Tokenization:</strong> Each word is a distinct token. This results in a more manageable sequence length but results in a huge vocabulary.</li> </ul> <p>There is an obvious tradeoff between vocabulary size and input size. Character-level tokenization creates very long sequences, which can be inefficient for transformers. On the other hand, word-level tokenization can lead to a massive vocabulary size, requiring a large embedding table. This can be computationally expensive and struggle with unseen words or typos.</p> <p>The ideal approach considers several factors:</p> <ul> <li><strong>Sequence Length:</strong> Shorter sequences are generally more efficient for processing, but extremely short sequences may not capture enough context.</li> <li><strong>Embedding Table Size:</strong> A larger vocabulary requires a bigger embedding table, increasing memory usage and training time.</li> <li><strong>Rare Words:</strong> Very infrequent tokens may not be adequately learned during training, impacting model performance.</li> <li><strong>Token Complexity:</strong> A single token shouldn’t represent too much information. Complex concepts might benefit from being broken down into smaller tokens for better processing by the model.</li> </ul> <p>In practice, finding the right balance often involves a compromise between character-level and word-level tokenization. Techniques like subword tokenization (splitting words into smaller meaningful units) can offer a middle ground, achieving a balance between sequence length, vocabulary size, and capturing text information effectively.</p> <h3 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h3> <p>The most common approach to implementing sub-word tokenization is Byte Pair Encoding (BPE).</p> <p>It works by first starting with the individual characters (bytes) in the text as its initial vocabulary. This ensures that all text can be encoded, though not efficiently. BPE then iteratively identifies the most frequently occurring pair of characters and merges them into a single new token. This process continues until a predefined maximum number of tokens is reached, preventing the vocabulary from becoming too large.</p> <p>One interesting feature of this approach is that the tokenizer uses a small and separate dataset for BPE. This dataset can be engineered to achieve certain properties in the tokenizer. For example, it is beneficial for this data to be balanced between different languages. For example, if the amount of data for Japanese is significantly lower than that for English. Rare pairs in English would be prioritized over common pairs in Japanese. This would be unfair to Japanese, and Japanese text would require far more tokens. To address this, the tokenizer dataset can be balanced between different languages.</p> <p>See <a href="https://platform.openai.com/tokenizer">platform.openai.com/tokenizer</a> for an interactive demo on how text is tokenized and mapped in token indices.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/text_tokenization-480.webp 480w,/assets/img/blog/transformer_pt1/text_tokenization-800.webp 800w,/assets/img/blog/transformer_pt1/text_tokenization-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/text_tokenization.png" width="100%" height="auto" alt="Example of tokenized text." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Example of tokenized text <a href="https://platform.openai.com/tokenizer"> Source </a> </figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/token_indices-480.webp 480w,/assets/img/blog/transformer_pt1/token_indices-800.webp 800w,/assets/img/blog/transformer_pt1/token_indices-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/token_indices.png" width="100%" height="auto" alt="Generated token indices" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Generated token indices <a href="https://platform.openai.com/tokenizer"> Source </a> </figcaption> </figure> <p>BPE also involves some hardcoded rules. Some bytes, such as punctuation can be ignored in the tokenizer merging. GPT tokenizers use different regex patterns to split the string prior to tokenization to prevent certain bytes from merging.</p> <p>BPE is expensive to run and to encode/decode text since it is an iterative process. This is optimized by OpenAI (<a href="https://github.com/openai/tiktoken">tiktoken</a>) by implementing it in Rust. <a href="https://github.com/google/sentencepiece">SentencePiece</a> by Google is another popular tokenizer. SentencePiece runs BPE on Unicode code points (Unicode characters). It falls back to bytes for rare code points. Unicode has nearly 150k code points, a large number of which are very rare. Most tokenizers use less than 100k tokens. Having 150k tokens before adding more through BPE is not practical.</p> <p>Once we have a trained tokenizer, we use it to map input text to token indices. These token indices are mapped to learned embeddings. Transformer models often include embedding tables, which store learned embeddings for each item in the model’s vocabulary.</p> <p>See this <a href="https://www.youtube.com/watch?v=zduSFxRajkE&amp;t=24s&amp;ab_channel=AndrejKarpathy">video</a> by Andrej Karpathy for a deep dive into text tokenizers.</p> <h2 id="audio-and-other-modalities">Audio and Other Modalities</h2> <p>Like images, audio is a continuous data modality. A popular method of tokenizing audio is to generate a spectrogram using a Fourier Transform. This creates an image that can be tokenized in the same way as images in ViT. The <a href="https://arxiv.org/abs/2104.01778">AST: Audio Spectrogram Transformer</a> paper does exactly this.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/audio_input-480.webp 480w,/assets/img/blog/transformer_pt1/audio_input-800.webp 800w,/assets/img/blog/transformer_pt1/audio_input-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/audio_input.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="Audio tokenization from AST" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Audio tokenization from AST <a href="https://arxiv.org/abs/2104.01778"> Source </a> </figcaption> </figure> <p>This paper uses a 2D position embedding so they can warmstart from a ViT model. If it were to train from audio only, a 1D position embedding could be used, as in OpenAI’s <a href="https://arxiv.org/abs/2212.04356">Whisper</a>.</p> <p>We have covered the basic methods for tokenizing text and continuous data domains, however, there is a lot of research covering alternative methods. This includes vector quantization which generates discrete tokens from continuous data modalities.</p> <h1 id="position-embedding">Position Embedding</h1> <p>The transformer takes a set of tokens as input. However, many inputs are better represented as sequences, such as text and audio Where a token occurs in the sequence is a crucial piece of information. Position embeddings can be added to the token embeddings to encode the position of the token in the sequence. Although the input is still a set, we are not losing the information of the order of the tokens within the input sequence. Position embeddings implicitly turn the transformer from a set processing architecture to a sequence processing one.</p> <p>The original transformer paper evaluates two methods of configuring the position embedding. These have equivalent results.</p> <ul> <li>Learned: A separate learned embedding is used for every position in the sequence up to the maximum sequence length.</li> <li>Fixed: The embedding values aren’t learned but are configured as a function of the position. \(i\) is the index in the embedding. \(d_{model}\) values have to be generated so the position embedding can be the same dimension as the token embedding.</li> </ul> \[PE_{(pos,2i)} = \sin\left(pos/10000^{2i/d_{model}}\right) \\ PE_{(pos,2i+1)} = \cos\left(pos/10000^{2i/d_{model}}\right)\] <p>These embeddings are added to the input token embeddings. This assumes that the addition of the positional encoding doesn’t cause conflicts in the embedding space (which is typically in a high dimension). However, it is also possible to concatenate position embedding values.</p> <p>For language, 1D position encodings are used. These embeddings should be designed to fit the data. For example, in images, the position embedding is 2 dimensional. For videos, an additional time dimension can be added. The embedding can be designed in any way to encode the structure of the data.</p> <h1 id="transformer-blocks">Transformer Blocks</h1> <p>The transformer blocks are where the bulk of the computation takes place. We will first go through the components needed to build these blocks, and then put them together.</p> <h2 id="attention">Attention</h2> <p>The core component of the Transformer architecture, as highlighted in the title “Attention Is All You Need,” is Attention. This concept predates transformers in Natural Language Processing (NLP). The fundamental equation for attention is:</p> \[\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V\] <p>Attention comes in two forms: self-attention and cross-attention. We’ll begin with self-attention. It can be conceptualized as a set-to-set mapping of embeddings where information is shared among all embeddings. Here’s how it works:</p> <p>\(Q\), \(K\), and \(V\) represent queries, keys, and values. These represent different linear projections of the input embeddings that are used in the attention operation. You can think of attention as tokens communicating information with each other. Each token’s query determines which other tokens it wants to read from, while its key determines which tokens will read from it. When a token’s query matches well with another token’s key, it receives more of that token’s value. The value represents the information that the token shares with others . Through these learned projections, the model determines how information flows between tokens.</p> <p>Attention is implemented by first generating these three matrices. Let’s say the input embeddings are stored in a matrix \(X\). Learned weight matrices \(W^Q\) , \(W^K\), and \(W^V\). Are used to project the input embeddings: \(Q = W^QX, K = W^KX, V = W^VX\).</p> <p>At this stage, no information has been transferred between tokens. Given the variable number of tokens, we can’t use a single large MLP layer. To mix the information, we set each embedding to be a weighted sum of all value embeddings.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/scaled_attention-480.webp 480w,/assets/img/blog/transformer_pt1/scaled_attention-800.webp 800w,/assets/img/blog/transformer_pt1/scaled_attention-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/scaled_attention.png" class="image-fluid mx-auto d-block" width="200" height="auto" alt="Scaled Dot-Product Attention" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Scaled Dot-Product Attention <a href="https://arxiv.org/abs/1706.03762"> Source </a> </figcaption> </figure> <p>To compute this weighted sum, we first compute the attention matrix \(QK^T\). This matrix is of shape \((N, N)\). This is the source of the \(N^2\) complexity of transformers. The attention matrix contains scores for every combination of token query and key embeddings: \(q*k\), which is scaled by a factor \(\frac{1}{\sqrt{d_k}}\). This scaling is applied to normalize the gradients, such that the magnitude of the dot product isn’t dependent on the embedding dimension. This specific attention formulation is called scaled dot-product attention.</p> <p>A softmax is applied to each row or column, creating a weight vector for each token. This is multiplied by the value matrix to generate a weighted sum for each token. Because all matrices are learned, each token can determine which tokens to attend to, which tokens should attend to it, and what information to broadcast. This function is highly flexible. A token could even learn to nullify its own information and instead read from other tokens.</p> <p>Self-attention is like a fully connected neural network layer in that information from all tokens can propagate to other tokens. However, self-attention has the benefit of supporting variable length input.</p> <h2 id="cross-attention">Cross-Attention</h2> <p>Self-attention is a mechanism where queries, keys, and values all derive from the same set of input embeddings. In contrast, cross-attention operates on two distinct sets of embeddings, which can have different lengths. The queries come from one set, while the keys and values come from another.</p> <p>In cross-attention, the attention matrix \(QK^T\) is of shape \((N_Q, N_K)\) . \(N_Q\) is the sequence length of the queries, and \(N_K\) is the sequence length of keys. The softmax is taken on the rows, so each query token has a probability distribution with respect to keys.</p> <p>Cross-attention is particularly relevant for machine translation. In this context, the keys and values come from the source language text, while the queries come from the target language. As the model generates the translation, it attends to the set of tokens from the source language, allowing it to draw information from the original text throughout the translation process.</p> <h2 id="masked-self-attention">Masked Self-Attention</h2> <p>The transformer decoder uses causal masking. The decoder is trained to predict the next token. This task becomes trivial if the next token and all future tokens are visible, as in full self-attention. Causal masking constrains the attention operation to only look at tokens to the left, making the decoder auto-regressive (meaning each output depends only on previous outputs). This one-way flow of information is essential for generating sequences one token at a time.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/masked_attention-480.webp 480w,/assets/img/blog/transformer_pt1/masked_attention-800.webp 800w,/assets/img/blog/transformer_pt1/masked_attention-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/masked_attention.png" width="100%" height="auto" alt="Example of a masked attention matrix" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Example of a masked attention matrix </figcaption> </figure> <p>Masking is applied on the \(QK^T\) matrix. Masked indices are set to \(-\infty\), this causes the softmax function to assign zero weight to these tokens. In many implementations of attention, the mask can be customized by passing in a Boolean matrix.</p> <h2 id="multi-head-attention">Multi-Head Attention</h2> <p>Multi-head attention (MHA) is a way to increase the expressivity of the attention operator. It is essentially running multiple attention operations in parallel and concatenating the output. This improves expressivity because each head is free to attend to different tokens.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/mha-480.webp 480w,/assets/img/blog/transformer_pt1/mha-800.webp 800w,/assets/img/blog/transformer_pt1/mha-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/mha.png" class="image-fluid mx-auto d-block" width="300" height="auto" alt="Multi-Head Attention" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Multi-Head Attention <a href="https://arxiv.org/abs/1706.03762"> Source </a> </figcaption> </figure> <p>Multi-head attention has two scaling parameters. Feature dimension for each head \(d_v\), and number of heads \(h\).</p> <p>Each head projects the input embeddings into queries, keys, and values of size \(d_v\). This means that the weight matrices \(W^Q\) , \(W^K\), and \(W^V\) are of size \((d_{model}, d_v)\). Attention is applied to the set of queries, keys, and values independently. This results in \(h\) sets of output embeddings of size \(d_v\). The output embeddings of each head are concatenated resulting in embeddings of size \(d_v*h\). The output needs to be the same dimension as the input, so there is linear projection back to size \(d_{model}\).</p> <p>Typically the embedding dimension to each head is \(d_v = d_{model}/h\). In this case, the concatenated output is the same dimension as the input token embeddings. However, it is possible to set \(d_v\) to be higher or lower.</p> <p>The output projection, which is a linear layer \(W^O\), learns to combine the outputs of the different heads. The output size of this layer is the same as the input token embedding size. This layer allows the model to give different importance to different attention heads. When \(d_v\) is set to \(d_{model}/h\), this projection is not required for dimensionality matching. However, it is beneficial in that the information from different heads can be mixed before the residual connection.</p> <p>Multi-head attention (MHA) effectively divides the softmax operation into separate parts. Each head has a fixed amount of attention weight to distribute among different value functions. This multi-headed approach allows for more complex token interactions. One of the advantages of the attention mechanism is its interpretability. For each head, it’s possible to examine which tokens are attending to which other tokens, providing insight into the model’s internal workings.</p> <h2 id="normalization">Normalization</h2> <p>In transformer architectures, layer normalization is typically used. Unlike batch normalization, the values are independent of other items in the batch. This is because batch-wide statistics aren’t used. Layer Normalization was <a href="https://arxiv.org/abs/1607.06450">introduced</a> just a year prior to transformers.</p> <p>For each set \(x\) in the batch, the mean \(\mu(x)\) and variance \(\sigma(x)^2\) of the embedding values (across all \(d_{model}\) values of each embedding) are calculated. LayerNorm operates on each embedding in the input completely independently. These values are used to normalize each embedding value:</p> \[\mathrm{LN}(x) = \frac{x-\mu(x)}{\sqrt{\sigma(x)^2+\epsilon}} *\gamma +\beta\] <p>\(\gamma\) and \(\beta\) are learned scalar parameters. \(\epsilon\) is a small constant used for numerical stability.</p> <p>Layer norm is effective for multiple reasons. Since batch statistics aren’t used, it makes data parallelism more efficient. This is because the batch statistics do not have to be communicated between GPUs. LayerNorm is also not affected by the size of the batch, which means different batch sizes can be used at different times.</p> <p>In <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>, the layer normalization occurs after each attention and feed-forward layer (Post-LN architecture). However, it is now more popular to put the layer normalization before these layers (Pre-LN architecture). This <a href="https://arxiv.org/abs/2002.04745">paper</a> from 2020 shows that the Pre-LN architecture generally performs better. This is the only fundamental change to the original transformer architecture.</p> <h2 id="feed-forward">Feed-Forward</h2> <p>After each attention layer, a small feed forward neural network processes each token embedding. This is a position-wise operation. In the original paper, this is a two layer network with a ReLU activation after the first layer. The first layer outputs a dimension \(d_{ff}=2048\). The second layer projects these embeddings back to the original token embedding dimension \(d_{model}=512\). The first layer is set to the 4x the size of the token embedding. This multiplier is arbitrary but is considered to be an effective value given the efficiency tradeoff.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/feed_forward-480.webp 480w,/assets/img/blog/transformer_pt1/feed_forward-800.webp 800w,/assets/img/blog/transformer_pt1/feed_forward-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/feed_forward.png" class="image-fluid mx-auto d-block" width="200" height="auto" alt="Feed Forward Layer" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Feed Forward Layer </figcaption> </figure> <p>The attention layer has \(4*d_{model}*d_{model}\) parameters (accounting for query, key, value, and output projection matrices), which is \(1.0*10^6\) for the default embedding size. The feed forward layer has \(d_{model}*d_{ff} + d_{ff}*d_{model}\) which is over \(2.1*10^6\) parameters with the default configuration. When \(d_{ff} = 4*d_{model}\), this is equivalent to \(8*d_{model}*d_{model}\). The feed forward layer has roughly twice the number of parameters.</p> <p>The attention layer computation scales quadratically with input length (default value is \(n=1024\)). The computational complexity of the feed forward layers is \(n*d_{model}*d_{ff}\). For attention, it is \(n^2d_{model}+d_{model}*d_{model}\). A recent trend is increasing the sequence length \(n\), which causes the attention layer to further dominate the computational cost. Due to the complexity of the attention operation and the different ways to implement it on hardware, we will skip calculating numerical values of the computation.</p> <p>The feed forward layers contain the bulk of the transformer’s parameters, while the attention layers have the bulk of the computation. Attention is meant to learn the relationships between tokens, while the feed forward layers are meant to learn the individual token representations themselves. The attention operation is computationally intense in modeling the relationships between tokens, but it does not process individual token embeddings as much. The feed forward layers complement attention by enabling complex transformations of these embeddings.</p> <p>This <a href="https://arxiv.org/abs/2012.14913">paper</a> by Geva et al. argues that the feed forward layers act as key value memories. The high parameter counts of these layers enable the model to store rich information about the data they are trained on. Models like GPT-4 may not have their impressive world knowledge without the storage capacity of the feed forward layer. The transformer is a powerful architecture due to its balance of computational complexity and high parameterization.</p> <h2 id="blocks">Blocks</h2> <p>Now that we have covered each component, we can describe the transformer blocks. There are three main types of transformer blocks: encoder, decoder with cross-attention, and decoder without cross-attention. These blocks can be repeated any number of times.</p> <h3 id="encoder-block">Encoder Block</h3> <p>The encoder block maps a set of embeddings to another set of embeddings. It uses full self-attention, so each token can attend to all other tokens.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/encoder_block-480.webp 480w,/assets/img/blog/transformer_pt1/encoder_block-800.webp 800w,/assets/img/blog/transformer_pt1/encoder_block-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/encoder_block.png" class="image-fluid mx-auto d-block" width="200" height="auto" alt="Encoder Block" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Encoder Block </figcaption> </figure> <h2 id="decoder-block">Decoder Block</h2> <p>The decoder block takes in a set of input embeddings but also attends to a set of embeddings from the encoder. The first attention layer processes input embeddings with causal attention. The second attention layer is cross-attention, where the keys and values come from the encoder output. This kind of block is only used in encoder-decoder architectures since it relies on the encoder output embeddings.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/decoder_cross_attention_block-480.webp 480w,/assets/img/blog/transformer_pt1/decoder_cross_attention_block-800.webp 800w,/assets/img/blog/transformer_pt1/decoder_cross_attention_block-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/decoder_cross_attention_block.png" class="image-fluid mx-auto d-block" width="300" height="auto" alt="Decoder block with cross-attention" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Decoder block with cross-attention </figcaption> </figure> <p>The cross-attention block is omitted in decoder-only transformers. This is because there are no encoder tokens to attend to. This block is identical to the encoder block, but the attention is masked.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/decoder_block-480.webp 480w,/assets/img/blog/transformer_pt1/decoder_block-800.webp 800w,/assets/img/blog/transformer_pt1/decoder_block-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/decoder_block.png" class="image-fluid mx-auto d-block" width="200" height="auto" alt="Decoder block without cross-attention" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Decoder block without cross-attention </figcaption> </figure> <h2 id="encoder-only-decoder-only-and-encoder-decoder-architectures">Encoder-Only, Decoder-Only, and Encoder-Decoder Architectures</h2> <p>The original transformer paper introduced an encoder-decoder architecture. Since then, encoder-only and decoder-only architectures have gained significant popularity for various use cases. You can even have “encoder-heavy” or “decoder-heavy” architectures where one part of the transformer has more layers than the other. Let’s explore the different types of transformer models and their applications.</p> <h3 id="encoder-decoder">Encoder-Decoder</h3> <p>The encoder-decoder architecture can be viewed as two interconnected transformers. An encoder, which is a stack of encoder blocks, maps a sequence of input embeddings to output embeddings. A stack of decoder blocks with cross-attention then processes these embeddings. The decoder blocks read the final output embeddings from the encoder in the cross-attention layer.</p> <p>The encoder and decoder can process different types of data. For instance, in speech recognition, the encoder might encode audio while the decoder translates it into text.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/whisper-480.webp 480w,/assets/img/blog/transformer_pt1/whisper-800.webp 800w,/assets/img/blog/transformer_pt1/whisper-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/whisper.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="OpenAI Whisper Encoder-Decoder Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> OpenAI Whisper Encoder-Decoder Architecture <a href="https://cdn.openai.com/papers/whisper.pdf"> Source </a> </figcaption> </figure> <p>This architecture employs cross-attention, whereas encoder-only and decoder-only architectures rely solely on self-attention.</p> <h3 id="encoder-only">Encoder-Only</h3> <p>Encoder-only transformers, popularized by <a href="https://arxiv.org/abs/1810.04805">BERT</a>, perform a one-to-one mapping of input embeddings to output embeddings. They can’t perform sequence-to-sequence modeling unless the input and output sequences have identical lengths.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/transformer_pt1/vit-480.webp 480w,/assets/img/blog/transformer_pt1/vit-800.webp 800w,/assets/img/blog/transformer_pt1/vit-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/transformer_pt1/vit.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="ViT Encoder-Only Architecture " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> ViT Encoder-Only Architecture <a href="https://arxiv.org/abs/2010.11929"> Source </a> </figcaption> </figure> <p>These models excel at scalar prediction tasks, such as classification or regression, where the output is a single value rather than a set or sequence. Text sentiment analysis and ImageNet classification are prime examples of their application.</p> <p>Encoder-only models are useful in tasks reducible to token classification. For instance, <a href="https://arxiv.org/abs/1810.04805">BERT</a>’s evaluation on the Stanford Question Answering Dataset (SQuAD) doesn’t generate text answers but identifies the relevant span in the input text. The task becomes classifying which tokens mark the start and end of the answer span. Similarly, Vision Transformer (ViT), another encoder-only architecture, is trained for ImageNet classification.</p> <h3 id="decoder-only">Decoder-Only</h3> <p>Decoder-only transformers have become the go-to architecture for Large Language Models (LLMs), popularized by OpenAI’s <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT</a> models.</p> <p>A decoder-only model can tackle any task an encoder-decoder can handle. Instead of processing source data through a separate encoder, all data flows through the decoder. The decoder omits the cross-attention layer since there’s no encoder to attend to. Its architecture mirrors that of the encoder-only transformer, with the key difference being causal attention.</p> <p>The encoder-decoder architecture can be viewed as a constrained version of the decoder-only architecture. The separate encoding of source data in encoder-decoder models represents a form of inductive bias that decoder-only architectures generalize away from.</p> <p>Encoder-decoder models require paired source and target text sequences for training, which can limit their flexibility. In contrast, decoder-only models can process a single input sequence, making them more versatile and adaptable to various tasks.</p> <p>Decoder-only architectures have surpassed encoder-decoder models in popularity due to their simplicity and versatility. However, encoder-decoder models still offer unique advantages, such as the ability to train on encoder-specific objectives or fine-tune the encoder for downstream tasks.</p> <h1 id="output-processing">Output Processing</h1> <p>The architecture of the output processing is simple. There is a final linear layer that maps embeddings of \(d_{model}\) to the size of the prediction. The output of the linear layer and how it is applied depends on the task the model is trained on.</p> <p>Transformer models can be trained with different objectives and losses based on the use case and architecture type. We will discuss different types of objectives and how they are used for training. We will also explain how inference works under these objectives.</p> <h2 id="next-token-prediction">Next Token Prediction</h2> <p>Encoder-Decoder and Decoder-only transformers are primarily trained on next token prediction. This task involves predicting the subsequent word in a sequence based on the preceding words. The output layer is applied to each embedding, with the output size matching the input vocabulary size. A softmax function then creates a probability distribution over the token vocabulary, from which the next token is sampled.</p> <p>During training, the decoder learns to predict the next word given the context. Causal masking ensures that for each token, the model can’t see the next or subsequent tokens in the sequence.</p> <p>For each token, the ground truth preceding tokens are used as context. This is known as teacher forcing, as the generated tokens aren’t used as context. However, at inference time, autoregressive decoding is used instead. We start with a context which is the set of input tokens. For chatbots, this might be the user’s input; for translation, the source text. The model predicts the next token, which is then added to the context, and another token is sampled. This process iterates until a special <END> token is produced or a predetermined limit is reached.</END></p> <p>At each step, the model outputs a probability distribution for the next token. This allows for sampling random sequences with a tunable temperature parameter, a common input for LLM APIs. Top-k sampling is another technique where you only consider the top-k tokens after the softmax. The probabilities are renormalized before sampling again. This prevents low probability tokens from ever getting predicted. Top-p or nucleus sampling is similar but possible tokens are selected so that their probabilities do not exceed p. This is more robust to changing entropies / confidence in the model’s predictions. These parameters are tuned to strike a balance between creativity and quality.</p> <p>The paper <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> and many other NLP applications use beam search for text generation. <a href="https://en.wikipedia.org/wiki/Beam_search">Beam search</a> is a search algorithm widely used in text decoding. <a href="https://thinking-teams.com/wp-content/uploads/2020/11/nmt.pdf">Wu et al. 2016</a> provides insights into how it is used for transformer decoding. With beam search, instead of greedily selecting the most probable next token, beam search maintains the top \(k\) most likely sequences, where \(k\) is the beam width. At each step, the model expands these \(k\) sequences with their most probable next tokens. To prevent the number of sequences from growing exponentially (\(k^n\)), only the top \(k\) paths are retained after each step. The path probability is calculated as the sum of negative log likelihoods, normalized by dividing by \(length^{\alpha}\). This method allows the model to explore multiple promising paths simultaneously, avoiding local optima that might occur from selecting single tokens. To manage computational costs and control output length, beam search requires constraints. The beam size determines the number of computed paths and token selections per step, and the length penalty \(\alpha\) exponentially penalizes longer sequences. <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> implements this with a beam size of 4 and an \(\alpha\) value of 0.6.</p> <p>Next token prediction training is highly parallelizable due to teacher forcing. A single forward pass generates predictions and losses for each token in the input. However, inference is an iterative process requiring a forward pass for each generated token.</p> <h2 id="masked-language-modeling">Masked Language Modeling</h2> <p>Masked Language Modeling (MLM) is a training objective introduced by <a href="https://arxiv.org/abs/1810.04805">BERT</a>. This method trains encoder-only transformers. MLM training follows these steps:</p> <ol> <li>Randomly select 15% of the input tokens for potential masking.</li> <li>Of these selected tokens: <ul> <li>• 80% are replaced with a special [MASK] token</li> <li>• 10% are replaced with a random token</li> <li>• 10% are left unchanged</li> </ul> </li> <li>Apply the linear output layer to all masked predictions. Use cross-entropy loss to predict the correct token, regardless of how it was masked.</li> </ol> <p>The intuition behind this technique differs fundamentally from next token prediction in that it’s bidirectional. As it’s an encoder-only architecture with full self-attention, tokens to the left and right are used to predict the masked tokens accurately.</p> <p>MLM is a pretraining method that doesn’t yield directly interpretable output. The model can be used for embedding representations, where the output embedding is aggregated and used in another application. The model can also be fine-tuned on a scalar prediction task.</p> <h2 id="scalar-predictions">Scalar Predictions</h2> <p>Encoder-only transformers support a wide variety of losses in addition to MLM. While MLM is a per-token objective where outputs are generated from multiple tokens, many objectives require using an output linear layer on a singular embedding to get a single output.</p> <p>There are multiple ways to achieve this:</p> <ul> <li>• Special output token <ul> <li>BERT and ViT add a <CLS> token to the input. The output embedding from this input is used for predictions.</CLS></li> </ul> </li> <li>• Output pooling <ul> <li>Alternatively, you can take the average of all the embeddings and apply the output layer on this pooled embedding.</li> </ul> </li> <li>• Attentive probing <ul> <li>Attention can process the output. A learnable query vector attends to all token embeddings, producing a weighted sum that is then used for the output layer. This is essentially a cross-attention with a fixed number of query embeddings.</li> </ul> </li> </ul> <p>Once you have a singular output embedding, it can be processed by the output linear layer, and then any loss function relevant to the objective.</p> <h1 id="conclusion">Conclusion</h1> <p>This blog post covered the basic components of early transformers. In part 2, we will cover more recent innovations that further optimize these models and enable new capabilities.</p>]]></content><author><name></name></author><category term="transformer"/><summary type="html"><![CDATA[The Transformer architecture has emerged as the cornerstone of deep learning and artificial intelligence. Despite its conceptual simplicity, the specific details of the architecture can be difficult to understand and reason about. This two-part blog series aims to provide a thorough examination of the Transformer, demystifying its core components and recent advancements. The goal is to cover the fundamental and cutting-edge concepts needed to design transformer-based models for any application in any modality.]]></summary></entry><entry><title type="html">Self-Supervision from Videos</title><link href="https://rohitbandaru.github.io/blog/Self-Supervision-from-Videos/" rel="alternate" type="text/html" title="Self-Supervision from Videos"/><published>2024-10-06T00:00:00+00:00</published><updated>2024-10-06T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Self-Supervision-from-Videos</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Self-Supervision-from-Videos/"><![CDATA[<p>In a previous <a href="https://rohitbandaru.github.io/blog/Self-Supervised-Learning/">blog post</a>, we explored image-based self-supervised learning primarily with contrastive learning. Self-supervised learning offers a way to train an ML model to generate useful image representations using large amounts of unlabeled data. The resulting models can be used for a wide variety of downstream tasks, such as classification, object detection, or segmentation on different datasets.</p> <p>Many of the current state-of-the-art results come from contrastive self-supervised learning. In contrastive SSL, an image from the dataset is transformed using multiple data augmentations, including cropping, color distortion, and flipping. These augmentations are inputted into a neural network to get representations. A contrastive loss is applied to push representations from the same image (different augmentations) closer together, and representations from different images are pushed further apart. Training with this objective will learn image representations that encode the content of the image. The data augmentations are needed for the network to avoid trivial solutions to optimize the contrastive loss, which would be comparing pixel values instead of the image’s semantic content.</p> <p>Contrastive learning is limited by its dependence on data augmentations. These augmentations are hacky and unnatural. They can change the meaning of the image. The act of augmenting an image destroys some amount of information.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/contrastive_ssl-480.webp 480w,/assets/img/blog/video-ssl/contrastive_ssl-800.webp 800w,/assets/img/blog/video-ssl/contrastive_ssl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/contrastive_ssl.png" width="500" height="auto" alt="Contrastive SSL" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/Purushwalkam-480.webp 480w,/assets/img/blog/video-ssl/Purushwalkam-800.webp 800w,/assets/img/blog/video-ssl/Purushwalkam-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/Purushwalkam.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Example showing that cropping can remove semantic information in images. <a href="https://arxiv.org/abs/2007.13916"> Source </a> </figcaption> </figure> <p>Example from <a href="https://arxiv.org/abs/2007.13916">Purushwalkam et al</a> showing that cropping can remove semantic information in images.</p> <p>Many AI researchers believe that humans learn through self-supervised learning. However, humans most likely don’t learn with data augmentations. It is unlikely we mentally do crops and color distortions. It is more likely we learn by tracking objects through time. For example, if you are watching a dog, the dog looks different from one point in time to another. The dog is likely in a different pose, different location, and has different lighting conditions. Through time, we can get <em>natural data augmentations</em>, but can they help train better image representation models?</p> <p>The blog post explores whether video can improve self supervised computer vision models. We look at some papers with different approaches to the problem.</p> <h1 id="image-vs-video-representations">Image vs Video Representations</h1> <p>Image and video representations are two distinct research problems. Image representation learning aims to learn fixed-size embeddings of images, while video representation learning aims to learn fixed-size embeddings of videos. There are many papers on applying contrastive learning techniques to videos; however, these involve applying data augmentations to videos. In this post, we will also discuss using videos to learn image representations so that we can utilize the concept of obtaining natural data augmentations from videos.</p> <h1 id="learning-video-representations">Learning Video Representations</h1> <p>Video representation learning can be understood through its downstream tasks. One of the most common datasets is <a href="https://www.deepmind.com/open-source/kinetics">Kinetics</a>. Which consists of short clips of human actions which are to be classified into classes such as “saluting” and “cutting cake”.</p> <h2 id="spatiotemporal-contrastive-video-representation-learning"><a href="https://arxiv.org/abs/2008.03800">Spatiotemporal Contrastive Video Representation Learning</a></h2> <p>This work is very similar to <a href="https://rohitbandaru.github.io/blog/Self-Supervised-Learning/#simclr">SimCLR</a>. It uses contrastive learning to learn representations of videos. Like SimCLR, InfoNCE loss is used to bring closer the representations of positive pairs and repel those of negative pairs. Positive pairs are data augmentations of the same video. There are two main differences with SimCLR:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/cvrl-480.webp 480w,/assets/img/blog/video-ssl/cvrl-800.webp 800w,/assets/img/blog/video-ssl/cvrl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/cvrl.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2008.03800"> Source </a> </figcaption> </figure> <h3 id="1-3d-cnn-model-architecture-for-the-video-encoder-to-produce-representations">1) 3D CNN model architecture for the video encoder to produce representations</h3> <p>3D-ResNets are used instead of 2D-ResNets.</p> <p>This is a natural extension of CNNs to handle video that treats time as a third dimension. This architecture obtains a fixed-size representation from a fixed number of frames of a certain resolution.</p> <h3 id="2-spatiotemporal-data-augmentations">2) Spatiotemporal data augmentations</h3> <p>In addition to the standard image data augmentations used in other contrastive SSL methods (color jittering, cropping, etc.), the authors add spatial and temporal augmentations. These are designed specifically for videos. It is important to carefully design the data augmentations used for videos. If the augmentations are too aggressive, the representations will be invariant to useful information. If the augmentations are too weak, the representations may learn trivial solutions.</p> <p><strong>Temporal Augmentations</strong></p> <p>The temporal interval is sampled first. This interval is the time difference between the two augmentations. Smaller intervals have more probability, which is desired because temporally distant augmentations might be too far apart. We want the content of the two augmentations to be the same. The further apart in the video they are, the less likely this is.</p> <p><strong>Spatial Augmentations</strong></p> <p>Applying spatial augmentations to each frame independently has the disadvantage of destroying motion between frames. If each frame is randomly cropped, the objects will randomly move around frame to frame. We want to preserve the consistency between frames. The solution to this is to sample a frame-level augmentation once per video clip and apply the same transformation to each frame. If each frame is cropped to exactly the same pixels, the motion will be perfectly preserved. For contrastive learning, we use data augmentations to make negative examples more different. We want two video clips to be different, but there is no need to make the frames of a single clip different from each other.</p> <p>This paper is a great example of how image SSL techniques can be extended to videos. It just requires different model architectures and data sampling / augmentation techniques. There are some gaps to this approach:</p> <ol> <li>It can only produce representations for fixed-size video clips. The resulting model can’t be used for downstream image tasks.</li> <li>Temporal data augmentations can still destroy some temporal information in the video, especially fine-grained details.</li> <li>Spatial data augmentations such as cropping are required and can be used to learn unwanted spatial invariances.</li> </ol> <h1 id="learning-image-representations-from-video">Learning Image Representations from Video</h1> <h2 id="self-supervised-learning-of-video-induced-visual-invariances-vivi"><a href="https://arxiv.org/abs/1912.02783">Self-Supervised Learning of Video-Induced Visual Invariances (VIVI)</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/vivi-480.webp 480w,/assets/img/blog/video-ssl/vivi-800.webp 800w,/assets/img/blog/video-ssl/vivi-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/vivi.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1912.02783"> Source </a> </figcaption> </figure> <p>They developed a video-based self-supervised learning framework for image representations and evaluated it on the <a href="https://ai.googleblog.com/2019/11/the-visual-task-adaptation-benchmark.html">VTAB</a> image representation benchmark.</p> <p>This uses the <a href="https://arxiv.org/abs/1609.08675">YouTube 8M</a> dataset. This is a larger dataset than Kinetics and contains longer and more complex videos. Videos are composed of frames. In this work, they also utilize “shots,” which are sequences of continuous frames within a video. Shots have a high-level relationship with each other. Frames within the same shot are used as positive pairs, and shots from different shots are used as negative pairs.</p> <p>Shot embeddings are defined as pooled (mean pooled or attention pooled) frame embeddings. These are used for shot order prediction. An LSTM or MLP is used to predict the next shot embedding given the current shot embedding. This models the relationship between shots in a video. An alternative to shot embedding prediction, would be to contrast shot embeddings between videos.</p> <p>This paper uses co-training with the ImageNet supervised classification task. There is a long way to go since YouTube-8M is much larger than ImageNet, so ImageNet should not be needed for good results. This gap may be due to the data itself. ImageNet is a relatively clean dataset where the object in each image is usually centered. Video frames from YouTube are much noisier.</p> <h2 id="demystifying-contrastive-self-supervised-learning-invariances-augmentations-and-dataset-biases"><a href="https://arxiv.org/abs/2007.13916"><strong>Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases</strong></a></h2> <p>In this paper, the authors observe that the cropping required for contrastive SSL hurts performance on downstream tasks such as object detection. They make a distinction between scene-centric and object-centric datasets. ImageNet is object-centric, which means that a single object is presented in the image and it is centered. The data in video often contains multiple objects in different parts of the frame. For certain tasks, ImageNet is a far superior training dataset due to this bias.</p> <p>They pretrain and evaluate an SSL model (MOCOv2) with the MSCOCO dataset (scene-centric) and MSCOCO bounding box cropped dataset (object-centric). The results show the effect of cropping when pretraining. They find that object-centric pretraining leads to better object-centric evaluation, while scene-centric pretraining leads to better scene-centric evaluation.</p> <p>This shows that for scene-centric evaluation tasks like object detection, cropping while pretraining is harmful.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/video-moco-480.webp 480w,/assets/img/blog/video-ssl/video-moco-800.webp 800w,/assets/img/blog/video-ssl/video-moco-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/video-moco.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2007.13916"> Source </a> </figcaption> </figure> <p>To replace cropping, videos can be used to get “temporal transformations”.</p> \[\mathcal{V}_{pairs} = \{(z_i, z_{i+k})\ |\ z \in \mathcal{V}, i \in \textnormal{N}(z), i \bmod k = 0\}\] <p>Pairs are formed by frames consecutive frames subsampled by $k$. We do not want frames that are too similar too each other in time, as their difference would be too insignificant. $k$ can be set depending on the frame rate of the video. If it is equal to 60, every 60th frame is considered and adjacent frames are pairs.</p> \[\mathcal{D}^+ = \{(t_i(z_i), t_j(z_{i+k}))\ |\ t_i,\ t_j \in T, (z_i, z_{i+\Delta}) \in V_{pairs}\}\] <p>The dataset is then formed by applying transformations to the pairs of frames. For this work, the same transformations as MOCO are used. MOCO doesn’t require negative examples to train. Positive example pairs are formed by applying a transformation on the pairs of frames. Although this work explores using video for natural transformations, it still relies on the traditional data augmentations used in SSL.</p> <p>This contrastive learning setup uses the whole frame, but the authors want to train on object-centric data to make the representations more robust for object recognition.</p> <p>They use region tracking to make the frames of the video object-centric. They track the same object across multiple frames and use these cropped versions of the frames. This way, the model learns how objects change in a video while ignoring the scene-wide changes. For example, the position of an object in a scene is ignored.</p> <h1 id="videomae"><a href="https://arxiv.org/abs/2203.12602">VideoMAE</a></h1> <p>This work extends the <a href="https://arxiv.org/abs/2111.06377">MAE</a> work (<a href="https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers/#masked-autoencoders-are-scalable-vision-learners">SSL with Vision Transformers blog post</a>) to video. Rather than using contrastive learning, masked patches are predicted in the video.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/video-ssl/videomae-480.webp 480w,/assets/img/blog/video-ssl/videomae-800.webp 800w,/assets/img/blog/video-ssl/videomae-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/video-ssl/videomae.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2203.12602"> Source </a> </figcaption> </figure> <p>The video is represented as a 3D image. The video is split into cubes (in this paper, it’s size 2 × 16 × 16). These cubes are treated like patches in ViT, but with an added time dimension. Each cube in the video is linearly projected and treated as a token embedding in the transformer. A position embedding is also added to the cube embedding. These cubes are treated as tokens in the transformer.</p> <p>For each video, a tube mask is applied. This means the same patch in multiple consecutive frames of the video is masked. This is meant to make the SSL task harder, as patches don’t change much frame to frame. There is a high temporal correlation that needs to be broken in designing the objective. A high masking ratio, along with the tube masking strategy, ensures the SSL task is difficult and forces the model to learn higher-level spatiotemporal information.</p> <p>An encoder processes the unmasked tokens. With a high masking ratio, we can save on computation cost by only processing the unmasked tokens. This is feasible with the transformer architecture, but not with CNNs. The encoder maps the token embeddings to a latent space. The decoder then predicts the masked cubes of the video. This can be trained with a simple reconstruction loss like MSE (mean squared error).</p> <p>One interesting architecture decision is that “joint space-time attention” is just full self-attention. This means the attention captures all pairwise interactions between all tokens. It would be interesting to introduce causal attention on the time dimension. This would mean that within a frame, there is full attention. But cubes can only attend to cubes in the future. However, this type of causal masking would likely require a lower cube masking ratio to be effective.</p> <p>Many other video SSL methods utilize image data in addition to video data. However, VideoMAE is able to achieve SOTA results on Kinetics-400 (video classification) without this external data. Ideally, we want video SSL methods that do not need to rely on images at all. This paper does not report results on image evaluation tasks. But this architecture would support this. We want video-pretrained models to achieve superior performance to image models on both video and image evaluation tasks. However, current video pretraining methods lag behind image-pretrained methods.</p> <p>This work leverages the flexibility of the transformer architecture to directly predict elements of the video. This is simpler in that it does not require tracking objects, constructing triplets, or applying data augmentations. It is closer to how language models are trained. This also allows producing image representations and video representations, unlike the CNN methods. This is because the number of input frames to the transformer is variable.</p> <h1 id="conclusion">Conclusion</h1> <p>Video lags behind images in representation learning for several reasons. While videos contain more information than static images, this information is spread across a much larger volume of data, making it computationally inefficient to process. The first frame of a video provides substantial information. You can learn what objects are present in the scene and the setting of the video. Subsequent frames offer diminishing returns. Future frames might just have one object moving across the frame. Because of this temporal redundancy, images are more information-dense than videos. Language models are more advanced than image models because language is a significantly more information-dense modality. Videos are an additional order of magnitude less information-dense than images. Data quality also plays a role. Video datasets, though vast, often lack the curated quality of image datasets like ImageNet, impacting the performance of video-based models. These challenges make it harder to build effective models with only video.</p> <p>Looking ahead, progress in video-based AI will be driven by improved datasets, advancements in computational power, and novel modeling techniques. As these developments unfold, we can expect more sophisticated vision models that effectively incorporate video data, bridging the current gap between image and video understanding in AI systems. This may unlock new capabilities in computer vision models.</p> <h3 id="datasets">Datasets</h3> <p><a href="https://www.image-net.org/">ImageNet</a>: 1,000,000 images, 1000 classes</p> <p><a href="https://arxiv.org/abs/1907.06987">Kinetics 700</a>: 650,000 videos, 700 classes, ~10 seconds each, 1800 hours</p> <p><a href="https://arxiv.org/abs/1609.08675">YouTube 8M</a>: ∼8 million videos, 500K hours of video—annotated with a vocabulary of 4800 entities</p>]]></content><author><name></name></author><category term="computer-vision"/><category term="self-supervised-learning"/><summary type="html"><![CDATA[In a previous blog post, we explored image-based self-supervised learning primarily with contrastive learning. Self-supervised learning offers a way to train an ML model to generate useful image representations using large amounts of unlabeled data. The resulting models can be used for a wide variety of downstream tasks, such as classification, object detection, or segmentation on different datasets.]]></summary></entry><entry><title type="html">SSL with Vision Transformers</title><link href="https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers/" rel="alternate" type="text/html" title="SSL with Vision Transformers"/><published>2024-08-01T00:00:00+00:00</published><updated>2024-08-01T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers/"><![CDATA[<p>In recent years, self-supervised learning (SSL) has emerged as a powerful paradigm in computer vision, allowing models to learn meaningful representations from unlabeled data. Prior work in this field focuses on using CNN architectures such as ResNet for this task. However, as evidenced by the success of self-supervised language models, transformers are a natural fit for self-supervised training. We will cover a set of recent papers that apply transformers for self-supervised visual learning.</p> <p>One key variation is that you often see masking in these methods. CNN-based SSL methods rely more on data augmentations to create a prediction task for the model. Masking is advantageous for several reasons outlined below, and it also aligns more with language model training (example: BERT).</p> <ul> <li>Computational efficiency <ul> <li>You do not have to process the masked regions of the image when a large portion of the image is masked.</li> </ul> </li> <li>Data augmentations can introduce unwanted invariances and remove useful information <ul> <li>For example, a data augmentation that strongly distorts the color may result in representations that do not encode color.</li> </ul> </li> </ul> <p>Masking is more naturally enabled by the transformer architecture. There is a reason that masking-based SSL training hasn’t worked well with CNNs.</p> <p>By examining these different methods, we’ll discuss what makes transformers work for vision.</p> <h1 id="dino"><a href="https://arxiv.org/abs/2104.14294"><strong>DINO</strong></a></h1> <p>This paper (Emerging Properties in Self-Supervised Vision Transformers) by Caron et al. introduces a new self-supervised training method called DINO, which they apply to vision transformers. They argue that transformers are better than CNNs for images with SSL training, more so than with supervised training. Transformers can match the performance of CNNs with supervised training, albeit with more training cost. However, they have more useful properties with SSL training. This follows our intuition that SSL and transformers are a natural combination.</p> <p>DINO takes inspiration from <a href="https://arxiv.org/abs/2006.07733">BYOL</a> but introduces two key innovations:</p> <ol> <li>A novel loss function that enables direct matching between student and teacher outputs</li> <li>Elimination of the prediction layer on the student, simplifying the architecture</li> </ol> <p>These changes result in a self-distillation approach that proves particularly effective with vision transformers.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/dino-480.webp 480w,/assets/img/blog/ssl-vit/dino-800.webp 800w,/assets/img/blog/ssl-vit/dino-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/dino.png" class="img-fluid mx-auto d-block" width="400" height="auto" alt="DINO architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2006.07733"> Source </a> </figcaption> </figure> <ol> <li>Two views of an image \(x\), \(x_1\) and \(x_2\) are generated through data augmentations. <ol> <li>A multi crop strategy is used in which two large global views are generated along with a set of smaller cropped local views. The teacher only processes global views, while the student processes all views, with the constraint that the loss is not trying to match the same views to each other. This method was introduced in the <a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper_files/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf&amp;hl=en&amp;sa=T&amp;oi=gsr-r-gga&amp;ct=res&amp;cd=0&amp;d=13209348926291080860&amp;ei=QYYkZu2RB5SCy9YP29Cc0AY&amp;scisig=AFWwaea44-zuGhikZl27njOvnygp">SwAV</a> paper, and helps the model learn local to global correspondences. Restricting the teacher to only global views also encourages the encoders to output global representations.</li> <li>Are position embeddings used?</li> </ol> </li> <li>The views are passed to their respective encoder (teacher/student)</li> <li>The teacher encoding is “centered”. <ol> <li>Perhaps centering allows this method to work without having the predictor layer. The center is a exponential moving average of the teacher encoding (of both views). This vector is subtracted from the teacher’s encoding before the softmax. A temperature is also applied with the softmax to achieve a “sharpening”. These methods help the teacher avoid collapse. Centering ensures that a single component of the vector doesn’t dominate. Sharpening ensures that it doesn’t collapse to a uniform vector.</li> </ol> </li> <li>Softmax is applied to each encoding. The student is trained with a cross entropy loss to match the teacher. The teachers weights are updated as an exponential moving average of the student.</li> </ol> <p>This paper compares the performance of DINO with ResNet and ViT architectures against <a href="https://rohitbandaru.github.io/blog//SSL-with-Vision-Transformers/">SOTA SSL methods</a> such as <a href="https://arxiv.org/abs/2006.07733">BYOL</a>, MoCov2, and SwAV. The combination os DINO and ViT has the most significant advantage. Interestingly, it is 6.6% better than ViT with BYOL training on linear ImageNet evaluation, despite minor differences in the methods. The SSL methods that are used for comparison were developed for CNN architectures, which put them at a disadvantage. DINO is designed for transformers, but what about it makes it work better with transformers? One possible explanation is that transformers handle different resolutions of images better. Higher resolution images results in more image patches generated in the transformer. The computation also scales quadratically in the attention operations with respect to the number of patches. For ResNet, the computation increases linearly.</p> <p>The two main “emerging properties” they observe is that DINO ViT features are useful for dense predictions such as semantic segmentation. Another property is that k nearest neighbors on the output encodings, without any finetuning. This enables image retrieval applications.</p> <p>They observe the teacher outperforms the student in DINO training. This is not observed with other SSL methods. They cite “Polyak-Ruppert averaging” as an explantation of this. This means the teacher simulates an ensemble model with its momentum weights.</p> <p>The multi-crop strategy enforces that the inputs be rectangular. This makes this method compatible with CNNs in addition to ViTs. DINO shows that SSL is effective with vision transformers. However, it is designed in a way that makes the training method compatible with CNNs. This leads to some very interesting comparisons between the properties of SSL CNN and ViT models. The other works we will discuss take advantage of the flexibility of the transformer architecture, at the cost of CNN compatibility.</p> <p><a href="https://arxiv.org/abs/2304.07193">DINOv2: Learning Robust Visual Features without Supervision</a> scales DINO using a 1 billion parameter ViT model along with a larger proprietary dataset. They used an interesting data processing pipeline to combine curated and uncurated data, to get a large dataset of high quality and diverse images. This step is important because unprocessed uncurated data can be of low quality and dominated by certain modes of data and duplicated data.</p> <p>There are several architectural and training changes applied on top DINO v1 that allow it to scale effectively. Notably, in addition to DINO, they add an <a href="https://arxiv.org/abs/2111.07832">iBOT</a> loss. This method masks some of the input tokens of the student. In order to combine DINO and iBOT losses, they learn separate heads on the student and teacher for each loss. iBOT does BERT style pretraining of image transformers, which we will also cover in this post.</p> <h1 id="data2vec"><a href="https://arxiv.org/abs/2202.03555">data2vec</a></h1> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec-480.webp 480w,/assets/img/blog/ssl-vit/data2vec-800.webp 800w,/assets/img/blog/ssl-vit/data2vec-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/data2vec.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="data2vec architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2202.03555"> Source </a> </figcaption> </figure> <p>The teacher model predicts representations from unmasked input, while the student model predicts representations from masked input. The student aims to match the teacher’s output by predicting the representations of the masked tokens. To avoid collapse, the teacher’s weights are an exponential moving average of the student’s weights.</p> <p>Instead of training a multimodal model, independent models are trained on different modalities. Data2VecAudio, Data2VecText, and Data2VecVision are developed. The learning objective remains the same, but the generation of embeddings and masking strategies differ.</p> <ol> <li>Encoding of inputs into embeddings: <ol> <li>Text is tokenized, and learned embeddings for each token are retrieved.</li> <li>Images are divided into 16x16 patches and linearly projected into an embedding.</li> <li>Audio is encoded by a 1D convolutional neural network with multiple layers. A 16 kHz waveform is mapped to a 50 Hz representation. This means a sequence of 320 integers is mapped to a single representation. <ol> <li>Unlike images, a multiple-layer network is used for audio, likely due to the absence of a Fourier transform.</li> </ol> </li> </ol> </li> <li>Masking: <ol> <li>Some of the student input embeddings are replaced by the MASK token embedding. <ol> <li>Text: Random tokens are masked.</li> <li>Images: Embeddings corresponding to rectangular blocks are masked.</li> <li>Audio: Continuous spans of embeddings are masked.</li> </ol> </li> </ol> </li> <li>Addition of position encoding.</li> <li>Both the teacher and student transformer models receive the input.</li> <li>Representations at different layers are distilled from the teacher to the student. Outputs from the masked tokens of the top \(K\) transformer blocks are normalized and averaged into a single vector.</li> <li>A regression loss (Smooth L1) is applied to the averaged vectors of each network. <ol> <li>The loss transitions from a squared loss to an L2 loss when the error margin goes below the hyperparameter \(\beta\). The L2 loss is only applied when the student and teacher predictions are close. This loss is designed to be less sensitive to outliers.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec_loss-480.webp 480w,/assets/img/blog/ssl-vit/data2vec_loss-800.webp 800w,/assets/img/blog/ssl-vit/data2vec_loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/data2vec_loss" class="mx-auto d-block" width="500" height="auto" alt="data2vec loss" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2202.03555"> Source </a> </figcaption> </figure> <ol> <li>The students weights are updated with SGD. The teacher’s weights are updated as a EMA of the students weights: \(\Delta \leftarrow \tau \Delta + (1-\tau)\theta\) <ol> <li>\(\Delta\) represents the teacher’s parameters, while \(\theta\) represents the student’s parameters.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec_architecture-480.webp 480w,/assets/img/blog/ssl-vit/data2vec_architecture-800.webp 800w,/assets/img/blog/ssl-vit/data2vec_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/data2vec_architecture.png" class="mx-auto d-block" width="500" height="auto" alt="data2vec architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The position encoding and feature encoder weights are shared between the two models. However, the teacher’s transformer weights are specified through an exponential moving average.</p> <p><a href="https://arxiv.org/abs/2212.07525"><strong>data2vec 2.0</strong></a></p> <p>Data2Vec 2.0 introduces several architectural and loss function changes that lead to a significant speed up in training.</p> <p>They use target representations for multiple masked predictions of a sample. This is more computationally efficient because we only need to run the teacher model once to train with \(M\) different masks of the input instead of 1. Further efficiency gains are implemented through not processing the masked parts of the image with the student, and sharing the feature encoder output across all masks.</p> <p>They use a L2 loss instead of a smooth L1 loss. This is a simplification of the earlier loss. They also use a convolutional decoder to predict the masked representations rather than a transformer.</p> <p>They also introduce inverse block masking. Rather than masking blocks. Blocks are chosen to be unmasked areas. The representations outside of the block will be predicted. There are multiple blocks which may overlap. A mask consists of multiple blocks. Training includes multiple masks for each target.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec_2-480.webp 480w,/assets/img/blog/ssl-vit/data2vec_2-800.webp 800w,/assets/img/blog/ssl-vit/data2vec_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/data2vec_2.png" class="mx-auto d-block" width="100%" height="auto" alt="data2vec 2.0" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2212.07525"> Source </a> </figcaption> </figure> <p>They also add a linear attention bias (<a href="https://arxiv.org/abs/2108.12409">ALiBi</a>). This essentially modifies self attention to increase the bias for query key pairs that are far apart. This enables faster training by providing an inductive bias.</p> <h1 id="masked-autoencoders-are-scalable-vision-learners"><a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a></h1> <p>This paper uses a simple autoencoder architecture to learn image representations. Parts of the images are masked, and the model is tasked to predict what is in the masked regions. This model can be trained through this <a href="https://github.com/ariG23498/mae-scalable-vision-learners/blob/master/mae-pretraining.ipynb">notebook</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/mae-480.webp 480w,/assets/img/blog/ssl-vit/mae-800.webp 800w,/assets/img/blog/ssl-vit/mae-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/mae.png" class="mx-auto d-block" width="100%" height="auto" alt="Masked Autoencoder" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2111.06377"> Source </a> </figcaption> </figure> <ol> <li>The image is split into patches, as done in Vision Transformers.</li> <li>Using a mask ratio (75%–95%), patches are selected randomly without replacement.</li> <li>The unmasked patches are input into the encoder. Note that the mask tokens do not get processed by the encoder (difference from BERT). The encoder uses a vanilla ViT architecture, where the unmasked patches are linearly projected into token embeddings which get processed by transformer blocks. The output is a ViT-processed embedding for each unmasked patch. Each patch has an added position embedding.</li> <li>The encoded tokens and the masked tokens are combined as an input to the decoder. The mask tokens map to a learned embedding. This embedding will be the same at all positions because it is not transformed by the encoder. At this stage, position embeddings are added to the full set. <ol> <li>Note that for unmasked tokens, position embeddings are added twice, once before the encoder and once before the decoder.</li> </ol> </li> <li>The decoder reconstructs the unmasked image from the set of patch embeddings. The decoder is trained by a mean squared error loss with respect to the unmasked input image.</li> </ol> <p>This architecture builds on the vision transformer. An alternative is to use CNNs. This would involve directly setting pixels in the input image to zero, learning a vector representation, and then decoding it back to the image. The reason this fails is that it aims to globally decode an image. With transformers, you first predict representations of the masked patches, and then decode into the image patch. This breaks it down into two easier problems. Also, with CNNs, you can’t explicitly encode masked regions like you can with a ViT. Having a mask token more explicitly indicates the mask.</p> <p>They mask a very high percentage of patches (80%). This reduces spatial redundancy and forces the model to learn more higher-level and useful features. With a lower mask ratio, the model might learn to represent small local changes, like color and lighting variation. It doesn’t need to understand the higher-level structure of the image, because it’s mostly already there. This is a notable change from language models. BERT masks 15% of tokens. MAE and related works mask a majority of the image (75%+).</p> <p>The model uses the ImageNet-1K dataset for pretraining and evaluation. Evaluation is done by either finetuning the full encoder model or using a linear probe (training one MLP layer on the output of the encoder) on the task of classification.</p> <p>One interesting result is that the performance of finetuning and linear probing has different trends when ablating the masking ratio. Linear probing accuracy increases linearly with masking ratio until 75%. Finetuning has relatively consistent performance between 40% and 80%.</p> <p>Having a deep decoder allows for the representations to be more abstract, because the decoder has more capacity for reconstruction. A shallower decoder would lead to the encoder having to represent more of the details needed for reconstruction. This is less relevant for finetuning than it is for linear probing, as during finetuning, the encoder can shift from focusing on reconstruction to recognition. In my opinion, linear probing results are more interesting since the goal is to build useful representations that can be used for various tasks. Finetuning offers just a marginal improvement over just training on the classification task directly without pretraining at all. However, linear probing discourages learning nonlinear features in the representation. To address this, the authors evaluate “partial finetuning” in which the last few blocks of the transformer are finetuned.</p> <p>Excluding mask tokens from the input and using a lightweight decoder makes this model very efficient to train. Using mask tokens in the encoder also creates a domain shift between pretraining and downstream tasks, which hurts performance. This is because a large portion of the pretraining input will be mask tokens, which is significantly different from what the model will see downstream.</p> <h1 id="beit-bert-pre-training-of-image-transformers"><a href="https://arxiv.org/abs/2106.08254"><strong>BEiT: BERT Pre-Training of Image Transformers</strong></a></h1> <p>This approach is most similar to BERT / NLP SSL models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/beit-480.webp 480w,/assets/img/blog/ssl-vit/beit-800.webp 800w,/assets/img/blog/ssl-vit/beit-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/ssl-vit/beit.png" class="mx-auto d-block" width="100%" height="auto" alt="beit" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2106.08254"> Source </a> </figcaption> </figure> <p>A fundamental difference in applying SSL to images compared to text is that images are continuous. Text has a finite number of tokens. You can use a softmax to get a probability distribution across all tokens. In ViTs, patches of an image are treated as tokens. However, you can’t get an explicit probability distribution over all possible image patches. BEiT addresses this problem by training a discrete variational autoencoder (dVAE) to learn discrete visual tokens. These discrete tokens are an approximation or compression of image patches.</p> <p>The main difference between this and a vanilla ViT architecture is the usage of discrete visual tokens.</p> <p>There are two steps to training:</p> <ol> <li>Tokenizer and Decoder are trained as a VAE to learn discrete visual tokens</li> <li>The discrete tokens from the learned tokenizer are used to pretrain a BEiT encoder.</li> </ol> <p>Why aren’t the tokens used as the input directly? The softmax distribution of tokens could be used as a soft label for the BEiT encoder.</p> <p>The transformer training task is named masked image modeling (MIM), as it is designed after BERT’s masked language modeling (MLM). 40% of the tokens are masked. Similar to other methods, BEiT masks a large portion of the image to make the pretraining task sufficiently difficult.</p> <h1 id="conclusion">Conclusion</h1> <p>The landscape of self-supervised learning for image processing is undergoing a significant transformation. While it originated with Convolutional Neural Networks (CNNs), a strong coupling with transformer-based architectures is emerging and may lead the way for further advancements.</p>]]></content><author><name></name></author><category term="self-supervised-learning"/><category term="transformer"/><category term="computer-vision"/><summary type="html"><![CDATA[In recent years, self-supervised learning (SSL) has emerged as a powerful paradigm in computer vision, allowing models to learn meaningful representations from unlabeled data. Prior work in this field focuses on using CNN architectures such as ResNet for this task. However, as evidenced by the success of self-supervised language models, transformers are a natural fit for self-supervised training. We will cover a set of recent papers that apply transformers for self-supervised visual learning.]]></summary></entry><entry><title type="html">Deep Dive into Yann LeCun’s JEPA</title><link href="https://rohitbandaru.github.io/blog/JEPA-Deep-Dive/" rel="alternate" type="text/html" title="Deep Dive into Yann LeCun’s JEPA"/><published>2024-07-31T00:00:00+00:00</published><updated>2024-07-31T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/JEPA-Deep-Dive</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/JEPA-Deep-Dive/"><![CDATA[<p>In the AI research community, Yann LeCun has a unique and often controversial perspective. As of 2024, LLMs and Generative AI are the main focus areas of the field of AI. We’ve all been impressed by the performance of LLMs in various contexts, and generative systems like OpenAI’s <a href="https://openai.com/sora">Sora</a>. However, it is not clear where these advances fit in the long term goal of achieving and surpassing human level intelligence, which many call AGI.</p> <p>In his position paper <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">A Path Towards Autonomous Machine Intelligence</a> and his many recent talks (linked below), Yann presents an alternative framework for achieving artificial intelligence. He also proposes a new architecture for a predictive world model: Joint Embedding Predictive Architecture (JEPA).</p> <p>This blog post will dive deep into Yann’s vision for AI, the JEPA architecture, current research, and energy-based models. We will go deep into the technical aspects of these ideas, as well as give my opinions, along with interesting references. I will also cover recent research advances such as <em>V-JEPA</em></p> <p>This is a long post, feel free to jump to the sections about JEPA, I-JEPA, and V-JEPA.</p> <h3 id="relevant-talks-by-yann-lecun">Relevant Talks by Yann LeCun</h3> <p><a href="https://drive.google.com/file/d/1RVYBVi_bWyz-4sZSsu4rSWzDwQBLsvHL/view"><em>From Machine Learning to Autonomous Intelligence</em></a></p> <div class="video"> <figure> <iframe width="560" height="315" src="https://www.youtube.com/embed/VRzvpV9DZ8Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </figure> </div> <p><a href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf"><em>Objective-Driven AI: Towards Machines that can Learn, Reason, and Plan”</em></a></p> <div class="video"> <figure> <iframe width="560" height="315" src="https://www.youtube.com/embed/d_bdU3LsLzE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </figure> </div> <h1 id="problems-with-current-ai">Problems with Current AI</h1> <p>The JEPA architecture aims to address current AI challenges. To contextualize these issues, we’ll examine Yann LeCun’s criticisms of popular AI trends as of 2024.</p> <p>Recent years have seen tremendous excitement around Large Language Models (LLMs) and Generative AI. LLMs are pretrained using autoregressive self-supervised learning, predicting the next token given preceding ones. They’re trained on vast datasets of text and code from the internet and books, often fine-tuned with supervised learning or reinforcement learning. Generative AI broadly refers to creation of multimodal media from inputs, such as text-to-image generation.</p> <p>However, these models face significant limitations:</p> <ol> <li>Factuality / Hallucinations: When uncertain, models often generate plausible-sounding but false information. They’re optimized for probabilistic likelihood, not factual accuracy.</li> <li>Limited Reasoning: While techniques like <a href="https://arxiv.org/abs/2201.11903">Chain of Thought</a> prompting improve LLM’s ability to reason, they’re restricted to solving the selected type of problem and approaches to solving them without improving generalized reasoning abilities.</li> <li>Lack of Planning: LLMs predict one step at a time, lacking effective long-term planning crucial for tasks requiring sustained goal-oriented behavior.</li> </ol> <p>Despite impressive advancements, the challenge of autonomous driving illustrates the gap between current AI and human-level intelligence. As LeCun notes, humans can learn driving basics in about 20 hours. In contrast, self-driving car development has consumed billions of dollars, extensive data collection, and decades of effort, yet still hasn’t achieved human-level performance.</p> <p>Even achieving Level 5 autonomy wouldn’t signify true human-level AI or Artificial General Intelligence (AGI). Such intelligence would involve learning to drive from scratch within a day, using only data collected during that experience, without relying on massive pre-existing datasets for finetuning. Realizing this level of adaptable intelligence might require several more decades of research.</p> <h2 id="common-sense">Common Sense</h2> <p>The limitations in AI models can often be attributed to a lack of common sense. Common sense can be defined as thinking and acting in a reasonable manner. Humans and many animals have this ability. This includes avoiding egregiously dangerous or incorrect actions. Expanding on the autonomous driving example, AV systems need to be trained to deal with new situations safely. When learning to drive, humans utilize their common sense to know to not do dangerous things like driving off the road or into other cars. This is not obvious to current AV systems, so they require a large amount of training data to avoid these actions.</p> <p>LLMs similarly demonstrate a lack of common sense through nonsensical or illogical outputs. Common sense is a vague term. One definition is that it is a lower bound on the types of errors an agent makes. For AI to be trustworthy, it needs this foundational level of understanding.</p> <p>Common sense can also be viewed as a collection of world models. These models enable quick learning of new skills, avoidance of dangerous mistakes in novel situations, and prediction of outcomes in unfamiliar scenarios. Essentially, we use world models to generalize our experiences.</p> <h3 id="how-humans-learn">How Humans Learn</h3> <p>Humans acquire a basic understanding of the world during early infancy, but we’re also born with some innate knowledge. The brain isn’t randomly initialized; it’s evolved, pre-trained, and fine-tuned throughout life. This differs significantly from artificial neural networks, which start with random initializations and have far weaker inductive biases than humans or animals. Life is generally pre-programmed to behave in a certain way from birth. More intelligent life is able to learn more and not purely rely on innate knowledge.</p> <p>Understanding the extent to which babies acquire common sense during infancy is crucial for AI development. If common sense is largely innate, the focus should be on massive datasets mimicking evolutionary timescales. If it’s primarily learned, priority should be given to models that excel at quick learning from limited data.</p> <p>A baby’s experience, while not comparable to evolutionary timescales, still represents a substantial dataset. If a baby is awake for <a href="https://intuitiveparentingdc.com/blog/2018/7/6/developmentally-appropriate-sleep-expectations-birth-to-age-5">8 hours</a> a day, in four months they have seen about 960 hours of data. This data is also augmented by other sensory signals and dense biological supervision (pain, hunger, emotions). This is around the same length as the <a href="https://arxiv.org/abs/1705.06950">Kinetics 400</a> video dataset. This is still dwarfed by the millions of hours of video that self driving cars are using.</p> <p>This Nature <a href="https://www.nature.com/articles/s42256-024-00802-0">paper</a> by Orhan and Lake explores learning from infant-perspective data. They demonstrate that computer vision models can be trained on noisy, less diverse datasets collected from infant headcams. These egocentric datasets are far noisier and less diverse than standard image/video datasets, but AI models without strong inductive biases can learn from them.</p> <p>Emmanuel Dupoux’s diagram, presented by Yann LeCun, suggests that babies often understand concepts like object permanence, solidity, and biological motion by around four months. While presented as quick learning, it’s important to note the significant amount of data processing that occurs during this time.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/dupoux-480.webp 480w,/assets/img/blog/jepa/dupoux-800.webp 800w,/assets/img/blog/jepa/dupoux-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/dupoux.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Dupoux diagram on cognitive development" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>We don’t yet know precisely how much data AI systems would need to learn the same concepts as babies. It’s likely that the data efficiency gap is relatively small for basic concepts that babies learn. For instance, object permanence could probably be learned from 960 hours of video data. However, it becomes evident that this gap grows substantially with age and with the complexity of the knowledge being assessed. The challenges in developing fully autonomous vehicles clearly demonstrate how large this data efficiency gap can become.</p> <p>In addition to the lack of common sense, we mention three other fundamental gaps in the ability of current AI: hallucinations, lack of planning, and lack of reasoning.</p> <h2 id="learning-to-think">Learning to Think</h2> <p>The question of whether Large Language Models (LLMs) can truly reason and plan is a contentious topic in the AI community. While these models exhibit behaviors that resemble <a href="https://arxiv.org/abs/2201.11903">reasoning</a> and planning, skeptics argue that they merely replicate patterns from their training data.</p> <p>To frame this discussion, let’s consider reasoning and planning as forms of “thinking”, which we will define as a variable length internal process that precedes any outputs.. Current deep learning models employ two primary mechanisms for this kind of processing:</p> <ol> <li>Depth: Each layer in a neural network can be viewed as a step in the thinking process. However, this depth is typically fixed, with some recent <a href="https://arxiv.org/abs/2404.02258">work</a> exploring dynamic depth adjustment based on input complexity. Despite these advances, maximum depth and other constraints still limit the model’s flexibility.</li> <li>Sequential Generation: Decoder-based LLMs, such as GPT, generate text one token at a time. Each step in this process involves some degree of computation that could be interpreted as thinking. Prompt engineering techniques leverage this sequential nature to guide the model towards desired outputs. A key limitation of this approach is that the model must produce a token at each step, preventing purely internal information processing.</li> </ol> <p>While these properties enable models to create the illusion of thought, significant advancements are necessary to achieve more effective reasoning and planning capabilities.</p> <p>Many researchers draw parallels between AI and the two-system model of thinking <a href="https://www.google.com/books/edition/Thinking_Fast_and_Slow/ZuKTvERuPG8C?hl=en&amp;gbpv=1&amp;printsec=frontcover">proposed</a> by Daniel Kahneman. System 1 thinking is fast and intuitive, providing immediate responses without conscious deliberation. System 2, in contrast, is slower and more deliberate, engaging in deeper cognitive processing. Current machine learning models, including LLMs, primarily operate in a System 1 mode by processing information in a single pass without the ability to plan ahead. While they excel at pattern recognition, they lack true reasoning or planning capabilities.</p> <p>This inability to plan contributes to factual errors in LLM outputs. Each generated word carries a risk of inaccuracy, with the probability of errors increasing exponentially as the output length grows. The sequential nature of token generation means that early mistakes can compound, potentially invalidating the entire output. This stands in stark contrast to human speech, where we typically plan our utterances at a higher level before vocalization, minimizing such errors. In this context, reasoning can be viewed as the planning of speech. Without the capacity to reason or plan effectively, LLMs essentially “speak without thinking.”</p> <p>In the JEPA paper, Yann LeCun proposes frameworks for models that can think. Learning to think may address the fundamental problems in current AI models and represent a crucial step towards achieving more human-like intelligence in AI.</p> <h1 id="modality">Modality</h1> <p>Recent advancements have expanded LLMs to include multimodal processing and outputs, but they remain primarily language-centric. This raises questions about the sufficiency of language alone for AI and the investment needed in visual understanding. Could visual comprehension help ground AI in reality, improving common sense and reducing hallucinations?</p> <p>Language serves as a compressed representation of the complex concepts humans experience. Its expressive power is vast, capable of describing intricate scientific theories and nuanced emotions. Yet, language alone may not suffice for complete understanding.</p> <p>Humans interpret language within the context of shared reality. It functions as a highly efficient medium for transmitting information through the relatively narrow bandwidth of speech. When we process language, our brains rely on prior knowledge and experiences. While some of this prior information can be acquired through text, a significant portion stems from visual and physical interactions with the world.</p> <p>Currently, it does seem that language models are more capable than vision models. Language models currently outperform visual models due to information density, data requirements, and data availability.</p> <p>In a given data point there is a certain amount of explicit information in the form of bits. But then there is relevant information that is useful. For example, if you take an image of the park, a lot of bits are used to represent the position of every blade of grass. But that is not useful in most scenarios. Language is very compressed. While there are some filler words that don’t add much <a href="https://www.youtube.com/watch?v=VvPaEsuz-tY&amp;ab_channel=Argonaut57">information</a>, the ratio of knowledge to bits is high. However, for images, most of the bits are not useful. This means you need orders of magnitude more bits of data to learn equivalent knowledge. Video models are further behind because you need another order of magnitude more bits since consecutive frames in video are mostly redundant.</p> <p>While language-based AI leads, scenarios exist where visual learning could catch up. One scenario in which visual learning could overtake language is that we will have a large number of robots / autonomous vehicles interacting with the world while collecting visual data. Language will be data constrained with the rate of new text generation limiting scaling. In a world with a lot of robots, the knowledge gained from the visual world and the size of the available datasets may exceed that of text. However, this is all very speculative. We don’t know how important vision or grounding is for intelligence.</p> <h1 id="a-framework-for-building-human-level-ai">A Framework for Building Human-Level AI</h1> <p>Yann proposes a high level architecture for building an AI system that is aimed at addressing the problems we outlined. This is a design for an intelligent agent that can perceive the world,</p> <p>We will then explore the various challenges that must be addressed to construct such an architecture. Currently, this is merely a theoretical architecture. Building certain components remains an open problem, and assembling all the modules will pose an additional challenge.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/jepa_brain-480.webp 480w,/assets/img/blog/jepa/jepa_brain-800.webp 800w,/assets/img/blog/jepa/jepa_brain-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/jepa_brain.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="High Level View of LeCun's Architecture for Intelligence" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> High Level View of LeCun's Architecture for Intelligence <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>This architecture contains different proposed components. We will explain these components and their relationships.</p> <p><strong>Configurator</strong>: Configures input from all other modules and configures them for the task at hand. It tells the perception module what information to extract.</p> <p><strong>Perception:</strong> Estimates the current state of the world from different sensory signals.</p> <p><strong>World module</strong>: Estimates missing information about the state of the world and predicts future states. It simulates the world and extracts relevant information as determined by the configurator.</p> <p><strong>Cost module</strong>: Measures the level of discomfort as energy. This energy is the sum of the intrinsic cost module and the trainable critic module.</p> <p><strong>Intrinsic cost</strong>: Computes a cost given the current state of the world and predicted future states. This cost can be imagined as hunger, pain, or general discomfort. This cost can be hardwired in AI agents, as done with rewards in RL.</p> <p><strong>Trainable Critic</strong>: Predicts future intrinsic energy. It has the same input as the intrinsic cost. This estimate is dependent on the intrinsic cost and cannot be hardwired. It is trained from past states and subsequent intrinsic cost, retrieved from memory.</p> <p><strong>Short term memory</strong>: Stores relevant information about past present and future states of the world along with intrinsic cost.</p> <p><strong>Actor</strong>: Proposes sequences of actions. These sequences are executed by the effectors. The world model predicts future states from the sequence which then generates a cost.</p> <h1 id="actor">Actor</h1> <p>The actor proposes an optimal action or sequence of actions.</p> <p>If the world model and cost are well behaved, gradient based optimization can be used to determine an optimal action sequence. If actions are discrete then dynamic programming methods such as beam search can be used.</p> <p>There are two different modes in the actor. These align with Kahneman’s System 1 and 2, which we mentioned earlier.</p> <p><strong>Mode 1 Reactive Behavior</strong>: A policy module that computes an action from the state generated by perception and short-term memory. This module acts fast and produces simple decisions. A world model is needed to estimate the cost of an action. Without a world model the agent would have to perturb their actions which is not feasible. The world model can be adjusted after observing the next state.</p> <p><strong>Mode 2 Reasoning and Planning</strong>: A sequence of actions along with predicted corresponding states is generated. From this sequence of states, a cost can be computed. Planning is done by optimizing the action sequence to minimize total cost. The action sequence is then sent to the effectors which execute at least the beginning of the sequence. The states and costs are stored in short-term memory. The sequence can be optimized through gradients since the cost and world model are differentiable. Dynamic programming can also be used. Planning in this setup is essentially inference time cost optimization.</p> <p>Agents may have multiple policy modules executing mode 1. In this design, the agent only has one world model, so mode 2 can only be run once. However, AIs could be designed to have multiple world models and mode 2 processes at the same time. This is similar to having multiple thoughts at the same time. However, this would be very complicated in that the different modules would have to coordinate with the effectors and other modules to avoid conflicts. Also, this may be why humans don’t think like this.</p> <p>Policy modules can be learned to approximate actions from mode 2 reasoning. This is the process of learning a new skill. In humans, system 2 thinking can be done through system 1 after enough learning. For example, in chess, inexperienced players plan steps explicitly and simulate outcomes. Experienced players can instantly recognize patterns and make optimal moves.</p> <h1 id="cost">Cost</h1> <p>Cost is the sum of an immutable intrinsic cost and a trainable cost or critic.</p> \[C(s) = \mathrm{IC}(s) + \mathrm{TC}(s)\] <p>Each of these costs are the sum of different sub-costs generated by submodules. The weights of the sub-cost at each state \(u\) and \(v\) are determined by the configurator. This allows the agent to focus on different goals at different times.</p> \[\mathrm{IC}(s) = \sum_{i=1}^ku_i\mathrm{IC_i}(s)\\ \mathrm{TC}(s) = \sum_{i=1}^kv_i\mathrm{TC_i}(s)\] <p>The IC being immutable prevents the agent from drifting towards bad behaviors. It constrains the behavior of the agent.</p> <p>\(\mathrm{TC}\) or the critic is trained to predict future intrinsic cost values. The intrinsic cost only considers the current state. The critic can be trained to predict the future cost so the agent can minimize cost in the future. The short term memory stores triplets of (time, state, intrinsic energy): \((\tau, s_{\tau}, IC(s_{\tau}))\). The critic can be trained to predict the cost of a future state or a discounted sum of future intrinsic costs. For example, the loss function of the critic could be \(\|\|\mathrm{IC}(s_{\tau+\delta}) - \mathrm{TC}(s_{\tau})\|\|^2\). This formulation trains the critic to predict the intrinsic cost of a state \(\delta\) steps in the future. \(\mathrm{IC}(s_{\tau+\delta})\) can be replaced with other targets that can be extracted from the sequence of triplets. However, it cannot depend on the future trainable cost itself.</p> <h1 id="configurator">Configurator</h1> <p>The configurator controls the other components of the system. If these components are implemented as transformers, they can be easily configured by adding tokens. The configurator would inject tokens to steer these components in certain directions. For example, it may influence certain types of actions from the actor, or for perception to focus on certain properties.</p> <p>The configurator is also responsible for setting the weights of the cost terms. This will allow for the agent to focus on different subgoals at different times. The unanswered question is how the configurator can learn to decompose a complex task into subgoals.</p> <h1 id="world-model">World Model</h1> <p>In JEPA, the purpose of the world model is to predict future representations of the state of the world. There are three main issues</p> <ol> <li>Diversity of the state sequences the model is able to observe during training</li> <li>The world isn’t fully predictable, so the model has to predict multiple plausible state representations following an action</li> <li>Predictions must be made at different time scales and abstractions</li> </ol> <h2 id="self-supervised-learning--energy-based-models">Self-Supervised Learning / Energy-Based Models</h2> <p>In order to train a world model, Yann LeCun proposes an SSL energy-based model (EBM).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/ebm-480.webp 480w,/assets/img/blog/jepa/ebm-800.webp 800w,/assets/img/blog/jepa/ebm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/ebm.png" class="image-fluid mx-auto d-block" width="300" height="auto" alt="ebm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>\(x\) and \(y\) can be considered videos, where \(y\) follows x. EBMs learn an energy function \(F(x,y)\) that take low values when \(x\) and \(y\) are compatible and high if not. Compatible in this context means that \(y\) is a plausible continuation of \(x\).</p> <p>This is different from generative models in that \(y\) is not directly predicted from \(x\). There is a large space of values of \(y\) that can follow \(x\). Predicting exactly what will happen is an intractable problem. However, it is feasible to understand what is possible and what is not. Being good at this task requires an understanding of the world and common sense. A value of \(y\) that defies the laws of physics should result in a high energy value.</p> <p>However, planning requires predictions of future states. Although \(y\) can’t be predicted directly, we can predict future representations of \(y\). We can get representations from an encoder: \(s_x = g_x(x)\), \(s_y = g_y(y)\)</p> <p>The encoder will be trained such that the representations are maximally informative about each other, and that \(s_y\) can easily be predicted from \(s_x\). We can make predictions on this representation to enable planning.</p> <p>A latent variable can be introduced to handle uncertainty. A latent variable is just an arbitrary random variable. It is the source of randomness that is transformed to a useful distribution. Here we want to map the latent variable to the large space of possible values \(s_y\) can take.</p> <p>A latent-variable EBM (LVEBM) is represented as \(E_w(x, y, z)\).</p> <p>The energy function can be determined by find the \(z\) value that minimizes the energy. \(F_w(x,y) = \min_{z \in \mathcal{Z} }E_w(x,y,z)\)</p> <p>The EBM collapses when all pairs have the same low energy. This can happen when the latent variable has too much information capacity. This happens because \(z\) can vary along a larger space. This means that the space for which the energy of \(y\) is low is correspondingly large. If it is too large then the energies of \(y\) collapse. If the \(z\) dimension is the same as the representation dimension, the model can ignore \(y\) entirely and set \(s_y\) to equal \(z\).</p> <p>The paper describes a high data density region. This refers to \((x, y)\) pairs that are commonly seen in the real data distribution. We want to lower energy in this region, but keep it high outside of it. Collapse is when the energy is low inside and outside of this region which makes the EBM useless.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/regularized_ebm-480.webp 480w,/assets/img/blog/jepa/regularized_ebm-800.webp 800w,/assets/img/blog/jepa/regularized_ebm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/regularized_ebm.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="regularized ebm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>There are two training methods used to prevent collapse.</p> <p>Contrastive methods: Collapse is avoided by increasing the energy with respect to negative examples. It requires some method to generate examples to contrast against. The number of contrastive examples needed grows exponentially with respect to the dimension of the representation.</p> <p>Regularized methods: In these methods, the loss is regularized to minimize the space in \(y\) where the energies are lowered. These are less likely to be affected by the curse of dimensionality. Contrastive architectures can be regularized. For example, the latent dimension can be constrained.</p> <h2 id="joint-embedding-predictive-architecture">Joint Embedding Predictive Architecture</h2> <p>JEPA is an EBM that performs predictions in the representation space. The energy is the error in predicting \(s_y\) from \(s_x\).</p> <p>JEPA needs multi-modality, which in this context means to represent multiple possible values of \(y\). There are two ways it can be achieved.</p> <p>Encoder invariance: This means that \(s_y\) will be the same for different values of \(y\). The encoder ignores aspects of the state that may vary.</p> <p>Latent variable predictor: Varying \(z\) will lead to different plausible predictions of \(s_y\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/lv_jepa-480.webp 480w,/assets/img/blog/jepa/lv_jepa-800.webp 800w,/assets/img/blog/jepa/lv_jepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/lv_jepa.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="JEPA with a latent variable" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>There are four criteria that can be used to train this architecture without contrastive loss:</p> <ol> <li>Maximize the information content of \(s_x\) about \(x\): \(-I(s_x)\)</li> <li>Maximize the information content of \(s_x\) about \(y\): \(-I(s_y)\)</li> <li>Make \(s_y\) predictable from \(s_x\): \(D(s_y, \tilde{s_y})\)</li> <li>Minimize the information content of the latent variable with a regularizer: \(R(z)\)</li> </ol> <h3 id="hierarchical-jepa-h-jepa">Hierarchical JEPA (H-JEPA)</h3> <p>There is a trade off between information loss in the encoding and the predictability of the encodings. If a representation contains most of the information of the input, it would be hard to predict. A more abstract and higher level representation would be lower in dimension and more predictable. Higher dimension representations are also more suitable for longer term predictions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/hjepa-480.webp 480w,/assets/img/blog/jepa/hjepa-800.webp 800w,/assets/img/blog/jepa/hjepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/hjepa.png" class="image-fluid mx-auto d-block" width="500" height="auto" alt="H-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"> Source </a> </figcaption> </figure> <p>H-JEPA (Hierarchical JEPA) enhances JEPA’s abstraction capabilities by splitting the architecture into two parts. The first JEPA handles low-level representations for short-term predictions, while the second operates at a higher abstraction level for longer-term forecasts. This two-tier structure, though innovative, is arbitrary. True intelligence requires multiple levels of abstraction. However, it is not clear how many levels of abstraction are needed. We may even need variable levels of abstraction. Different situations have different levels of complexity.</p> <p>This architecture can enable higher level planning. In JEPA-2, we can sample from the latent variable for several time steps. Directed search / pruning can be employed in order to efficiently search. This search can be used to determine an optimal action.</p> <p>This kind of search would be different in JEPA-1 or without H-JEPA because the latent dimension would be too large to efficiently sample from. Abstraction is needed to enable this kind of planning.</p> <h2 id="world-model-architecture">World Model Architecture</h2> <p>The world is unpredictable but the agent itself is predictable to the agent. This may motivate a model of self (ego model) that does not have a latent variable.</p> <p>The state of the world varies only slightly between time steps. Rather than regenerating, it can be updated in memory. With this architecture, the world model will only output the change in the state. This can be implemented with an attention-like mechanism.</p> <ol> <li>The world model outputs query value pairs: \((q[i], v[i])\)</li> <li>The world model retrieves a value from memory using the query <ul> <li> \[\mathrm{Mem}(q) = \sum_jc_jv_j\] <ul> <li>The value retrieved from memory is a weighted sum of all values.</li> </ul> </li> <li> \[\tilde{c}_j = \mathrm{Match}(k_j,q)\] <ul> <li>Measures dissimilarity between the key and query.</li> </ul> </li> <li> \[c = \mathrm{Normalize}(\tilde{c})\] <ul> <li>This is often a softmax.</li> </ul> </li> <li> \[v_j = \mathrm{Update}(r,v_j,c_j)\] <ul> <li>Value is updated using the current value and new value.</li> <li>The update function can be \(cr+(1-c)v\)</li> </ul> </li> </ul> </li> </ol> <h1 id="data-streams">Data Streams</h1> <p>In building a world model, we have to consider the fundamental differences in the type of data that humans and AI models process. Yann lists 5 modes of information gathering that an agent can use to learn its world model.</p> <ol> <li>Passive observation: sensor stream without control</li> <li>Action foveation: The agent can direct attention within the data stream</li> <li>Passive agency: Observing another agent’s actions and causal effects</li> <li>Active Egomotion: The sensors can be configured, for example moving a camera</li> <li>Active Agency: Sensory streams that are influenced by the agent’s actions</li> </ol> <p>Current AI methods largely focus on passive observation. Other modes may be needed to reach intelligence.</p> <p>AI is trained on internet data. Internet data is not experienced by the agent. Humans train on data that they experience. This is a fundamental difference. This is also why autonomous cars need so much training data. The AI driving systems don’t have other datasets that they have experienced. For example, if they trained on a large dataset of just walking around, they would need less driving data.</p> <p>It is challenging to create large-scale datasets from the perspective of an agent, especially reaching the scale of internet datasets. A present-day example is autonomous car datasets. AV companies have large fleets of vehicles on the road collecting data. These are active data streams.</p> <h1 id="objective-driven-ai">Objective Driven AI</h1> <p>The components of this architecture can be put together to build an intelligent system that follows human defined objectives.</p> <p>Perception is used to generate an initial representation of the state of the world. The actor proposes a sequence of actions. The world model then predicts the state reached if the action sequence is executed. This state is then used in the objectives. The task objective defines what we want the system to do. This could be a task or particular problem. The guardrail objective makes sure the system accomplishes the task without any unwanted behavior. These guardrails would be designed for safety.</p> <p>The action sequence is optimized with respect to the objects. There will be a lot of flexibility in designing the objects to get the system to behave in the way we want.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/objective_driven_ai-480.webp 480w,/assets/img/blog/jepa/objective_driven_ai-800.webp 800w,/assets/img/blog/jepa/objective_driven_ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/objective_driven_ai.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Objective Driven AI" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf"> Source </a> </figcaption> </figure> <p>The system can also be extended to achieve hierarchal planning. The higher levels of planning produce a state that will serve as an objective for the lower level. This state can be considered as a subgoal that is necessary to achieve the higher level goal. We can have unique objectives and guardrails for each level of planning.</p> <p>Latent variables are also introduced to represent the uncertainty in predictions of future states. The latent variables at the higher levels can be thought as imaginary higher level actions. However, only the lower level actions can actually be directly executed.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/h_objective_driven_ai-480.webp 480w,/assets/img/blog/jepa/h_objective_driven_ai-800.webp 800w,/assets/img/blog/jepa/h_objective_driven_ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/h_objective_driven_ai.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Hierarchal Objective Driven AI" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf"> Source </a> </figcaption> </figure> <h1 id="towards-implementing-jepa">Towards Implementing JEPA</h1> <p>The JEPA paper is a position paper that describes a vision for AI that may take decades to materialize. However, since its publication in the summer of 2022, there have been a few steps in advancing the architecture. These papers particularly explore the training of JEPAs. They do not explore the other components such as planning. These JEPAs are the first steps to creating a world model.</p> <p>These are essentially self supervised pretraining methods. When comparing against other works, these papers cite training speed as their advantage. They can achieve strong downstream performance with fewer pretraining epochs.</p> <h2 id="i-jepa-self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture">I-JEPA: <a href="https://arxiv.org/abs/2301.08243">Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</a></h2> <p>Compared to other image SSL approaches, I-JEPA takes advantage of the flexibility of the transformer architecture. ViT is used because it can handle an arbitrary amount of patches in an image, without requiring a strict shape in the input like CNNs</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/ijepa-480.webp 480w,/assets/img/blog/jepa/ijepa-800.webp 800w,/assets/img/blog/jepa/ijepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/ijepa.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="I-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2301.08243"> Source </a> </figcaption> </figure> <p>The input image is split into \(N\) non-overlapping patches and fed into a target encoder \(f_{\theta}\) to compute patch representations. \(s_y = \{s_{y1} … s_{yN}\}\)</p> <p>\(M\) possibly overlapping blocks are sampled from these representations. These blocks are basically larger sections of the image that contain multiple patches.</p> <p>Context is generated by sampling a block (larger than the target blocks). When predicting a target from this context, the overlap with the target block is masked from the context. The network is trained to predict the representations of the target blocks given the context block, and position encodings for the target block. The position encodings are added to the input so that the model knows where the target is. It is just tasked with predicting representations at those positions.</p> <p>This architecture avoids collapse by having exponential moving average weights in the target encoder. This is the same approach used in data2vec and BYOL.</p> <p>The main hyperparameters introduced by this work is the scale and aspect ratio of the target and context blocks. Generally, a small context is used to make this task difficult, which would force the model to learn higher level and more useful features.</p> <h2 id="v-jepa-revisiting-feature-prediction-for-learning-visual-representations-from-video">V-JEPA: <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/">Revisiting Feature Prediction for Learning Visual Representations from Video</a></h2> <p>V-JEPA is an extension of I-JEPA to videos. This is done by treating videos are 3d images.</p> <ol> <li>A clip of 64 frames (~2.1 seconds of video at 30 frames per second) is extracted from the video and resized to 16 × 224 × 224 × 3.</li> <li>The clip is split into \(L\) spatiotemporal patches of size 16x16x2 (2 is the number of consecutive frames.</li> <li> <p>A random mask is calculated for the context. This is a 2D that is similar to the mask in I-JEPA. This mask is then repeated across the time dimension. This repetition is necessary because the videos are short and there would be too much redundancy for the same patch at different time steps. This redundancy would make the learning task too easy. This masking creates a context image, while the target is the original image.</p> <ol> <li>2 masks are sampled: one short range and one long range. The short range mask covers less area in the image and is more discontinuous. These masks are constructed by different configurations of overlapping blocks, as done in I-JEPA. The target encoder only needs to run once, even if there are multiple masks for the context. Having multiple masks leads to more efficient training.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/vjepa_masking-480.webp 480w,/assets/img/blog/jepa/vjepa_masking-800.webp 800w,/assets/img/blog/jepa/vjepa_masking-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/vjepa_masking.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="V-JEPA masking" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Short-range (left), long-range (right) <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/"> Source </a> </figcaption> </figure> <ol> <li>The tokens are processed by a transformer encoder (linear projection of patches + multiple transformer blocks). The masked out patches do not need to be processed. There is a separate encoder for the target and context. The target encoder is an EMA of the context encoder (same as I-JEPA).</li> <li>The predictor predicts the representations of the masked tokens by the unmasked tokens processed by the context encoder. The loss is the L1 distance between the representations of these masked tokens (from the target encoder, and the context encoder + predictor).</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/vjepa-480.webp 480w,/assets/img/blog/jepa/vjepa-800.webp 800w,/assets/img/blog/jepa/vjepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/vjepa.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="V-JEPA Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Very similar to I-JEPA but with an added temporal dimension. <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/"> Source </a> </figcaption> </figure> <p>This is predicting gaps in short videos. It does not predict across time. Human learning is across the time dimension.</p> <p>Attentive probing is used to evaluate this model on different finetuning tasks. This is needed in place of linear probing since the input size may vary. This just requires learning a query token specific to the task and a linear classifier on top of the pretrained encoder.</p> <p>V-JEPA processes small sequences of frames. These short videos are essentially images with a little animation. However, that is the current state of video self-supervised learning. To achieve a model that is closer to human or even animal-level intelligence, this approach needs to scale up significantly. The resolution of the video needs to be increased. Also, the model needs to process longer durations of video and make predictions across time. For example, you should be able to predict what happens in the next 1 minute, based on the previous ten minutes of video input. Such a model could be the basis for an intelligent agent’s world model.</p> <p>V-JEPA is a very interesting model that may be the start of a highly important line of research.</p> <h2 id="mc-jepa-a-joint-embedding-predictive-architecture-for-self-supervised-learning-of-motion-and-content-features">MC-JEPA: <a href="https://arxiv.org/abs/2307.12698">A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</a></h2> <p>This is an extension of JEPA to include motion information. It uses an optical flow objective to learn motion from videos and uses general SSL to learn about the content of the images/videos. Optical flow is estimating the direction in which pixels move between two consecutive frames of a video.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/mcjepa_architecture-480.webp 480w,/assets/img/blog/jepa/mcjepa_architecture-800.webp 800w,/assets/img/blog/jepa/mcjepa_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/mcjepa_architecture.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="MC-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2307.12698"> Source </a> </figcaption> </figure> <p>The details of this dense flow estimation are out of the scope of this blog post. Flow estimation and content feature learning are combined as a multitask learning objective. Images are sampled for content learning, while consecutive frames are sampled from videos for flow estimation. The encoder is shared for both tasks. This is a JEPA architecture because the representations from one frame are warped to match the representations from the next frame. The same encoder is used to process both frames.</p> <p>The architecture for flow estimation is hierarchal. This may be the first instantiation of an H-JEPA architecture. This architecture is based on <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_PWC-Net_CNNs_for_CVPR_2018_paper.pdf">PWC-Net</a>. Each level has a different resolution.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/mcjepa_full_architecture-480.webp 480w,/assets/img/blog/jepa/mcjepa_full_architecture-800.webp 800w,/assets/img/blog/jepa/mcjepa_full_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/jepa/mcjepa_full_architecture.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="MC JEPA full architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2307.12698"> Source </a> </figcaption> </figure> <p>The image features are sampled from ImageNet, while a video dataset is used for flow estimation. It is also possible to use frames from video as images for content learning.</p> <p>This work shows that the JEPA framework is generalizable. There are a lot of ways that we could design a world model and it could include many possible objectives.</p> <h2 id="whats-next">What’s Next?</h2> <p>The current research in JEPA represents a significant step towards Yann LeCun’s vision of building a world model capable of human-level AI. While the present focus is on creating effective representation learning models for visual data, the ultimate goal is far more ambitious. The holy grail of this research is a V-JEPA model that can predict across extended time horizons, potentially through a Hierarchical JEPA architecture capable of processing complex, lengthy videos like 10-minute YouTube clips.</p> <p>To realize this vision, several crucial advancements are necessary. Firstly, we need to embrace true multimodality, incorporating audio and other modalities that are often overlooked in current video models. Scaling up V-JEPA is also essential, requiring larger video datasets and more sophisticated model architectures that can handle higher resolutions. Additionally, the development of more challenging benchmarks for video understanding is critical, as current standards fall short of the complexity seen in image or language modeling tasks.</p> <p>Future iterations of V-JEPA must evolve beyond spatial masking to make predictions across various time horizons. This capability to forecast future representations based on present information is fundamental to understanding the temporal dynamics of video content. Achieving this may necessitate a hierarchical JEPA structure, where different levels handle predictions at various time scales and abstraction levels. Maybe the next JEPA paper will introduce a hierarchal video JEPA (HV-JEPA).</p>]]></content><author><name></name></author><category term="self-supervised-learning"/><category term="ai"/><category term="computer-vision"/><summary type="html"><![CDATA[In the AI research community, Yann LeCun has a unique and often controversial perspective. As of 2024, LLMs and Generative AI are the main focus areas of the field of AI. We’ve all been impressed by the performance of LLMs in various contexts, and generative systems like OpenAI’s Sora. However, it is not clear where these advances fit in the long term goal of achieving and surpassing human level intelligence, which many call AGI.]]></summary></entry><entry><title type="html">Scaling Deep Learning</title><link href="https://rohitbandaru.github.io/blog/Scaling-Deep-Learning/" rel="alternate" type="text/html" title="Scaling Deep Learning"/><published>2023-02-21T00:00:00+00:00</published><updated>2023-02-21T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Scaling-Deep-Learning</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Scaling-Deep-Learning/"><![CDATA[<p>Many of the state-of-the-art results in deep learning are achieved using multiple GPUs. For some of the largest and most data-intensive ML models, it can take months or even years to train on one CPU or GPU. Training is sped up by scaling to large numbers of GPUs/TPUs. Some neural networks are too large to even fit on one GPU. For example, training large language models like BERT can easily exceed the available memory on a single <a href="https://github.com/google-research/bert/blob/master/README.md#out-of-memory-issues">GPU</a>.</p> <p>If you have the resources it can be easy to speed up training by adding more GPUs. However, it is important to understand the impact this scaling will have on training. Machine learning acceleration is a huge and complex field. I intend to just cover the basic intuitions to keep in mind when training a typical model.</p> <aside> ✍️ We will use GPU and TPU interchangeably. We are treating ML accelerators as generic. </aside> <p>There are two types of machine learning training parallelization: data parallelism and model parallelism.</p> <h1 id="data-parallelism">Data Parallelism</h1> <p>Data parallelism splits a training batch into smaller batches for each GPU. Each GPU has its own copy of the model. Each GPU computes gradients with its own training batch. These gradients are then aggregated across all the GPUs. Each GPU can send its gradients to all other GPUs. For example, if you train with a batch size of 64 and 4 GPUs, each GPU will get a batch size of 16. It will compute gradients for this batch. Once all the GPUs are done with their computations, they can send their gradients to each other. The gradients are then averaged and applied to the model. This allows us to train a model with batch size 64 at the speed of batch size 16. However, there is additional latency in communicating the gradients and synchronizing the GPUs, but it is usually negligible compared to the gradient computations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/data_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/data_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/data_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/data_parallelism.png" width="100%" height="auto" alt="Data parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Another option is to skip the gradient aggregation and simply apply the updates to the model separately. This can be done by having an orchestrator take a lock on the model. Or in <a href="https://arxiv.org/abs/1106.5730">Hogwild</a>, you can just update the model without any lock. This will allow some GPU batches to be dropped due to race conditions but minimizes synchronization delays.</p> <p>Adding GPUs doesn’t make training steps faster. It allows you to have larger mini-batch sizes, which in turn trains models faster.</p> <p>There are three variables to consider: mini-batch size, GPU batch size, and number of GPUs. Since the number of GPUs is a function of the other two variables, we will only discuss the two types of batch size and how to optimize them.</p> \[\textrm{Mini batch size} = \textrm{GPU batch size} * \textrm{Number of GPUs}\] <p>The implementation of parallelism can vary between ML frameworks: <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch</a>, <a href="https://www.tensorflow.org/guide/distributed_training">TensorFlow</a>, <a href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html#way-batch-data-parallelism">Jax</a></p> <h2 id="optimal-gpu-batch-size">Optimal GPU Batch Size</h2> <p>With GPUs, we simply want to minimize the training step time. If we operate the GPUs in the optimal GPU batch size range, we can then just set the number of GPUs to get the optimal mini-batch size. Also, note the GPU batch size has an upper limit from the memory available.</p> <p>To see how GPU performance relates to speed, I timed the training steps of a ResNet50 model against ImageNet-sized batches of different sizes. I tested batch sizes of every power of two until the GPU ran out of memory.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/step_speed_vs_batch_size-480.webp 480w,/assets/img/blog/scaling_ml/step_speed_vs_batch_size-800.webp 800w,/assets/img/blog/scaling_ml/step_speed_vs_batch_size-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/step_speed_vs_batch_size.jpg" width="100%" height="auto" alt="step speed vs batch size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We see that the throughput (examples per ms) is maximized at the largest possible batch size. We also see that for batch sizes of less than 2^4 or 16, the throughput is lower. GPUs are inefficient with small batches due to overhead. CPUs perform better in some settings. The takeaway is that we want to maximize the GPU utilization by fitting the largest possible batch. Some libraries have the functionality to search for the largest possible batch size given a GPU and dataset. In the flat region, GPU step time increases linearly with batch size.</p> <h2 id="optimal-mini-batch-size">Optimal Mini Batch Size</h2> <p>To optimize the mini-batch size, we will ignore accelerators and just focus on mini-batch gradient descent. Mini batch size is less hardware-dependent and more problem dependent. We will use ImageNet as an example, but the effects of batch size on training should be considered for every new problem.</p> <p>Assuming maximize GPU usage/batch size, optimizing the mini-batch size means selecting the number of GPUs to use. The assumption is that with more GPUs, we can train a model faster. Training on more GPUs means faster training epochs. However, we are interested in the test accuracy of the model, not just completing epochs.</p> <p>Consider this plot from the paper <a href="https://arxiv.org/abs/1811.03600">Measuring the Effects of Data Parallelism on Neural Network Training</a> by Shallue et al.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/effects_of_dp-480.webp 480w,/assets/img/blog/scaling_ml/effects_of_dp-800.webp 800w,/assets/img/blog/scaling_ml/effects_of_dp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/effects_of_dp.png" width="500" height="auto" alt="Plot of training speed vs batch size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1811.03600"> Source </a> </figcaption> </figure> <p>For points on the dashed line, the number of training steps is halved whenever the batch size is doubled. This means that doubling the GPUs/TPUs would have the training time. This is ideal. In this region, you can happily speed up your model training by adding more GPUs that you may have available. However, this tradeoff changes at batch size 2^13 or 8192. From here, doubling the GPUs still speeds up model training, but the speed will be more than half. This is the point of diminishing returns. If you have the GPUs, you might as well use them but those additional GPUs are not as effective.</p> <p>The paper goes into great detail on this relationship and the effects of other factors such as model architecture, optimizers, and datasets. The takeaway for this blog is that if you set the maximum GPU batch size, up to a point, adding additional GPUs will linearly speed up the training of your model.</p> <h1 id="model-parallelism">Model Parallelism</h1> <p>This type of parallelism is much less commonly used. It can be used along with data parallelism. Model parallelism is when an ML model is too large to fit in the memory of one device. It is partitioned across multiple devices. This has enabled us to train larger and larger networks. For example, the GPT-3 model is about <a href="https://www.reddit.com/r/MachineLearning/comments/gzb5uv/comment/fti44lv/?utm_source=share&amp;utm_medium=web2x&amp;context=3">350 GB</a>. No single GPU can store the whole model in memory.</p> <p>There are different ways of achieving model parallelism. You can split the model vertically by layer, or horizontally by splitting the tensors.</p> <h2 id="pipeline-parallelism">Pipeline Parallelism</h2> <p>The simplest solution is to process different layers of a neural network on different accelerations. A simple illustration of this:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipeline_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/pipeline_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/pipeline_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/pipeline_parallelism.png" width="100%" height="auto" alt="pipeline parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In <a href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html#speed-up-by-pipelining-inputs">PyTorch</a>, the layers are bucketed into groups of roughly equal memory so that the computations are evenly distributed across the accelerators.</p> <p>A major issue with this approach is that after Layer 0 has a forward pass, it has to wait for the other layers to compute forward and backward passes. The GPU is idle for about 75% of the time. The following diagrams are from the <a href="https://arxiv.org/abs/1811.06965">GPipe</a> paper by Huang et al.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism.png" width="100%" height="auto" alt="batches without pipeline parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1811.06965"> Source </a> </figcaption> </figure> <p>The solution is to have the layer compute the next graph while it is waiting for the gradient of the current batch. This essentially combines data parallelism with model parallelism. I explained above that to maximize training speed, we want to maximize the utilization of accelerators. For large models that require model parallelism, we have an additional problem of GPU waiting time. Pipelining GPU batches helps reduce this gap.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipeline_parallelism_batches-480.webp 480w,/assets/img/blog/scaling_ml/pipeline_parallelism_batches-800.webp 800w,/assets/img/blog/scaling_ml/pipeline_parallelism_batches-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/pipeline_parallelism_batches.png" width="100%" height="auto" alt="Pipeline parallelism batches" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1811.06965"> Source </a> </figcaption> </figure> <p>We see that with 4 GPU batches, each GPU is idle for about 6/16 of the time. The variables here are the number of GPUs and number of GPU batches per GPU. With 1 GPU batch (no pipelining), the utilization is: \(\frac{2}{n_{GPU}* 2} = \frac{1}{n_{GPU}}\). With pipelining, we get \(\frac{n_{batches}*2}{n_{batches}*2 + 2* (n_{GPU}-1)}\). This simplifies to the following:</p> \[utilization = \frac{n_{batches}}{n_{batches} + n_{GPU}-1}\] <p>This equation explains the tradeoff. Increasing the number of GPU batches drives the utilization closer to 1, while increasing the number of GPUs reduces the utilization.</p> <p>In an optimal setup, we split the model among as few GPUs as possible, but increase the number of batches that they process in a step. Pipeline parallelism has the added benefit of the speedups of data parallelism. This makes it a very effective solution.</p> <p>In the <a href="https://arxiv.org/abs/1806.03377">PipeDream</a> paper, Harlap et al. show that we can further reduce idle time by interleaving forward and backward operations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipedream-480.webp 480w,/assets/img/blog/scaling_ml/pipedream-800.webp 800w,/assets/img/blog/scaling_ml/pipedream-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/pipedream.png" width="100%" height="auto" alt="pipedream" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1806.03377"> Source </a> </figcaption> </figure> <p>However, this eliminates gradient synchronization. Even eliminating batches above 4, we get the same utilization as GPipe parallelism, just in a different order. For many of the backward passes, a stale version of the model parameters is used. Gradient synchronization is an important tradeoff in all types of ML parallelism.</p> <p>In analyzing utilization, we have been assuming that forward and backward computations are equivalent. Backward passes tend to take more time. If we interleave operations as to always prioritize backward passes, we can get a utilization gain. From AWS Sagemaker <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html">documentation</a>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipedream1-480.webp 480w,/assets/img/blog/scaling_ml/pipedream1-800.webp 800w,/assets/img/blog/scaling_ml/pipedream1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/pipedream1.png" width="100%" height="auto" alt="without backward prioritization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html"> Source </a> </figcaption> </figure> <p>The idle time here is 1 forward pass and 1 backward pass.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipedream2-480.webp 480w,/assets/img/blog/scaling_ml/pipedream2-800.webp 800w,/assets/img/blog/scaling_ml/pipedream2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/pipedream2.png" width="100%" height="auto" alt="with backward prioritization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html"> Source </a> </figcaption> </figure> <p>With backward prioritization, the idle time for GPU 0 is 3 forward passes. This effect will be increased with more GPUs. There are many tradeoffs in parallel ML, such as communication between model layers, the memory overhead of forward and backward passes, different model splits, staleness, etc. We are only covering the high-level intuitions to achieve fast and effective training of large models.</p> <p>What if we want to use more GPUs for data parallelism, but without splitting up the model further? We can simply run pipelines in parallel. For example, we can split the model among four GPUs but duplicate each model split twice. We can then aggregate the gradients of both pipelines in the update. This is often called hybrid model and data parallelism.</p> <h2 id="tensor-parallelism">Tensor Parallelism</h2> <p>Instead of splitting the model into layers, we can split the layers themselves. From the <a href="https://arxiv.org/abs/1909.08053">Megatron-LM paper</a> by Shoeybi et al.:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/tensor_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/tensor_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/tensor_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/scaling_ml/tensor_parallelism.png" width="100%" height="auto" alt="tensor parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1909.08053"> Source </a> </figcaption> </figure> <p>The input X has to be completely copied for each split of the model. The layer is split into two halves. The splits of the model are then aggregated in the last layers of the model. Splitting the tensors themselves offers some benefits. The latency is reduced since you can fit more layers on a GPU. This is parallel computation instead of serialized computation. You don’t have to worry about scheduling to minimize idle time.</p> <p>An issue with this approach is that the activations are also separated, so you are learning a different model architecture. There is an additional cost in concatenating \(Y_1\) and \(Y_2\) for both GPUs. The Megatron-LM architecture is designed to reduce the cost of communicating between GPUs.</p> <h1 id="conclusion">Conclusion</h1> <p>We touched the surface on the many tradeoffs, optimizations, and considerations needed for distributed and large scale ML. As models grow larger, it will become more import to understand and keep up to date with this field.</p> <h1 id="additional-resources">Additional Resources</h1> <p><a href="https://www.youtube.com/watch?v=3XUG7cjte2U">https://www.youtube.com/watch?v=3XUG7cjte2U</a></p> <p><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">https://lilianweng.github.io/posts/2021-09-25-train-large/</a></p> <p><a href="https://openai.com/blog/techniques-for-training-large-neural-networks/">https://openai.com/blog/techniques-for-training-large-neural-networks/</a></p> <p><a href="https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/">https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/</a></p> <p><a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">https://huggingface.co/docs/transformers/v4.15.0/parallelism</a></p>]]></content><author><name></name></author><category term="applied-ml"/><summary type="html"><![CDATA[Many of the state-of-the-art results in deep learning are achieved using multiple GPUs. For some of the largest and most data-intensive ML models, it can take months or even years to train on one CPU or GPU. Training is sped up by scaling to large numbers of GPUs/TPUs. Some neural networks are too large to even fit on one GPU. For example, training large language models like BERT can easily exceed the available memory on a single GPU.]]></summary></entry><entry><title type="html">Knowledge Distillation as Self-Supervised Learning</title><link href="https://rohitbandaru.github.io/blog/knowledge-distillation-ssl/" rel="alternate" type="text/html" title="Knowledge Distillation as Self-Supervised Learning"/><published>2022-01-11T00:00:00+00:00</published><updated>2022-01-11T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/knowledge-distillation-ssl</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/knowledge-distillation-ssl/"><![CDATA[<p>Self-supervised learning (SSL) methods have been shown to effectively train large neural networks with unlabeled data. These networks can produce useful image representations that can exceed the performance of supervised pretraining on downstream tasks. However, SSL is not effective with smaller models. This limits applications where computational power is limited, such as edge devices. Knowledge distillation (KD) is a popular method to train a smaller student network from a larger and more powerful teacher network. The <a href="https://arxiv.org/abs/2101.04731">SEED</a> paper by Fang et al., published in ICLR 2021, applies knowledge distillation to self-supervised learning to pretrain smaller neural networks without supervision. In this post, we will discuss self-supervised learning and knowledge distillation and how they are unified in SEED.</p> <h1 id="self-supervised-learning">Self-supervised Learning</h1> <p>Self-supervised learning is a form of unsupervised learning. Self-supervision refers to labels that are generated from the data itself rather than manual annotations (ex: images vs class labels). Different SSL methods have different tasks that are used for the self-supervision.</p> <p>In computer vision, it is very common to pretrain a neural network on ImageNet classification. This is an example of supervised pretraining. This network can then be fine-tuned for various downstream tasks, such as semantic segmentation, object detection, or even medical image classification. Supervised pretraining has been a standard practice in computer vision.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/sl_vs_ssl-480.webp 480w,/assets/img/blog/distillation_ssl/sl_vs_ssl-800.webp 800w,/assets/img/blog/distillation_ssl/sl_vs_ssl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/sl_vs_ssl.png" width="400" height="auto" alt="Self-supervised vs Supervised Pretraining" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Self-supervised vs Supervised Pretraining </figcaption> </figure> <p>Self-supervised learning provides an alternative to supervised pretraining with two main benefits:</p> <ol> <li> <p>Generalizability: A supervised objective like classification can limit what a model learns about data. This is because not all the information in an image is needed for classification. For example, you can train a network to classify cats and dogs. The color of the animal’s fur is not relevant to this objective. Therefore, representations from this network may not be useful for a downstream task of fur color classification.</p> </li> <li> <p>Unlabeled data: The amount of available unlabeled data dwarfs labeled datasets. SSL is a form of unsupervised learning. It can leverage datasets of billions of images rather than be limited to supervised datasets, such as ImageNet which has about one million images.</p> </li> </ol> <p>There are many methods of SSL. Most of the recent state of art methods implement a form of contrastive learning. This includes <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, <a href="https://arxiv.org/abs/2006.09882">SwAV</a>, and <a href="https://arxiv.org/abs/1911.05722">MoCo</a>. In contrastive learning, representations are pushed towards positive examples and away from negative examples. In SSL, the positive examples are variations of the original image and the negative examples are from other images in the dataset. Contrastive SSL methods share some common steps:</p> <ol> <li> <p>Image augmentation: In supervised learning, augmentations, such as random cropping, flipping, and color distortions are used to generate more training data. In SSL, augmentation is used to produce positive examples. It is needed to avoid the trivial solution of encoding raw pixel values without learning anything about the content of the image.</p> </li> <li> <p>Contrastive loss: The goal of contrastive learning is to push positive examples closer together and negatives apart. It is most common to see a version of the <a href="https://arxiv.org/abs/1807.03748">InfoNCE</a> loss. This loss (defined below) is meant to maximize the similarity of a data point with one positive example, and minimize the similarity with many negative examples. The similarity function \(s\) is usually just the dot product.</p> </li> <li> <p>Negative samples: SSL needs a large amount of negative examples for the best performance. We want to push an image representation away from all other possible image representations from the dataset. This can be accomplished by having a large batch size (SimCLR). All the other images in the batch will be negative examples. An alternative is to keep negative examples in memory through multiple training batches. MoCo does this by keeping a queue of the most recent image representations. It is preferred to keep recent image representations, since the network changes gradually over time. Recent representations are more similar to representations that would be generated from the current network. The queue essentially approximates a large training batch.</p> </li> </ol> \[\begin{equation} \mathcal{L}_{\mathrm{InfoNCE}} = -\mathbb{E} \left[ \mathrm{log} \frac{ \exp(s(x, y)) } { \sum_{y_j} \exp(s(x,y_j)) } \right] \end{equation}\] <h2 id="moco">MoCo</h2> <p><a href="https://arxiv.org/abs/1911.05722">MoCo</a> (momentum contrast) by He et al. implements contrastive SSL by keeping a queue of examples. The queue allows for a large number of negative examples to be used in the contrastive loss The momentum encoder is trained at the same time as the encoder in a bootstrapped fashion. They must have identical architectures for the momentum update to occur.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/moco-480.webp 480w,/assets/img/blog/distillation_ssl/moco-800.webp 800w,/assets/img/blog/distillation_ssl/moco-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/moco.png" width="700" height="auto" alt="MoCo training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>With a queue of representations encoded by the momentum encoder, the main encoder is trained to contrast the representations. \(q\) is the query or the representation from the encoder. \(k_+\) is the corresponding representation from the momentum encoder. The loss aims to push \(q\) towards \(k_+\) and away from all other representations \(k\) in the queue which serve as negative examples.</p> \[\begin{equation} \mathcal{L}_i = -\log\frac{\exp(q*k_+/\tau)}{\sum_{i=0}^K\exp(q*k_i/\tau)}\\ \end{equation}\] <p>MoCo is very effective in pretraining large neural networks for many downstream tasks. SEED aims to extend this for smaller networks.</p> <h1 id="knowledge-distillation">Knowledge Distillation</h1> <p>In knowledge distillation (<a href="https://arxiv.org/abs/1503.02531">Hinton et al.</a>, <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">Buciluǎ et al.</a>), a large teacher model is used to train a smaller and more efficient student model. It is useful in the case that a large neural network can perform well on a task, but a small network cannot be directly trained to high accuracy. This makes it relevant to SSL, where only large neural networks have strong performance.</p> <p>In supervised learning for classification, the labels are hard targets or one-hot encoded vectors. All of the probability is assigned to one class, and all other classes have a value of zero. The teacher model will have a softmax layer which will return a soft target. The soft target will assign some probability to other classes. Knowledge distillation uses the teacher network to produce these soft targets and uses them to train the student model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/soft_vs_hard-480.webp 480w,/assets/img/blog/distillation_ssl/soft_vs_hard-800.webp 800w,/assets/img/blog/distillation_ssl/soft_vs_hard-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/soft_vs_hard.png" width="600" height="auto" alt="dark knowledge" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Shiba Inu dogs are known to have cat-like characteristics, soft labels can encode this by assigning some probability to the cat class. </figcaption> </figure> <p>The soft targets encode more information than hard targets. Hinton describes this as “dark knowledge”. For example, from a soft target you can tell which class is the second most likely or the relative probabilities between two classes. This information is not available in a hard target.</p> \[\begin{equation} p_i = \frac{\exp(\frac{z_i}{T})}{ \sum_{j} \exp(\frac{z_j}{T})}\\ \end{equation}\] <p>The soft targets can be made softer by increasing the temperature of the softmax. The temperature \(T\) is typically set to 1. However, in knowledge distillation higher temperatures can yield better results as it increases the magnitude of the non-max values.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/kd-480.webp 480w,/assets/img/blog/distillation_ssl/kd-800.webp 800w,/assets/img/blog/distillation_ssl/kd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/kd.png" width="700" height="auto" alt="Knowledge Distillation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Knowledge Distillation </figcaption> </figure> <ol> <li> <p>A teacher model is trained for high accuracy. This can be a large neural network or an ensemble.</p> </li> <li> <p>The teacher model generates soft labels for a dataset. This dataset can be the same or different from the hard labeled dataset.</p> </li> <li> <p>The student network is trained to predict the soft labels. It can also be simultaneously trained with hard labels in a separate loss term.</p> </li> </ol> <p>Distillation can use unlabeled data. Once a model is trained, it can be used to produce soft labels for a large unsupervised dataset. This can be larger than the initial labeled dataset and effectively train the student network on a much larger dataset.</p> <h1 id="knowledge-distillation-for-ssl">Knowledge Distillation for SSL</h1> <p>Knowledge distillation aims to transfer dark knowledge between models. Self-supervised learning aims to increase the dark knowledge learned by a model. When training a model on a supervised classification objective, it will not need to learn information that does not help with classification. The objective limits what the model learns. Self-supervised learning methods are designed to be general and not task specific.</p> <p>SSL does not perform well with smaller models which limits its applicability. Also, the downstream task is likely less complex than the SSL task and can be achieved more efficiently with a smaller model. Knowledge distillation offers a way to reduce the size of the model while maintaining accuracy and relevant knowledge.</p> <p>One way to apply KD to SSL is to train the teacher on a SSL objective and then apply KD to train the student on a downstream task. This would require first fine-tuning the teacher network on the downstream test, with a new output layer. It would be more efficient to distill knowledge to a smaller network before training on the downstream task.</p> <p>Although it is simple to apply knowledge distillation on a supervised downstream task, you cannot directly apply it to the self-supervised training objective. This is because SSL models do not output classification predictions. SSL models output feature representations of the input. Training a student network to match these feature representations would not be effective. Self-supervised training involves optimizing with an objective on top of the representations.</p> <h1 id="seed">SEED</h1> <p>In the <a href="https://arxiv.org/abs/2101.04731">SEED</a> paper, the authors propose a self-supervised approach to knowledge distillation. It uses a contrastive objective on the representations.</p> <p>This will allow knowledge distillation to occur before the downstream task. The method produces an SSL trained student network that can be efficiently fine-tuned on downstream tasks. SEED extends self-supervision to smaller models allowing us to compress SSL models to use in more applications.</p> <h2 id="method">Method</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/seed-480.webp 480w,/assets/img/blog/distillation_ssl/seed-800.webp 800w,/assets/img/blog/distillation_ssl/seed-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/seed.png" width="100%" height="auto" alt="SEED" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> SEED <a href="https://arxiv.org/abs/2101.04731"> Source </a> </figcaption> </figure> <ol> <li> <p>Train the teacher, independent of the student network. Any of the recent state-of-the-art SSL methods or even supervised models (ResNet trained on ImageNet classification) can be used here. The only requirement is that the model must produce image representations. The teacher networks weights are then frozen.</p> </li> <li> <p>Apply an augmentation to the input image. The same augmentation of the image is used for both the student and the teacher networks. In most other SSL methods, different augmentations would be used. SEED reports better performance when using the same augmentation. This may be because trivial solutions are avoided by the pretraining of the teacher network.</p> </li> <li> <p>Input the image to both the student and teacher networks to get two vector representations: \(Z^S\) and \(Z^T\).</p> </li> <li> <p>Add teacher vector \(Z^T\) to the instance queue \(D\) which is a fixed size FIFO queue that persists between training batches. Self-supervised learning in general benefits from a large number of negative examples.</p> </li> <li> <p>Apply the self-supervised SEED loss, using the student and teacher vectors, and the instance queue. The student and teacher vectors are each compared to every embedding in the queue, to produce two similarity vectors. A cross-entropy loss is applied between the similarity vectors The student network is trained to produce vectors that have the same similarities as the teacher. We will further explain the loss used in SEED.</p> </li> </ol> <h2 id="loss">Loss</h2> <p>In self-supervised learning, a supervised objective is formed from the input rather than human annotations. In this case, the supervised objective is predicting the current image representation from a queue containing the current representation and negative examples. Knowledge distillation is applied with respect to this objective. The scores from applying the softmax to the teacher similarity vector form the soft label.</p> <p>The cross-entropy loss is used like the contrastive InfoNCE loss in SSL. The student vector is pushed towards the teacher vector and away from the vectors in the queue. However, some of the negative vectors are closer than others. The student network is also trained to match this information. This is where the dark knowledge of KD is applied.</p> <p>Unlike the InfoNCE loss, there are no hard positive and negative examples in this objective. The teacher network creates a soft probability distribution. Each example is assigned a continuous score between 0 and 1 that indicates how positive the example is. SEED can be viewed as a <em>soft contrastive learning</em> method.</p> \[\begin{align} \mathcal{L}_{SEED} &amp;= - \sum_i^N \textbf{p}^T(\textbf{x}_i; \theta_T, \textbf{D}^+) * \log \textbf{p}^S(\textbf{x}_i; \theta_S, \textbf{D}^+) \\ &amp;= - \sum_i^N \sum_j^{K + 1} \frac{\exp(\textbf{z}_i^T * \textbf{d}_j / \tau^T)}{\sum_{d\sim\textbf{D}^+}\exp(\textbf{z}_i^T * \textbf{d} / \tau^T)} * \log \frac{\exp(\textbf{z}_i^S * \textbf{d}_j / \tau^S)}{\sum_{d\sim\textbf{D}^+}\exp(\textbf{z}_i^S * \textbf{d} / \tau^S)} \end{align}\] <p>For each example in the batch (size \(N\)), two similarity functions are applied: one using the teacher network \(p^T\) and one using the student network \(p^S\). The similarity function is applying an inner product and softmax with the vectors to the instance queues. This produces a probability distribution with more probability on examples in the queue that are close to the input. Since we want these probability distributions to match, a cross entropy loss is applied between the two probability distributions.</p> \[\mathcal{L}_{cross-entropy} = \sum_{i} y_i * \log(\hat{y}_i)\] <p>Referring to the formula for cross-entropy. \(\textbf{p}^T(..)\) corresponds to the label \(y_i\). In classification, \(y_i\) would be binary or a one-hot encoded vector. In this case, \(\textbf{p}^T(..)\) is a score between 0 and 1. As in KD, this is a soft label. With hard labels and standard contrastive learning, the scores would be binary with a 1 for the current datapoint. \(\textbf{p}^S(..)\) corresponds to the prediction \(\hat{y}_i\). Here the prediction is the student similarity score. We want the student to produce similarity scores matching the teacher.</p> <h2 id="seed-vs-moco">SEED vs MoCo</h2> <p>SEED is trained very similarly to MoCo. The differences are the lack of momentum weight updates and the soft contrastive loss.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/seed2-480.webp 480w,/assets/img/blog/distillation_ssl/seed2-800.webp 800w,/assets/img/blog/distillation_ssl/seed2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/seed2.png" width="700" height="auto" alt="SEED training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> SEED training </figcaption> </figure> <h1 id="self-supervised-vs-supervised-knowledge-distillation">Self-supervised vs Supervised Knowledge Distillation</h1> <p>SEED or self-supervised distillation in general does not aim to replace supervised knowledge distillation. The authors report their best results when training models with both self-supervised and supervised knowledge distillation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/distillation_ssl/s_vs_sl_kd-480.webp 480w,/assets/img/blog/distillation_ssl/s_vs_sl_kd-800.webp 800w,/assets/img/blog/distillation_ssl/s_vs_sl_kd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/distillation_ssl/s_vs_sl_kd.png" width="900" height="auto" alt="Self-supervised KD with Supervised KD" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Self-supervised KD with Supervised KD </figcaption> </figure> <p>SEED allows for more effective self-supervised training of smaller models. It is better to train a large model with SSL and distill it to a small model than to train the small model directly. After SEED pretraining, the model can be fine-tuned with supervised knowledge distillation with the downstream task. In this step, the student is initialized from the self-supervised KD trained model, instead of initializing from scratch.</p> <h1 id="conclusion">Conclusion</h1> <p>Self-supervised knowledge distillation allows the impressive gains of large SSL models to be transferred to smaller neural networks. This allows for more applications of these models. We can even view knowledge distillation as a form of self-supervised learning. Hard labels are not used in SSL. The soft labels provide self-supervision since they are produced from the data.</p> <p>SEED essentially adapts momentum contrast to be used as knowledge distillation. An interesting future direction would be adapting other SSL methods such as SimCLR to be used as knowledge distillation. Nearly every contrastive SSL method can be adapted in this way.</p>]]></content><author><name></name></author><category term="paper-review"/><category term="self-supervised-learning"/><category term="knowledge-distillation"/><category term="computer-vision"/><summary type="html"><![CDATA[Self-supervised learning (SSL) methods have been shown to effectively train large neural networks with unlabeled data. These networks can produce useful image representations that can exceed the performance of supervised pretraining on downstream tasks. However, SSL is not effective with smaller models. This limits applications where computational power is limited, such as edge devices. Knowledge distillation (KD) is a popular method to train a smaller student network from a larger and more powerful teacher network. The SEED paper by Fang et al., published in ICLR 2021, applies knowledge distillation to self-supervised learning to pretrain smaller neural networks without supervision. In this post, we will discuss self-supervised learning and knowledge distillation and how they are unified in SEED.]]></summary></entry><entry><title type="html">Self-Supervised Learning  -  Getting more out of data</title><link href="https://rohitbandaru.github.io/blog/Self-Supervised-Learning/" rel="alternate" type="text/html" title="Self-Supervised Learning  -  Getting more out of data"/><published>2021-08-14T00:00:00+00:00</published><updated>2021-08-14T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Self-Supervised-Learning</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Self-Supervised-Learning/"><![CDATA[<p>Yann LeCun <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">describes</a> self-supervised learning as the next big challenge in the field of AI. How does it work? Self-supervised learning (SSL) is a specific type of unsupervised learning. It aims to learn from large datasets of unlabeled data to enable building more robust models in different domains such as vision and NLP.</p> <p>For many computer vision problems, it is a common practice to pretrain the model on a supervised learning task. For example, there are <a href="https://keras.io/api/applications/">many</a> neural networks that are pretrained to do image classification on ImageNet. However, self-supervised learning has recently been shown to outperform supervised pretraining learning on certain tasks. SSL is an active area of research with heavy involvement from top AI labs in Google, Facebook, Deepmind, and academia. Rather than focusing on the details of SSL architectures, we will explore the intuitions on why it works and what is needed.</p> <h1 id="code">Code</h1> <p>In order to make it easy to directly interact with SSL, I wrote a Colab notebook showing a few algorithms. This notebook demonstrates transfer learning on CPC, SwAV, and SimCLR pretrained models on the CIFAR10 classification task. This uses PyTorch Lightning’s implementations of these algorithms.</p> <p><a href="https://colab.research.google.com/drive/1PDCTe5dIQgYyiuLQw3WGyCuT03gX1qZR?usp=sharing"> <img src="/blog/assets/img/colab.svg" alt="Open In Colab"/> </a></p> <p>We are experimenting with the simple example of pretraining on ImageNet and evaluating on CIFAR10 classification. SSL can be effective on other datasets and learning tasks (object detection, segmentation, etc.), but these won’t be the focus of this post.</p> <h1 id="motivations">Motivations</h1> <h2 id="data">Data</h2> <p>Self-supervised learning does not need labels. The amount of unlabeled data generally far exceeds the amount of labeled data. SSL can leverage large amounts of unlabeled data to build powerful models. Although most research does not use datasets larger than ImageNet, there are real world applications of using larger unlabeled datasets. For example, Facebook/Meta can train the <a href="https://ai.facebook.com/blog/seer-the-start-of-a-more-powerful-flexible-and-accessible-era-for-computer-vision/">SEER</a> model on billions of Instagram images.</p> <h2 id="generalizability">Generalizability</h2> <p>If you train a model on image classification, it may not perform as well on non-classification tasks. This is because only part of the image’s information is needed to classify it. A self-supervised learning algorithm may be able to use more of the information in the data.</p> <p>The reason for the generalization gap is that the classification task does not always require a strong understanding of the object. For example, if you trained a supervised model to classify dog breeds, it may only look at the texture and color of the dog’s fur. In order to classify the breeds, the network may not need to understand other characteristics of the dog, such as size and facial features. This model would then not generalize well if you want to add a new dog breed with an indistinctive skin pattern. It will also not generalize well to new tasks like classifying the size or shape of the dog.</p> <h2 id="better-performance">Better Performance</h2> <p>It is common to think that unsupervised / self-supervised learning is only useful when you lack labels to do supervised learning. However, these approaches can actually increase performance compared to a fully supervised approach. The ability to learn more accurate and robust models is what gives self-supervised learning the potential to shift the field of AI.</p> <p>In research, there are comparisons between training on ImageNet images and labels with supervised learning and ImageNet with only images for self-supervised learning. Although the motivation for SSL is often framed as being able to use more data, in this case, the size of the dataset is the same. The ability to use larger unlabeled datasets is just a side benefit of SSL.</p> <h1 id="vision-vs-nlp">Vision vs NLP</h1> <p>Self-supervised learning has been long applied in NLP, but as Yann LeCun and Ishan Misra point <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">out</a>, it is much harder to apply to vision. In NLP, language models are often trained with self supervision. Given a some text, you can mask a word and try to predict it given the rest of the text. There is a limited vocabulary, so you can assign a probability to each word. This is the basis of many popular NLP methods.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/nlp-480.webp 480w,/assets/img/blog/self_supervised_learning/nlp-800.webp 800w,/assets/img/blog/self_supervised_learning/nlp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/nlp.png" class="img-fluid mx-auto d-block" width="400" height="auto" alt="Predicting masked words in NLP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Predicting masked words in NLP </figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/image_patch-480.webp 480w,/assets/img/blog/self_supervised_learning/image_patch-800.webp 800w,/assets/img/blog/self_supervised_learning/image_patch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/image_patch.png" class="img-fluid mx-auto d-block" width="300" height="auto" alt="Image SSL with patches" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Predicting patches of an image is much harder. </figcaption> </figure> <p>The analogue for vision is to mask a patch of an image and try to fill it in. However, because there is an intractable number of possible ways to fill in an image, you can’t compute a probability for each one. There can also be a large number of possible solutions. For example, in the image above, there is many facial expressions the dog could have. The NLP approach is straight forward but cannot be directly applied to vision.</p> <h1 id="pretext-task">Pretext Task</h1> <p>The earlier approaches to self-supervised learning focused on training the network on a pretext task. This task would not require labels in the label. The labels will be made up through the task. In <a href="https://arxiv.org/abs/2012.01985">RotNet</a>, each image is rotated by 0, 90, 180, or 270 degrees, and a network is trained to predict the rotation. In <a href="https://arxiv.org/abs/1603.09246">Jigsaw</a>, the image is split up into patches and scrambled like a jigsaw puzzle. A network is then trained to solve the puzzle by predicting the permutation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/rotnet-480.webp 480w,/assets/img/blog/self_supervised_learning/rotnet-800.webp 800w,/assets/img/blog/self_supervised_learning/rotnet-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/rotnet.png" width="100%" height="auto" alt="RotNet, SSL by predicting rotations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1803.07728"> Source </a> </figcaption> </figure> <p>The problem with pretext task-based SSL is the same as supervised learning. There can be shortcuts to achieve high accuracy on the task. There have been attempts to avoid this. For example, in Jigsaw, each path is randomly cropped, so the task can’t be solved by simply lining up edges. However, the limitations still exist regardless, so more recent research has focused on contrastive learning.</p> <h1 id="contrastive-learning">Contrastive Learning</h1> <p>A neural network outputs a vector representation for every image. The goal of contrastive learning is to push these vectors closer for similar images and pull them apart unrelated images. This in different ways in different research papers.</p> <h2 id="cpc"><a href="https://arxiv.org/abs/1807.03748">CPC</a></h2> <p>Contrastive Predictive Coding is a method developed by Deepmind. It is a generic approach that can be applied to any data modality. In the paper, it is applied to images, audio, and text. It is a very general framework with two main components: an encoder, and an autoregressive model. These can be anything and are designed to fit the domain.</p> <p>The encoder simply encodes the data into a lower-dimensional vector \(z_t\). This can be any model. For images, this can be a convolutional neural network.</p> <p>Autoregressive models the variables in the data are given an order. In images, the pixels can be ordered from left to right and top to bottom. We can imagine unrolling each datapoint (ex: image, audio clip) into a list. We can call each element of this list an observation. CPC encodes a sequence of observations \(X\) into a sequence of encodings \(Z\).</p> \[X = [x_1, x_2, x_3, x_4 ... x_N]\\ Z = [z_1, z_2, z_3, z_4 ... z_N]\\ z_t = g_{enc}(x_t)\] <p>The prediction of an observation in the sequence depends only on the previous observations. This similar to predicting the future from the past in a time series. In CPC, the autoregressive model is used to generate context vectors from the encodings \(z_t\). Context vector \(c_t\) is a function of encodings \(z_{\leq t}\), but not any encoding after \(z_t\). Note that the autoregressive model is trying to predict the encodings of the observations, but not the observations themselves. The architecture of this autoregressive model depends on the application.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/cpc-480.webp 480w,/assets/img/blog/self_supervised_learning/cpc-800.webp 800w,/assets/img/blog/self_supervised_learning/cpc-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/cpc.png" width="100%" height="auto" alt="CPC" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> CPC applied to audio, \(g_{enc}\) is the encoder, \(g_{ar}\) is the autoregressive model <a href="https://arxiv.org/abs/1807.03748"> Source </a> </figcaption> </figure> <p>With these two models, we can generate an encoding of the data and context vectors. These vectors can be used as representations of the data. But how are these models trained? The self-supervised task is essentially predicting the input from the context. For example, given \(c_t\), we want to be able to go backwards and identify that it was generated from \(x_t/z_t\). The models are trained on a contrastive InfoNCE loss.</p> \[\begin{equation} \mathcal{L}_{\mathrm{InfoNCE}} = -\mathbb{E} \left[ \mathrm{log} \frac{ s(x, y) } { \sum_{y_j} s(x,y_j) } \right] \end{equation}\] <p>\(x\) is the sample we are trying to predict. \(c\) is the correct context. \(c_j\) are the context vectors for the negative samples. The negative samples come from other observations of the same datapoint and other datapoints in the batch. We want to maximize \(s(x,c)\) and minimize the sum of \(s(x, y_j)\). This is contrastive in that we are pushing \(y\) to be close to \(x\), and all other \(y_j\) to be far from \(x\).</p> \[\begin{equation} f_k(x_{t+k},c_t) = \mathrm{exp} \left( (g_{enc}(x_{t+k}))^TW_kc_t \right) = \mathrm{exp} \left( z_{t+k}^TW_kc_t \right) \end{equation}\] <p>The \(s\) function is modeled by \(f_k\) a log bilinear model. \(W_k\) is linear transforms the context vector, which can then be compared with the encoding \(z\).</p> <p>To apply this to vision, the image is split up into 7x7 patches (with 50% overlap) which will be considered the observations. Each patch is encoded by a CNN (ResNet without pretraining). If the encoding returns at 1024 dimensional vector, the encoded image will have a size of 7x7x1024. An autoregressive model (<a href="https://arxiv.org/abs/1606.05328">PixelCNN</a> or <a href="https://arxiv.org/abs/1601.06759">PixelRNN</a>) is applied to the encodings of the patches. For 1D data like audio, an RNN/LSTM scan be used. The self-supervised task in this case is predicting which patch generated each context vector. Refer to the PixelRNN paper for more information on autoregressive models and PixelCNN. The final representation is computed by mean pooling the encodings into a single 1024 dimensional vector. This can then be used for downstream tasks, like image classification.</p> <p>Why do we need the autoregressive model? We could optimize the InfoNCE loss using the 7x7 encodings. The self supervised task here is predicting the next context vector given a sequence of context vectors. This is similar to predicting the next patch of an image given all the previous patches. But rather predict the patch, which as we discussed is too difficult, we just predict a lower dimensional vector. Without this autoregressive constraint, we are just optimizing for generating unique embeddings for each patch and ignoring the relation between the patches. The InfoNCE loss is just ensuring that the predictions are correct.</p> <p>Why not just mask out the current context / observation? The architecture for this may be a masked fully connected layer that learns the context vector for each observation, while excluding the connection to the observation itself. Or there could be two PixelCNNs, one from the left and one from the right. We can then concatenate these two context vectors and possibly add additional neural network layers on top of it. Both methods would be more computationally expensive and complex, but likely still feasible. This would be bidirectional model for images similar to <a href="https://arxiv.org/abs/1810.04805">BERT</a>. This idea may be explored in other research papers or, it may be an open idea to try.</p> <h2 id="simclr"><a href="https://arxiv.org/abs/2002.05709">SimCLR</a></h2> <p>SimCLR is a method from Google Brain which takes a different approach for self-supervised learning of image representations. The basis of SimCLR is image augmentations. Image augmentation has long been used in supervised learning. The augmentations are transformations applied to the image and cropping, color change, and rotation. The idea is that these transformations do not change the content of the image and the network will learn to ignore and be invariant to these transformations. In supervised learning, data augmentation is used to just increase the size of the dataset for a supervised task like classification. Many SSL methods including SimCLR make invariance to the augmentation the actual learning objective. The augmented images are fed into an encoder to get the representation. These representations are then learned to be close of augmentations of the same image.</p> <p>However, the problem with just comparing within the same image is collapse. The network would learn the trivial solution of a constant vector for all representations (ex: a vector of all zeros). This would maximize the similarity between augmentations but obviously not contain any useful images. We need negative samples to minimize similarity with. In SimCLR, the negative samples are augmentations of other images from the same training batch. The assumption made here is that the other images are unrelated to the current image and the representations should be far apart.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/simclr_arch-480.webp 480w,/assets/img/blog/self_supervised_learning/simclr_arch-800.webp 800w,/assets/img/blog/self_supervised_learning/simclr_arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/simclr_arch.png" width="500" height="auto" alt="SimCLR architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> The architecture of SimCLR. Diagram by Author, but dog images from SimCLR paper </figcaption> </figure> <p>Why do we need a projection head? It would not change the architecture much by optimizing the similarity losses on the representations themselves. The encoder may even include the same fully connected layers that would have been in the projection head. The projection head allows for a more complex and nonlinear similarity relationship between the encodings. Without it, the representations would have to have a high cosine similarity. This may restrict the expressivity of the vectors. The projection head can also ignore some information in the representations. For example, SimCLR may train to make the representations invariant to rotations. The rotation angle may be encoded in the representation but ignored by the projection head. If the rotation is encoded in the first 5 values of the vector, the projection MLP may have zero weights for those values. This may be desirable in a variant of the architecture in which the self-supervised learning happens simultaneously with a downstream task. The SimCLR architecture itself has no reason to include unnecessary information in the representation. It is unclear whether having “extra” information in the representation is desirable or not.</p> <p>Projection heads are very common in self-supervised learning. The autoregressive model in CPC can be viewed as a projection head.</p> <p>Aggressive augmentation yields the best results. This means applying multiple augmentations at a time. This makes the contrastive learning more challenging and forces the network to learn more about the image. The augmentations also avoid trivial solution to the contrastive objective. Without cropping, the network can match two augmented images by their local features (edges in the same place), instead of learning global features. Without color distortion, images can be matched by their color distribution. These augmentations can be composed with others, such as rotation and blur.</p> \[\begin{equation} \ell_{i,j} = \log{\frac{\exp(\mathrm{sim}(z_i,z_j)/\tau))}{\sum_{k=1}^{2N}\mathbb{1}_{[k\neq i]}\exp(\mathrm{sim}(z_i,z_k)/\tau)}} \end{equation}\] <p>The loss is referred to as NT-Xent (the normalized temperature-scaled cross entropy loss). The similarity function \(s\) can simply be cosine similarity (\(\frac{u^\top v}{\|u\|\|v\|}\)). This loss is similar to the InfoNCE loss. The main difference is the temperature \(\tau\). The temperature essentially controls how strongly should attract and repel the other vectors in the loss.</p> <h2 id="scaling">Scaling</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/scaling-480.webp 480w,/assets/img/blog/self_supervised_learning/scaling-800.webp 800w,/assets/img/blog/self_supervised_learning/scaling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/scaling.png" width="500" height="auto" alt="SSL scaling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2002.05709"> Source </a> </figcaption> </figure> <p>Self-supervised learning is often evaluated on ImageNet classification. The projection head is replaced with a linear layer. The network is then trained to classify ImageNet with the encoder weights frozen. The encodings learned with self-supervision must be useful enough for a linear layer to classify them.</p> <p>An interesting property of self-supervised trained encoders, is how the scale in terms of depth and width. We see that only SimCLR(4x) is able to match the accuracy of fully supervised learning. “4x” means the network is 4 times as wide and as deep. It is not necessarily a bad thing that SSL requires a much larger network for ImageNet classification. This likely means the network is learning more information from the data than what is needed for supervised learning. Although this doesn’t help with ImageNet classification, the vectors may be more effective in other downstream tasks.</p> <p>One issue with SimCLR is its reliance on huge batch sizes. The best results come from a batch size of 4096. It needs many negative samples to be effective. This makes the network inefficient to train. Other approaches attempt to address this problem.</p> <h2 id="byol"><a href="https://arxiv.org/abs/2006.07733">BYOL</a></h2> <p>BYOL is a paper from Deepmind that aims to remove the need for negative samples. There are two networks: a target network and an online network. The target network’s weights are an exponential moving average of the online encoder. Similar to SimCLR, augmented versions of an image are passed through the encoders. Unlike SimCLR, the loss does not use negative examples so there is no need for large batch sizes. There is a projection head on top of the online encoder. The online encoder is used for downstream tasks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/byol-480.webp 480w,/assets/img/blog/self_supervised_learning/byol-800.webp 800w,/assets/img/blog/self_supervised_learning/byol-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/byol.png" width="100%" height="auto" alt="BYOL architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> BYOL architecture <a href="https://arxiv.org/abs/2006.07733"> Source </a> </figcaption> </figure> <p>Bootstrapping is a poorly defined word used in machine learning. It can mean simultaneously optimizing two objectives that depend on each. In BYOL, that refers to the two encoders.</p> <p>BYOL is able to learn useful representations without collapse because only the parameters of the online encoder are optimized. The online encoder can’t learn to output a constant because it is following the representations of the target encoder. The bootstrapping ensures that the trivial solution is avoided.</p> <p>BYOL is a non-contrastive method of SSL. However one criticism of BYOL is that batch normalization causes implicit contrastive learning by leaking information between batch elements. However, in a <a href="https://arxiv.org/abs/2010.10241">follow up paper</a>, the authors show that replacing batch normalization with group normalization and weight standardization leads to comparable performance.</p> <h1 id="clustering">Clustering</h1> <p>Clustering is an important class of unsupervised learning algorithms. Although more often used outside of deep learning, clustering can be applied to self supervised learning. Feature vectors can be clustered. Clusters can indicate a group of related images. In this sense clusters are similar to classes and can be used as labels in SSL.</p> <h2 id="deepcluster"><a href="https://arxiv.org/abs/1807.05520">DeepCluster</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/deepcluster-480.webp 480w,/assets/img/blog/self_supervised_learning/deepcluster-800.webp 800w,/assets/img/blog/self_supervised_learning/deepcluster-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/deepcluster.png" width="100%" height="auto" alt="DeepCluster Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> DeepCluster architecture <a href="https://arxiv.org/abs/1807.05520"> Source </a> </figcaption> </figure> <p>DeepCluster trains a neural network in two alternating steps: clustering and classification. In the clustering step, each image is assigned a cluster as a pseudolabel by clustering the feature vectors from the network. K-means is used for clustering. There are \(k\) clusters of the same dimension as the feature vectors. The network is then trained to predict the clusters from the images. After training on this classification objective, the features improve. The dataset is reclustered with better clusters. This iterative training procedure improves the clusters and the representations.</p> <p>The main problem with DeepCluster is that it requires periodically clustering the entire dataset. This limits this method in scaling to extremely large datasets. This is addressed by SwAV with an online approach to clustering based SSL.</p> <h2 id="swav"><a href="https://arxiv.org/abs/2006.09882">SwAV</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/swav-480.webp 480w,/assets/img/blog/self_supervised_learning/swav-800.webp 800w,/assets/img/blog/self_supervised_learning/swav-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/self_supervised_learning/swav.png" width="500" height="auto" alt="SwAV" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> SwAV <a href="https://arxiv.org/abs/2006.09882"> Source </a> </figcaption> </figure> <p>SwAV extends on DeepCluster to be online, while also taking inspiration from contrastive SSL methods. Two augmentations of an image are passed to an encoder. These representations are then assigned prototypes. There are K prototypes, which are vectors of the same representation as the encoding.</p> <h1 id="conclusion">Conclusion</h1> <p>There are many approaches to self supervised learning, however there are common elements. There are contrastive losses, data augmentation, bootstrapping, projection heads, and sometimes negative samples.</p>]]></content><author><name></name></author><category term="computer-vision"/><category term="self-supervised-learning"/><summary type="html"><![CDATA[Yann LeCun describes self-supervised learning as the next big challenge in the field of AI. How does it work? Self-supervised learning (SSL) is a specific type of unsupervised learning. It aims to learn from large datasets of unlabeled data to enable building more robust models in different domains such as vision and NLP.]]></summary></entry><entry><title type="html">Domain Adaptation</title><link href="https://rohitbandaru.github.io/blog/Domain-Adaptation/" rel="alternate" type="text/html" title="Domain Adaptation"/><published>2021-08-09T00:00:00+00:00</published><updated>2021-08-09T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Domain-Adaptation</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Domain-Adaptation/"><![CDATA[<p>Machine learning performance depends on the dataset that it is trained on. Datasets are imperfect, so problems in the data affect the models. One type of problem is domain shift. This means that a model trained to learn a task on one dataset, may not be able to perform the same task on a slightly different dataset.</p> <p>Say you train a model to detect dogs in outdoor settings like public parks. It may perform very well on test images of dogs in outdoor places. However, that model may not function well when trying to detect dogs indoors, although the task itself is identical. This is a problem because the background of the image should not matter since you are just trying to detect dogs. We will explore four different research papers that address this problem.</p> <h1 id="vocabulary">Vocabulary</h1> <p>There are two datasets: a source dataset and a target dataset. The dataset that the model is trained on is the source dataset. The target dataset is the one that it will be tested on.</p> <p>For domain generalization, a similar problem, the target dataset is not available during training. The network is trained on the source dataset to not overfit to the domain-specific features.</p> <p>In domain adaptation, both the source and target datasets are available during training, but labels for the target dataset are not always available. For unsupervised domain adaptation, there are no labels available for the target dataset during training time. Semi-supervised domain adaptation involves a few labeled examples from the target dataset. With supervised domain adaptation, all the data from both the source and target datasets have labels.</p> <p>Unsupervised domain adaptation is the most commonly studied problem, as it has the most applications. Supervised DA can be useful when you have a labeled dataset, but it is too small to directly train on.</p> <p>These methods can be applied to many ML problems. However, a common application is image classification. I will focus on image classification on two common benchmark datasets: MNIST and SVHN. A model trained on handwritten digits (MNIST) often performs poorly on printed house number digits (SVHN).</p> <h1 id="adversarial-methods">Adversarial Methods</h1> <p>The most common approaches to the domain adaptation method follow an adversarial approach. For some context, I would suggest reading about <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29">Generative Adversarial Networks (GANs)</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/framework-480.webp 480w,/assets/img/blog/domain_adaptation/framework-800.webp 800w,/assets/img/blog/domain_adaptation/framework-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/framework.png" width="100%" height="auto" alt="Framework for Adversarial Domain Adaptation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Framework for Adversarial Domain Adaptation </figcaption> </figure> <p>There are two encoders, which learn to produce a vector representation of each input. There is a classifier to classify the inputs and a discriminator that is trained to differentiate between the datasets. The goal is to eliminate differences in the domain from the encodings. This is similar to the GAN objective in that we want the encoders to fool the discriminator by generating encodings that are difficult to differentiate. However, this needs to be done such that the classifier is also effective for both datasets. The same classifier can then be applied to both datasets.</p> <p>There are many approaches to this with different training methods, architectures, and losses. The high-level goal is consistent. We want the encoders to generate encodings that contain the useful information needed for classification but remove the shift in domains.</p> <p>The key difference between the many algorithms is what the discriminator is and how it is trained. In simple cases, it is just an additional loss term. For example, maximum mean discrepancy (MMD) measures the difference between the encodings of the source and target datasets. Training the networks while minimizing the discrepancy can reduce domain shift. This may be useful for simple DA problems but does not work well for larger disparities.</p> <h2 id="adda"><a href="https://arxiv.org/abs/1702.05464">ADDA</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/adda-480.webp 480w,/assets/img/blog/domain_adaptation/adda-800.webp 800w,/assets/img/blog/domain_adaptation/adda-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/adda.png" width="100%" height="auto" alt="The steps of ADDA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> The steps of ADDA <a href="https://arxiv.org/abs/1702.05464"> Source </a> </figcaption> </figure> <p>Adversarial Discriminative Domain Adaptation (ADDA) applies a simple approach to discriminative DA. There is only one encoder shared between the source and target datasets. The networks are trained in two steps.</p> <ol> <li> <p>The encoder and classifier are first trained to achieve high classification accuracy on the source dataset.</p> </li> <li> <p>The encoder is trained with the discriminator to lose domain discriminability. The discriminator is trained to classify the two domains with an adversarial loss. The encoder is trained with the negation of this loss since it is adversarial with respect to the discriminator. This negative is done through gradient reversal, which means in backpropagation, the gradients are negated before going to the encoder.</p> </li> </ol> <p>One major shortcoming of this approach is that the classification performance can be lost or forgotten in the adaptation step. This is because the labels are not used in this step.</p> <h2 id="dann"><a href="https://arxiv.org/abs/1505.07818">DANN</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/dann-480.webp 480w,/assets/img/blog/domain_adaptation/dann-800.webp 800w,/assets/img/blog/domain_adaptation/dann-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/dann.png" width="100%" height="auto" alt="DANN" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> DANN <a href="https://arxiv.org/abs/1505.07818"> Source </a> </figcaption> </figure> <p>Domain-Adversarial Training of Neural Networks (DANN) is very similar to ADDA. Rather than have a separate adaptation step, the domain discriminator is trained alongside the classier. A gradient reversal layer is used because the domain discriminator and the classier have adversarial loss functions. This allows classification and discrimination to be trained together and avoid the network from forgetting the task.</p> <h1 id="image-translation">Image Translation</h1> <p>Another approach to addressing the domain gap is to convert examples from one domain to another. An example of this is transforming street-view digits (SVHN) to look like handwritten MNIST (digits). After this translation, you can apply an MNIST trained image classifier. The architectures are more complex because, in addition to the main task (image classification), the networks must translate images to and from the source and target domains.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/translation-480.webp 480w,/assets/img/blog/domain_adaptation/translation-800.webp 800w,/assets/img/blog/domain_adaptation/translation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/translation.png" width="100%" height="auto" alt="SVHN DA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="image-to-image-translation"><a href="https://arxiv.org/abs/1712.00479">Image to Image translation</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/i2i-480.webp 480w,/assets/img/blog/domain_adaptation/i2i-800.webp 800w,/assets/img/blog/domain_adaptation/i2i-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/i2i.png" width="100%" height="auto" alt="I2I" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1712.00479"> Source </a> </figcaption> </figure> <p>Like the adversarial methods Image to Image Translation (I2I) aims to learn a domain invariant encoding (Z) for the images. There are six networks in this architecture: the source encoder, source decoder, target encoder, target decoder, domain discriminator, and task network (ex: classifier). The decoders aim to reconstruct the images from the encoding. This also includes adversarial learning with the domain discriminator.</p> <p>The network is trained on a weighted combination of six different losses. The paper studies which combination of losses yields the best performance.</p> <ol> <li> <p>Qc is the classification loss on the source domain. We cannot get this loss for the target domain since there are no labels. However, the loss can be extended to include the target domain if labels exist.</p> </li> <li> <p>Qid is the loss of encoding an image and decoding it back into the same domain. Encoding an image into Z and decoding it back to the original domain should ideally return the same image. This loss can be the L1 norm of the difference between the original and decoded image.</p> </li> <li> <p>Qz is the domain discriminator’s loss. This is similar to ADDA in that it is trying to determine the domain of the encoding. We want this loss to increase as the encodings improve.</p> </li> <li> <p>Qtr is another discrimination loss in which the image is translated into the other domain before going to the domain discriminator.</p> </li> <li> <p>Qcyc is the cycle consistency loss. This loss is similar to Qid. The difference is that the images are decoded in the other domain before being encoding and decoded in the original domain. The image from the source domain is encoded into Z. This is decoded into the target domain and encoded back to Z. This is then decoded into the source domain and compared with the original image. A loss with the source and target switched is also applied. This aims to ensure encodings from similar images in different domains have similar encodings.</p> </li> <li> <p>Qtrc is similar to Qcyc, but instead of decoding back into the original domain, the encoding is classified. Unlike Qcyc, this is not symmetric since it involves labels. An image from the source domain is translated into the target domain and then classified.</p> </li> </ol> <h2 id="cycada"><a href="https://arxiv.org/abs/1711.03213">CyCADA</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/cycada-480.webp 480w,/assets/img/blog/domain_adaptation/cycada-800.webp 800w,/assets/img/blog/domain_adaptation/cycada-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/domain_adaptation/cycada.png" width="100%" height="auto" alt="CyCADA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1711.03213"> Source </a> </figcaption> </figure> <p>CyCADA is similar to I2I. Many of the I2I losses and networks have counterparts here. The main difference is that the target images are not translated to the source domain. Also, the GAN losses can be applied to both the images and the encodings.</p> <p>The source images are translated into the target domain. They are translated back into the source domain to apply the cycle consistency loss (L1 difference with the original image).</p> <p>The fs network is trained on the supervised learning task in the source domain. The semantic consistency loss ensures that the features from this network remain close before and after translation into the target domain. This ensures that the images retain the semantic information after translation.</p> <p>A GAN loss is then applied to the images and features (from fT) for the translated images and the target images. This loss is needed to train the translations to be similar to the target domain. There are two GAN losses to ensure that both the images and the features are similar.</p> <p>Finally, a task loss is applied to the translated images. This applies the task to the original target images.</p> <h1 id="other-domains">Other Domains</h1> <p>Image classification is the primary problem used to benchmark domain adaptation methods. However, domain adaptation can also be applied to other computer vision problems, such as image segmentation. It can also be applied in different research areas, such as natural language processing (NLP).</p> <p>One particularly interesting application of domain adaptation is self-driving cars and robotics. It is a common practice to train deep neural networks for these applications using data from simulated environments. It is much easier to collect large amounts of data in a simulation rather than in the real world. However, in order for a model trained on simulation data to function in a real-world environment, domain adaptation is often required to achieve good performance.</p> <p>There are also many variants to the problem, including few-shot domain adaptation, domain generalization, and multiclass domain adaptation.</p> <h1 id="conclusion">Conclusion</h1> <p>There are several approaches to domain adaptation but they often share some common characteristics. Adversarial learning with a domain discrimination network is common. There is also a lot of work using image to image translation with a cycle consistency loss. Apply domain adaptation to new problems will likely involve some combination of these components.</p> <h1 id="references">References</h1> <p>[1] Long, Mingsheng, et al. “Learning transferable features with deep adaptation networks.” International conference on machine learning. PMLR, 2015.</p> <p>[2] Eric Tzeng et al. “Adversarial discriminative domain adaptation”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017, pp. 7167-7176.</p> <p>[3] Yaroslav Ganin and Victor Lempitsky. “Unsupervised domain adaptation by backpropagation”. In: arXiv preprint arXiv:1409.7495 (2014).</p> <p>[4] Zak Murez et al. “Image to image translation for domain adaptation”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018, pp. 4500-4509.</p> <p>[5] Hoffman, Judy, et al. “Cycada: Cycle-consistent adversarial domain adaptation”. In: arXiv preprint arXiv:1711.03213 (2017).</p>]]></content><author><name></name></author><category term="computer-vision"/><summary type="html"><![CDATA[Machine learning performance depends on the dataset that it is trained on. Datasets are imperfect, so problems in the data affect the models. One type of problem is domain shift. This means that a model trained to learn a task on one dataset, may not be able to perform the same task on a slightly different dataset.]]></summary></entry><entry><title type="html">Pruning Neural Networks</title><link href="https://rohitbandaru.github.io/blog/Neural-Network-Pruning/" rel="alternate" type="text/html" title="Pruning Neural Networks"/><published>2020-09-01T00:00:00+00:00</published><updated>2020-09-01T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/Neural-Network-Pruning</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/Neural-Network-Pruning/"><![CDATA[<p>Much of the success of deep learning has come from building larger and larger neural networks. This allows these models to perform better on various tasks, but also makes them more expensive to use. Larger models take more storage space which makes them harder to distribute. Larger models also take more time to run and can require more expensive hardware. This is especially a concern if you are productionizing a model for a real-world application.</p> <p>Model compression aims to reduce the size of models while minimizing loss in accuracy or performance. Neural network pruning is a method of compression that involves removing weights from a trained model. In agriculture, pruning is cutting off unnecessary branches or stems of a plant. In machine learning, pruning is removing unnecessary neurons or weights. We will go over some basic concepts and methods of neural network pruning.</p> <h1 id="remove-weights-or-neurons">Remove weights or neurons?</h1> <p>There are different ways to prune a neural network.</p> <ol> <li>You can prune weights.</li> </ol> <p>This is done by setting individual parameters to zero and making the network sparse. This would lower the number of parameters in the model while keeping the architecture the same.</p> <ol> <li>You can remove entire nodes from the network.</li> </ol> <p>This would make the network architecture itself smaller, while aiming to keep the accuracy of the initial larger network.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/pruning/weights_vs_neurons-480.webp 480w,/assets/img/blog/pruning/weights_vs_neurons-800.webp 800w,/assets/img/blog/pruning/weights_vs_neurons-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pruning/weights_vs_neurons.png" width="100%" height="auto" alt="pruning weights vs nodes" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Visualization of pruning weights/synapses vs nodes/neurons <a href="https://arxiv.org/abs/1506.02626"> Source </a> </figcaption> </figure> <p>Weight-based pruning is more popular as it is easier to do without hurting the performance of the network. However, it requires sparse computations to be effective. This requires hardware support and a certain amount of sparsity to be efficient. Pruning nodes will allow dense computation which is more optimized. This allows the network to be run normally without sparse computation. This dense computation is more often better supported on hardware. However, removing entire neurons can more easily hurt the accuracy of the neural network.</p> <h1 id="what-to-prune">What to prune?</h1> <p>A major challenge in pruning is determining what to prune. If you are removing weights or nodes from a model, you want the parameters you remove to be less useful. There are different heuristics and methods of determining which nodes are less important and can be removed with minimal effect on accuracy. You can use heuristics based on the weights or activations of a neuron to determine how important it is for the model’s performance. The goal is to remove more of the less important parameters.</p> <p>One of the simplest ways to prune is based on the magnitude of the weight. Removing a weight is essentially setting it to zero. You can minimize the effect on the network by removing weights that are already close to zero, meaning low in magnitude. This can be implemented by removing all weights below a certain threshold. To prune a neuron based on weight magnitude you can use the L2 norm of the neuron’s weights.</p> <p>Rather than just weights, activations on training data can be used as a criteria for pruning. When running a dataset through a network, certain statistics of the activations can be observed. You may observe that some neurons always outputs near-zero values. Those neurons can likely be removed with little impact on the model. The intuition is that if a neuron rarely activates with a high value, then it is rarely used in the model’s task.</p> <p>In addition to the magnitude of weights or activations, redundancy of parameters can mean a neuron can be removed. If two neurons in a layer have very similar weights or activations, it can mean they are doing the same thing. By this intuition, we can remove one of the neurons and preserve the same functionality.</p> <p>Ideally in a neural network, all the neurons have unique parameters and output activations that are significant in magnitude and not redundant. We want all the neurons are doing something unique, and remove those that are not.</p> <h1 id="when-to-prune">When to prune?</h1> <p>A major consideration in pruning is where to put it in the training/testing machine learning timeline. If you are using a weight magnitude-based pruning approach, as described in the previous section, you would want to prune after training. However, after pruning, you may observe that the model performance has suffered. This can be fixed by fine-tuning, meaning retraining the model after pruning to restore accuracy.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/pruning/pruning_flow-480.webp 480w,/assets/img/blog/pruning/pruning_flow-800.webp 800w,/assets/img/blog/pruning/pruning_flow-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pruning/pruning_flow.png" width="300" height="auto" alt="flow of iterative pruning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1611.06440"> Source </a> </figcaption> </figure> <p>The usage of pruning can change depending on the application and methods used. Sometimes fine-tuning or multiple iterations of pruning are not necessary. This depends on how much of the network is pruned.</p> <h1 id="how-to-evaluate-pruning">How to evaluate pruning?</h1> <p>There multiple metrics to consider when evaluating a pruning method: accuracy, size, and computation time. Accuracy is needed to determine how the model performs on its task. Model size is how much bytes of storage the model takes. To determine computation time, you can use FLOPs (Floating point operations) as a metric. This is more consistent to measure than inference time and it does not depend on what system the model runs on.</p> <p>With pruning, there is a tradeoff between model performance and efficiency. You can prune heavily and have a smaller more efficient network, but also less accurate. Or you could prune lightly and have a highly performant network, that is also large and expensive to operate. This trade-off needs to be considered for different applications of the neural network.</p> <h1 id="conclusion">Conclusion</h1> <p>Pruning is an effective method of making neural networks more efficient. There are plenty of choices and areas of research in this area. We want to continue to make advances in deep learning while also keeping our models energy, time, and space-efficient.</p> <h1 id="references">References</h1> <p>[1] Blalock, Davis, et al. “What is the state of neural network pruning?.” arXiv preprint arXiv:2003.03033 (2020).</p> <p>[2] Han, Song, et al. “Learning both weights and connections for efficient neural network.” Advances in neural information processing systems. 2015.</p> <p>[3] PyTorch Pruning Tutorial <a href="https://pytorch.org/tutorials/intermediate/pruning_tutorial.html">https://pytorch.org/tutorials/intermediate/pruning_tutorial.html</a></p> <p>[4] Keras / Tensorflow Pruning Tutorial <a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras">https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras</a></p> <p>[5] Molchanov, Pavlo, et al. “Pruning convolutional neural networks for resource efficient inference.” arXiv preprint arXiv:1611.06440 (2016).</p> <p>[6] Babaeizadeh, Mohammad, Paris Smaragdis, and Roy H. Campbell. “Noiseout: A simple way to prune neural networks.” arXiv preprint arXiv:1611.06211 (2016).</p>]]></content><author><name></name></author><category term="applied-ml"/><summary type="html"><![CDATA[Much of the success of deep learning has come from building larger and larger neural networks. This allows these models to perform better on various tasks, but also makes them more expensive to use. Larger models take more storage space which makes them harder to distribute. Larger models also take more time to run and can require more expensive hardware. This is especially a concern if you are productionizing a model for a real-world application.]]></summary></entry></feed>