<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rohitbandaru.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rohitbandaru.github.io/blog/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-04T05:15:50+00:00</updated><id>https://rohitbandaru.github.io/blog/feed.xml</id><title type="html">blank</title><subtitle>ML blog. </subtitle><entry><title type="html">SSL with Vision Transformers</title><link href="https://rohitbandaru.github.io/blog/blog/2024/SSL-with-Vision-Transformers/" rel="alternate" type="text/html" title="SSL with Vision Transformers"/><published>2024-08-01T00:00:00+00:00</published><updated>2024-08-01T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/blog/2024/SSL-with-Vision-Transformers</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/blog/2024/SSL-with-Vision-Transformers/"><![CDATA[<p>In recent years, self-supervised learning (SSL) has emerged as a powerful paradigm in computer vision, allowing models to learn meaningful representations from unlabeled data. The prior work in this field focuses on using CNN architectures such as ResNet on this task. However, as evidenced by the success of self supervised language models, transformers are a natural fit for self supervised training. We will cover a set of recent papers that apply transformers for self supervised visual learning.</p> <p>One key variation is that you often see masking in these methods. CNN based SSL methods rely more on data augmentations to create a prediction task for the model. Masking is advantageous for several reasons outlined below, and it also aligns more with language model training (example: BERT).</p> <ul> <li>Computational efficiency <ul> <li>You do not have to process the masked regions of the image. When a large portion of the image is masked</li> </ul> </li> <li>Data augmentations can introduce unwanted invariances and remove useful information <ul> <li>For example, a data augmentation that strongly distorts the color, may result in representations that do not encode color</li> </ul> </li> </ul> <p>Masking is more naturally enabled by the transformer architecture. There is a reason that masking based SSL training hasn’t worked well with CNNs.</p> <p>By examining these different methods, we’ll discuss what makes transformers work for vision.</p> <h1 id="dino"><a href="https://arxiv.org/abs/2104.14294"><strong>DINO</strong></a></h1> <p>This paper (Emerging Properties in Self-Supervised Vision Transformers) by Caron et al. introduces a new self-supervised training method called DINO, which they apply to vision transformers. They argue that transformers are better than CNNs for images with SSL training, more so than with supervised training. Transformers can match the performance of CNNs with supervised training, albeit with more training cost. However, they have more useful properties with SSL training. This follows our intuition that SSL and transformers are a natural combination.</p> <p>DINO takes inspiration from <a href="https://arxiv.org/abs/2006.07733">BYOL</a> but introduces two key innovations:</p> <ol> <li>A novel loss function that enables direct matching between student and teacher outputs</li> <li>Elimination of the prediction layer on the student, simplifying the architecture</li> </ol> <p>These changes result in a self-distillation approach that proves particularly effective with vision transformers.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/ssl-vit/dino-480.webp 480w,/blog/assets/img/blog/ssl-vit/dino-800.webp 800w,/blog/assets/img/blog/ssl-vit/dino-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/ssl-vit/dino" class="img-fluid mx-auto d-block" width="400" height="auto" alt="DINO architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li>Two views of an image \(x\), \(x_1\) and \(x_2\) are generated through data augmentations. <ol> <li>A multi crop strategy is used in which two large global views are generated along with a set of smaller cropped local views. The teacher only processes global views, while the student processes all views, with the constraint that the loss is not trying to match the same views to each other. This method was introduced in the <a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper_files/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf&amp;hl=en&amp;sa=T&amp;oi=gsr-r-gga&amp;ct=res&amp;cd=0&amp;d=13209348926291080860&amp;ei=QYYkZu2RB5SCy9YP29Cc0AY&amp;scisig=AFWwaea44-zuGhikZl27njOvnygp">SwAV</a> paper, and helps the model learn local to global correspondences. Restricting the teacher to only global views also encourages the encoders to output global representations.</li> <li>Are position embeddings used?</li> </ol> </li> <li>The views are passed to their respective encoder (teacher/student)</li> <li>The teacher encoding is “centered”. <ol> <li>Perhaps centering allows this method to work without having the predictor layer. The center is a exponential moving average of the teacher encoding (of both views). This vector is subtracted from the teacher’s encoding before the softmax. A temperature is also applied with the softmax to achieve a “sharpening”. These methods help the teacher avoid collapse. Centering ensures that a single component of the vector doesn’t dominate. Sharpening ensures that it doesn’t collapse to a uniform vector.</li> </ol> </li> <li>Softmax is applied to each encoding. The student is trained with a cross entropy loss to match the teacher. The teachers weights are updated as an exponential moving average of the student.</li> </ol> <p>This paper compares the performance of DINO with ResNet and ViT architectures against <a href="https://rohitbandaru.github.io/blog/blog/2024/SSL-with-Vision-Transformers/">SOTA SSL methods</a> such as <a href="https://arxiv.org/abs/2006.07733">BYOL</a>, MoCov2, and SwAV. The combination os DINO and ViT has the most significant advantage. Interestingly, it is 6.6% better than ViT with BYOL training on linear ImageNet evaluation, despite minor differences in the methods. The SSL methods that are used for comparison were developed for CNN architectures, which put them at a disadvantage. DINO is designed for transformers, but what about it makes it work better with transformers? One possible explanation is that transformers handle different resolutions of images better. Higher resolution images results in more image patches generated in the transformer. The computation also scales quadratically in the attention operations with respect to the number of patches. For ResNet, the computation increases linearly.</p> <p>The two main “emerging properties” they observe is that DINO ViT features are useful for dense predictions such as semantic segmentation. Another property is that k nearest neighbors on the output encodings, without any finetuning. This enables image retrieval applications.</p> <p>They observe the teacher outperforms the student in DINO training. This is not observed with other SSL methods. They cite “Polyak-Ruppert averaging” as an explantation of this. This means the teacher simulates an ensemble model with its momentum weights.</p> <p>The multi-crop strategy enforces that the inputs be rectangular. This makes this method compatible with CNNs in addition to ViTs. DINO shows that SSL is effective with vision transformers. However, it is designed in a way that makes the training method compatible with CNNs. This leads to some very interesting comparisons between the properties of SSL CNN and ViT models. The other works we will discuss take advantage of the flexibility of the transformer architecture, at the cost of CNN compatibility.</p> <p><a href="https://arxiv.org/pdf/2304.07193.pdf">DINOv2: Learning Robust Visual Features without Supervision</a> scales DINO using a 1 billion parameter ViT model along with a larger proprietary dataset. They used an interesting data processing pipeline to combine curated and uncurated data, to get a large dataset of high quality and diverse images. This step is important because unprocessed uncurated data can be of low quality and dominated by certain modes of data and duplicated data.</p> <p>There are several architectural and training changes applied on top DINO v1 that allow it to scale effectively. Notably, in addition to DINO, they add an <a href="https://arxiv.org/abs/2111.07832">iBOT</a> loss. This method masks some of the input tokens of the student. In order to combine DINO and iBOT losses, they learn separate heads on the student and teacher for each loss. iBOT does BERT style pretraining of image transformers, which we will also cover in this post.</p> <h1 id="data2vec"><a href="https://arxiv.org/abs/2202.03555">data2vec</a></h1> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/ssl-vit/data2vec-480.webp 480w,/blog/assets/img/blog/ssl-vit/data2vec-800.webp 800w,/blog/assets/img/blog/ssl-vit/data2vec-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/ssl-vit/data2vec.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="data2vec architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The teacher model predicts representations from unmasked input, while the student model predicts representations from masked input. The student aims to match the teacher’s output by predicting the representations of the masked tokens. To avoid collapse, the teacher’s weights are an exponential moving average of the student’s weights.</p> <p>Instead of training a multimodal model, independent models are trained on different modalities. Data2VecAudio, Data2VecText, and Data2VecVision are developed. The learning objective remains the same, but the generation of embeddings and masking strategies differ.</p> <ol> <li>Encoding of inputs into embeddings: <ol> <li>Text is tokenized, and learned embeddings for each token are retrieved.</li> <li>Images are divided into 16x16 patches and linearly projected into an embedding.</li> <li>Audio is encoded by a 1D convolutional neural network with multiple layers. A 16 kHz waveform is mapped to a 50 Hz representation. This means a sequence of 320 integers is mapped to a single representation. <ol> <li>Unlike images, a multiple-layer network is used for audio, likely due to the absence of a Fourier transform.</li> </ol> </li> </ol> </li> <li>Masking: <ol> <li>Some of the student input embeddings are replaced by the MASK token embedding. <ol> <li>Text: Random tokens are masked.</li> <li>Images: Embeddings corresponding to rectangular blocks are masked.</li> <li>Audio: Continuous spans of embeddings are masked.</li> </ol> </li> </ol> </li> <li>Addition of position encoding.</li> <li>Both the teacher and student transformer models receive the input.</li> <li>Representations at different layers are distilled from the teacher to the student. Outputs from the masked tokens of the top \(K\) transformer blocks are normalized and averaged into a single vector.</li> <li>A regression loss (Smooth L1) is applied to the averaged vectors of each network. <ol> <li>The loss transitions from a squared loss to an L2 loss when the error margin goes below the hyperparameter \(\beta\). The L2 loss is only applied when the student and teacher predictions are close. This loss is designed to be less sensitive to outliers.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/ssl-vit/data2vec_loss-480.webp 480w,/blog/assets/img/blog/ssl-vit/data2vec_loss-800.webp 800w,/blog/assets/img/blog/ssl-vit/data2vec_loss-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/ssl-vit/data2vec_loss" class="mx-auto d-block" width="500" height="auto" alt="data2vec loss" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li>The students weights are updated with SGD. The teacher’s weights are updated as a EMA of the students weights: \(\Delta \leftarrow \tau \Delta + (1-\tau)\theta\) <ol> <li>\(\Delta\) represents the teacher’s parameters, while \(\theta\) represents the student’s parameters.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/ssl-vit/data2vec_architecture-480.webp 480w,/blog/assets/img/blog/ssl-vit/data2vec_architecture-800.webp 800w,/blog/assets/img/blog/ssl-vit/data2vec_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/ssl-vit/data2vec_architecture.png" class="mx-auto d-block" width="500" height="auto" alt="data2vec architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The position encoding and feature encoder weights are shared between the two models. However, the teacher’s transformer weights are specified through an exponential moving average.</p> <p><a href="https://ai.meta.com/research/publications/efficient-self-supervised-learning-with-contextualized-target-representations-for-vision-speech-and-language/"><strong>data2vec 2.0</strong></a></p> <p>Data2Vec 2.0 introduces several architectural and loss function changes that lead to a significant speed up in training.</p> <p>They use target representations for multiple masked predictions of a sample. This is more computationally efficient because we only need to run the teacher model once to train with \(M\) different masks of the input instead of 1. Further efficiency gains are implemented through not processing the masked parts of the image with the student, and sharing the feature encoder output across all masks.</p> <p>They use a L2 loss instead of a smooth L1 loss. This is a simplification of the earlier loss. They also use a convolutional decoder to predict the masked representations rather than a transformer.</p> <p>They also introduce inverse block masking. Rather than masking blocks. Blocks are chosen to be unmasked areas. The representations outside of the block will be predicted. There are multiple blocks which may overlap. A mask consists of multiple blocks. Training includes multiple masks for each target.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/ssl-vit/data2vec_1-480.webp 480w,/blog/assets/img/blog/ssl-vit/data2vec_1-800.webp 800w,/blog/assets/img/blog/ssl-vit/data2vec_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/ssl-vit/data2vec_1.png" class="mx-auto d-block" width="100%" height="auto" alt="data2vec 2.0" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>They also add a linear attention bias (<a href="https://arxiv.org/abs/2108.12409">ALiBi</a>). This essentially modifies self attention to increase the bias for query key pairs that are far apart. This enables faster training by providing an inductive bias.</p> <h1 id="masked-autoencoders-are-scalable-vision-learners"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper">Masked Autoencoders Are Scalable Vision Learners</a></h1> <p>This paper uses a simple autoencoder architecture to learn image representations. Parts of the images are masked, and the model is tasked to predict what is in the masked regions. This model can be trained through this <a href="https://github.com/ariG23498/mae-scalable-vision-learners/blob/master/mae-pretraining.ipynb">notebook</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/ssl-vit/mae-480.webp 480w,/blog/assets/img/blog/ssl-vit/mae-800.webp 800w,/blog/assets/img/blog/ssl-vit/mae-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/ssl-vit/mae.png" class="mx-auto d-block" width="100%" height="auto" alt="Masked Autoencoder" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li>The image is split into patches, as done in Vision Transformers.</li> <li>Using a mask ratio (75%-95%), patches are selected randomly without replacement.</li> <li>The unmasked patches are inputted into the encoder. Note that the mask tokens do not get processed by the encoder (difference from BERT). The encoder uses a vanilla ViT architecture, where the unmasked patches are linearly projected into token embeddings which get processed by transformer blocks. The output is a ViT processed embedding for each unmasked patch. Each patch has an added position embedding.</li> <li>The encoded tokens and the masked tokens are combined as an input to the decoder. The mask tokens map to a learned embedding. This embedding will be the same at all positions because it is not transformed by the encoder. At this stage position embeddings are added to the full set. <ol> <li>Note that for unmasked tokens, position embeddings are added twice, once before the encoder and once before the decoder.</li> </ol> </li> <li>The decoder reconstructs the unmasked image from the set of patch embeddings. The decoder is trained by a mean squared error loss with respect to the unmasked input image.</li> </ol> <p>This architecture builds on the vision transformer. An alternative is to use CNNs. This would involve directly setting pixels in the input image to zero, learn a vector representation, and then decode it back to the image. The reason this fails is that it aims to globally decode an image. With transformers you first predict representations of the masked patches, and then decode into the image patch. This breaks it down into two easier problems. Also with CNNs, you can’t explicitly encode masked regions like you can with a ViT. Having a mask token more explicitly indicates the mask.</p> <p>They mask a very high percentage of patches (80%). This reduces spatial redundancy and forces the model to learn more higher level and useful features. With a lower mask ratio, the model might learn to represent small local changes, like color and lighting variation. It doesn’t need to understand the higher level structure of the image, because its mostly already there. This is notable change from language models. BERT masks 15% of tokens. MAE and related works mask a majority of the image 75%+.</p> <p>The model uses the ImageNet-1K dataset for pretraining and evaluation. Evaluation is done by either finetuning to full encoder model, or a linear probe (train one MLP layer on the output of the encoder) on the task of classification.</p> <p>One interesting result is that the performance of finetuning and linear probing has different trends when ablating the masking ratio. Linear probing accuracy increases linearly with masking ratio until 75%. Finetuning has relatively consistent performance between 40% and 80%.</p> <p>Having a deep decoder allows for the representations to be more abstract, because the decoder has more capacity for reconstruction. A shallower decoder would lead to the encoder having to represent more of the details needed for reconstruction. This is less relevant for finetuning that it is for linear probing, as during FT the encoder than shift from focusing on reconstruction to recognition. In my opinion linear probing results are more interesting since the goal is build useful representations that can be used for various tasks. Finetuning offers just a marginal improvement over just training on the classification task directly without pretraining at all. However linear probing discourages learning nonlinear features in the representation. To address this the authors evaluate “partial finetuning” in which the last few blocks of the transformer are finetuned.</p> <p>Excluding mask tokens from the input and using a lightweight decoder makes this model very efficient to train. Using mask tokens in the encoder also creates a domain shift between pretraining and downstream tasks which hurts performance. This is because a large portion of the pretraining input will be mask tokens, which is significantly different than what the model will see downstream.</p> <h1 id="beit-bert-pre-training-of-image-transformers"><a href="https://arxiv.org/abs/2106.08254"><strong>BEiT: BERT Pre-Training of Image Transformers</strong></a></h1> <p>This approach is most similar to BERT / NLP SSL models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/ssl-vit/beit-480.webp 480w,/blog/assets/img/blog/ssl-vit/beit-800.webp 800w,/blog/assets/img/blog/ssl-vit/beit-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/ssl-vit/beit.png" class="mx-auto d-block" width="100%" height="auto" alt="beit" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A fundamental difference in applying SSL to images compared to text is that images are continuous. Text has a finite number of tokens. You can use a softmax to get a probability distribution across all tokens. In ViTs, patches of an image are treated as tokens. However, you can’t get an explicit probability distribution over all possible image patches. BEiT addresses this problem by training a discrete variational autoencoder (dVAE) to learn discrete visual tokens. These discrete tokens are an approximation or compression of image patches.</p> <p>The main difference between this and a vanilla ViT architecture is the usage of discrete visual tokens.</p> <p>There are two step to training:</p> <ol> <li>Tokenizer and Decoder are trained as a VAE to learn discrete visual tokens</li> <li>The discrete tokens from the learned tokenizer are used to pretrain a BEiT encoder.</li> </ol> <p>Why aren’t the tokens used as the input directly? The softmax distribution of tokens could be used as a soft label for the BEIT encoder.</p> <p>The transformer training task is named as masked image modeling (MIM), as it is designed after BERT’s masked language modeling (MLM). 40% of the tokens are masked. Similar to other methods, BEIT masks a large portion of the image to make the pretraining task sufficiently difficult.</p> <h1 id="conclusion">Conclusion</h1> <p>The landscape of self-supervised learning for image processing is undergoing a significant transformation. While it originated with Convolutional Neural Networks (CNNs), a strong coupling with transformer-based architectures is emerging and may lead the way for further advancements.</p>]]></content><author><name></name></author><category term="self-supervised-learning"/><category term="transformer"/><summary type="html"><![CDATA[In recent years, self-supervised learning (SSL) has emerged as a powerful paradigm in computer vision, allowing models to learn meaningful representations from unlabeled data. The prior work in this field focuses on using CNN architectures such as ResNet on this task. However, as evidenced by the success of self supervised language models, transformers are a natural fit for self supervised training. We will cover a set of recent papers that apply transformers for self supervised visual learning.]]></summary></entry><entry><title type="html">Deep Dive into Yann LeCun’s JEPA</title><link href="https://rohitbandaru.github.io/blog/blog/2024/JEPA-Deep-Dive/" rel="alternate" type="text/html" title="Deep Dive into Yann LeCun’s JEPA"/><published>2024-07-31T00:00:00+00:00</published><updated>2024-07-31T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/blog/2024/JEPA-Deep-Dive</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/blog/2024/JEPA-Deep-Dive/"><![CDATA[<p>In the AI research community, Yann LeCun has a unique and often controversial perspective. As of 2024, LLMs and Generative AI are the main focus areas of the field of AI. We’ve all been impressed by the performance of LLMs in various contexts, and generative systems like OpenAI’s <a href="https://openai.com/sora">Sora</a>. However, it is not clear where these advances fit in the long term goal of achieving and surpassing human level intelligence, which many call AGI.</p> <p>In his position paper <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">A Path Towards Autonomous Machine Intelligence</a> and his many recent talks (linked below), Yann presents an alternative framework for achieving artificial intelligence. He also proposes a new architecture for a predictive world model: Joint Embedding Predictive Architecture (JEPA).</p> <p>This blog post will dive deep into Yann’s vision for AI, the JEPA architecture, current research, and energy based models. We will go deep into the technical aspects of these ideas, as well as give my opinions, along with interesting references. I will also cover recent research advances such as <em>V-JEPA</em></p> <p>This is a long post, feel free to jump to the sections about JEPA, I-JEPA, and V-JEPA.</p> <h3 id="relevant-talks-by-yann-lecun">Relevant Talks by Yann LeCun</h3> <p><a href="https://drive.google.com/file/d/1RVYBVi_bWyz-4sZSsu4rSWzDwQBLsvHL/view"><em>From Machine Learning to Autonomous Intelligence</em></a></p> <div class="video"> <figure> <iframe width="560" height="315" src="https://www.youtube.com/embed/VRzvpV9DZ8Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </figure> </div> <p><a href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf"><em>Objective-Driven AI: Towards Machines that can Learn, Reason, and Plan”</em></a></p> <div class="video"> <figure> <iframe width="560" height="315" src="https://www.youtube.com/embed/d_bdU3LsLzE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </figure> </div> <h1 id="problems-with-current-ai">Problems with Current AI</h1> <p>The JEPA architecture aims to address current AI challenges. To contextualize these issues, we’ll examine Yann LeCun’s criticisms of popular AI trends as of 2024.</p> <p>Recent years have seen tremendous excitement around Large Language Models (LLMs) and Generative AI. LLMs are pretrained using autoregressive self-supervised learning, predicting the next token given preceding ones. They’re trained on vast datasets of text and code from the internet and books, often fine-tuned with supervised learning or reinforcement learning. Generative AI broadly refers to creation of multimodal media from inputs, such as text-to-image generation.</p> <p>However, these models face significant limitations:</p> <ol> <li>Factuality / Hallucinations: When uncertain, models often generate plausible-sounding but false information. They’re optimized for probabilistic likelihood, not factual accuracy.</li> <li>Limited Reasoning: While techniques like <a href="https://arxiv.org/pdf/2201.11903.pdf">Chain of Thought</a> prompting improve LLM’s ability to reason, they’re restricted to solving the selected type of problem and approaches to solving them without improving generalized reasoning abilities.</li> <li>Lack of Planning: LLMs predict one step at a time, lacking effective long-term planning crucial for tasks requiring sustained goal-oriented behavior.</li> </ol> <p>Despite impressive advancements, the challenge of autonomous driving illustrates the gap between current AI and human-level intelligence. As LeCun notes, humans can learn driving basics in about 20 hours. In contrast, self-driving car development has consumed billions of dollars, extensive data collection, and decades of effort, yet still hasn’t achieved human-level performance.</p> <p>Even achieving Level 5 autonomy wouldn’t signify true human-level AI or Artificial General Intelligence (AGI). Such intelligence would involve learning to drive from scratch within a day, using only data collected during that experience, without relying on massive pre-existing datasets for finetuning. Realizing this level of adaptable intelligence might require several more decades of research.</p> <h2 id="common-sense">Common Sense</h2> <p>The limitations in AI models can often be attributed to a lack of common sense. Common sense can be defined as thinking and acting in a reasonable manner. Humans and many animals have this ability. This includes avoiding egregiously dangerous or incorrect actions. Expanding on the autonomous driving example, AV systems need to be trained to deal with new situations safely. When learning to drive, humans utilize their common sense to know to not do dangerous things like driving off the road or into other cars. This is not obvious to current AV systems, so they require a large amount of training data to avoid these actions.</p> <p>LLMs similarly demonstrate a lack of common sense through nonsensical or illogical outputs. Common sense is a vague term. One definition is that it is a lower bound on the types of errors an agent makes. For AI to be trustworthy, it needs this foundational level of understanding.</p> <p>Common sense can also be viewed as a collection of world models. These models enable quick learning of new skills, avoidance of dangerous mistakes in novel situations, and prediction of outcomes in unfamiliar scenarios. Essentially, we use world models to generalize our experiences.</p> <h3 id="how-humans-learn">How Humans Learn</h3> <p>Humans acquire basic understanding of the world during early infancy, but we’re also born with some innate knowledge. The brain isn’t randomly initialized; it’s evolved, pre-trained, and fine-tuned throughout life. This differs significantly from artificial neural networks, which start with random initializations and have far weaker inductive biases than humans or animals. Life is generally pre-programmed to behave in a certain way from birth. More intelligent life is able to learn more and not purely rely on innate knowledge.</p> <p>Understanding the extent to which babies acquire common sense during infancy is crucial for AI development. If common sense is largely innate, focus should be on massive datasets mimicking evolutionary timescales. If it’s primarily learned, priority should be given to models that excel at quick learning from limited data.</p> <p>A baby’s experience, while not comparable to evolutionary timescales, still represents a substantial dataset. If a baby is awake for <a href="https://intuitiveparentingdc.com/blog/2018/7/6/developmentally-appropriate-sleep-expectations-birth-to-age-5">8 hours</a> a day, in four months they have seen about 960 hours of data. This data is also augmented by other sensory signals and dense biological supervision (pain, hunger, emotions). This is around the same length as the <a href="https://arxiv.org/pdf/1705.06950.pdf">Kinetics 400</a> video dataset. This is still dwarfed by the millions of hours of video that self driving cars are using.</p> <p>This Nature <a href="https://www.nature.com/articles/s42256-024-00802-0">paper</a> by Orhan and Lake explores learning from infant-perspective data. They demonstrate that computer vision models can be trained on noisy, less diverse datasets collected from infant headcams. These egocentric datasets are far noisier and less diverse than standard image/video datasets, but AI models without strong inductive biases can learn from them.</p> <p>Emmanuel Dupoux’s diagram, presented by Yann LeCun, suggests that babies often understand concepts like object permanence, solidity, and biological motion by around four months. While presented as quick learning, it’s important to note the significant amount of data processing that occurs during this time.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/dupoux-480.webp 480w,/blog/assets/img/blog/jepa/dupoux-800.webp 800w,/blog/assets/img/blog/jepa/dupoux-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/dupoux.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Dupoux diagram on cognitive development" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We don’t yet know precisely how much data AI systems would need to learn the same concepts as babies. It’s likely that the data efficiency gap is relatively small for basic concepts that babies learn. For instance, object permanence could probably be learned from 960 hours of video data. However, it becomes evident that this gap grows substantially with age and with the complexity of the knowledge being assessed. The challenges in developing fully autonomous vehicles clearly demonstrate how large this data efficiency gap can become.</p> <p>In addition to the lack of common sense, we mention three other fundamental gaps in the ability of current AI: hallucinations, lack of planning, and lack of reasoning.</p> <h2 id="learning-to-think">Learning to Think</h2> <p>The question of whether Large Language Models (LLMs) can truly reason and plan is a contentious topic in the AI community. While these models exhibit behaviors that resemble <a href="https://arxiv.org/abs/2201.11903">reasoning</a> and planning, skeptics argue that they merely replicate patterns from their training data.</p> <p>To frame this discussion, let’s consider reasoning and planning as forms of “thinking”, which we will define as a variable length internal process that precedes any outputs.. Current deep learning models employ two primary mechanisms for this kind of processing:</p> <ol> <li>Depth: Each layer in a neural network can be viewed as a step in the thinking process. However, this depth is typically fixed, with some recent <a href="https://arxiv.org/abs/2404.02258">work</a> exploring dynamic depth adjustment based on input complexity. Despite these advances, maximum depth and other constraints still limit the model’s flexibility.</li> <li>Sequential Generation: Decoder-based LLMs, such as GPT, generate text one token at a time. Each step in this process involves some degree of computation that could be interpreted as thinking. Prompt engineering techniques leverage this sequential nature to guide the model towards desired outputs. A key limitation of this approach is that the model must produce a token at each step, preventing purely internal information processing.</li> </ol> <p>While these properties enable models to create the illusion of thought, significant advancements are necessary to achieve more effective reasoning and planning capabilities.</p> <p>Many researchers draw parallels between AI and the two-system model of thinking <a href="https://www.google.com/books/edition/Thinking_Fast_and_Slow/ZuKTvERuPG8C?hl=en&amp;gbpv=1&amp;printsec=frontcover">proposed</a> by Daniel Kahneman. System 1 thinking is fast and intuitive, providing immediate responses without conscious deliberation. System 2, in contrast, is slower and more deliberate, engaging in deeper cognitive processing. Current machine learning models, including LLMs, primarily operate in a System 1 mode by processing information in a single pass without the ability to plan ahead. While they excel at pattern recognition, they lack true reasoning or planning capabilities.</p> <p>This inability to plan contributes to factual errors in LLM outputs. Each generated word carries a risk of inaccuracy, with the probability of errors increasing exponentially as the output length grows. The sequential nature of token generation means that early mistakes can compound, potentially invalidating the entire output. This stands in stark contrast to human speech, where we typically plan our utterances at a higher level before vocalization, minimizing such errors. In this context, reasoning can be viewed as the planning of speech. Without the capacity to reason or plan effectively, LLMs essentially “speak without thinking.”</p> <p>In the JEPA paper, Yann LeCun proposes frameworks for models that can think. Learning to think may address the fundamental problems in current AI models and represent a crucial step towards achieving more human-like intelligence in AI.</p> <h1 id="modality">Modality</h1> <p>Recent advancements have expanded LLMs to include multimodal processing and outputs, but they remain primarily language-centric. This raises questions about the sufficiency of language alone for AI and the investment needed in visual understanding. Could visual comprehension help ground AI in reality, improving common sense and reducing hallucinations?</p> <p>Language serves as a compressed representation of the complex concepts humans experience. Its expressive power is vast, capable of describing intricate scientific theories and nuanced emotions. Yet, language alone may not suffice for complete understanding.</p> <p>Humans interpret language within the context of shared reality. It functions as a highly efficient medium for transmitting information through the relatively narrow bandwidth of speech. When we process language, our brains rely on prior knowledge and experiences. While some of this prior information can be acquired through text, a significant portion stems from visual and physical interactions with the world.</p> <p>Currently, it does seem that language models are more capable than vision models. Language models currently outperform visual models due to information density, data requirements, and data availability.</p> <p>In a given data point there is a certain amount of explicit information in the form of bits. But then there is relevant information that is useful. For example if you take an image of the park, a lot of bits are used to represent the position of every blade of grass. But that is not useful in most scenarios. Language is very compressed. While there are some filler words that don’t add much <a href="https://www.youtube.com/watch?v=VvPaEsuz-tY&amp;ab_channel=Argonaut57">information</a>, the ratio of knowledge to bits is high. However for images, most of the bits are not useful. This means you needs orders of magnitude more bits of data to learn equivalent knowledge. Video models are further behind because you need another order of magnitude more bits, since consecutive frames in video are mostly redundant.</p> <p>While language-based AI leads, scenarios exist where visual learning could catch up. One scenario in which visual learning could overtake language is that we will have a large number of robots / autonomous vehicles interacting with the world while collecting visual data. Language will be data constrained with the rate of new text generation limiting scaling. In a world with a lot of robots, the knowledge gained from the visual world and the size of the available datasets may exceed that of text. However, this is all very speculative. We don’t know how important vision or grounding is for intelligence.</p> <h1 id="a-framework-for-building-human-level-ai">A Framework for Building Human Level AI</h1> <p>Yann proposes a high level architecture for building an AI system that is aimed at addressing the problems we outlined. This is a design for an intelligent agent that can perceive the world,</p> <p>We will then explore the various challenges that must be addressed to construct such an architecture. Currently, this is merely a theoretical architecture. Building certain components remains an open problem, and assembling all the modules will pose an additional challenge.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/jepa_brain-480.webp 480w,/blog/assets/img/blog/jepa/jepa_brain-800.webp 800w,/blog/assets/img/blog/jepa/jepa_brain-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/jepa_brain.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="High Level View of LeCun's Architecture for Intelligence" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">High Level View of LeCun's Architecture for Intelligence</figcaption> </figure> <p>This architecture contains different proposed components. We will explain these components and their relationships.</p> <p><strong>Configurator</strong>: Configures input from all other modules and configures them for the task at hand. It tells the perception module what information to extract.</p> <p><strong>Perception:</strong> Estimates the current state of the world from different sensory signals.</p> <p><strong>World module</strong>: Estimates missing information about the state of the world and predicts future states. It simulates the world and extracts relevant information as determined by the configurator.</p> <p><strong>Cost module</strong>: Measures the level of discomfort as energy. This energy is the sum of the intrinsic cost module and the trainable critic module.</p> <p><strong>Intrinsic cost</strong>: Computes a cost given the current state of the world and predicted future states. This cost can be imagined as hunger, pain, or general discomfort. This cost can be hard wired in AI agents, as done with rewards in RL.</p> <p><strong>Trainable Critic</strong>: Predicts future intrinsic energy. It has the same input as the intrinsic cost. This estimate is dependent on the intrinsic cost and cannot be hardwired. It is trained from past states and subsequent intrinsic cost, retrieved from memory.</p> <p><strong>Short term memory</strong>: Stores relevant information about past present and future states of the world along with intrinsic cost.</p> <p><strong>Actor</strong>: Proposes sequences of actions. These sequences are executed by the effectors. The world model predicts future states from the sequence which then generates a cost.</p> <h1 id="actor">Actor</h1> <p>The actor proposes an optimal action or sequence of actions.</p> <p>If the world model and cost are well behaved, gradient based optimization can be used to determine an optimal action sequence. If actions are discrete then dynamic programming methods such as beam search can be used.</p> <p>There are two different modes in the actor. These align with Kahneman’s System 1 and 2, that we mentioned earlier.</p> <p><strong>Mode 1 Reactive Behavior</strong>: A policy module that computes an action from the state generated by perception and short term memory. This module acts fast and produces simple decisions. A world model is needed to estimate the cost of an action. Without a world model the agent would have to perturb their actions which is not feasible. The world model can be adjusted after observing the next state.</p> <p><strong>Mode 2 Reasoning and Planning</strong>: A sequence of actions along with predicted corresponding states is generated. From this sequence of states, a cost can be computed. Planning is done by optimizing the action sequence to minimize total cost. The action sequence is then sent to the effectors which execute at least the beginning of the sequence. The states and costs are stored in short term memory. The sequence can be optimized through gradients since the cost and world model are differentiable. Dynamic programming can also be used. Planning in this setup is essentially inference time cost optimization.</p> <p>Agents may have multiple policy modules executing mode 1. In this design, the agent only has one world model, so mode 2 can only be run once. However, AIs could be designed to have multiple world models and mode 2 processes at the same time. This is similar to having multiple thoughts at the same time. However this would be very complicated in that the different modules would have to coordinate with the effectors and other modules to avoid conflicts. Also, this may be why humans don’t think like this.</p> <p>Policy modules can be learned to approximate actions from mode 2 reasoning. This is the process of learning a new skill. In humans system 2 thinking can be done through system 1 after enough learning. For example, in chess, inexperienced players plan steps explicitly and simulate outcomes. Experienced players can instantly recognize patterns and make optimal moves.</p> <h1 id="cost">Cost</h1> <p>Cost is the sum of an immutable intrinsic cost and a trainable cost or critic.</p> \[C(s) = \mathrm{IC}(s) + \mathrm{TC}(s)\] <p>Each of these costs are the sum of different sub-costs generated by submodules. The weights of the sub-cost at each state \(u\) and \(v\) are determined by the configurator. This allows the agent to focus on different goals at different times.</p> \[\mathrm{IC}(s) = \sum_{i=1}^ku_i\mathrm{IC_i}(s)\\ \mathrm{TC}(s) = \sum_{i=1}^kv_i\mathrm{TC_i}(s)\] <p>The IC being immutable prevents the agent from drifting towards bad behaviors. It constrains the behavior of the agent.</p> <p>\(\mathrm{TC}\) or the critic is trained to predict future intrinsic cost values. The intrinsic cost only considers the current state. The critic can be trained to predict the future cost so the agent can minimize cost in the future. The short term memory stores triplets of (time, state, intrinsic energy): \((\tau, s_{\tau}, IC(s_{\tau}))\). The critic can be trained to predict a cost of a future state or a discounted sum of future intrinsic costs. For example the loss function of the critic could be \(\|\|\mathrm{IC}(s_{\tau+\delta}) - \mathrm{TC}(s_{\tau})\|\|^2\). This formulation trains the critic to predict the intrinsic cost of a state \(\delta\) steps in the future. \(\mathrm{IC}(s_{\tau+\delta})\) can be replaced with other targets that can be extracted from the sequence of triplets. However, it cannot depend on the future trainable cost itself.</p> <h1 id="configurator">Configurator</h1> <p>The configurator controls the other components of the system. If these components are implemented as transformers, they can be easily configured by adding tokens. The configurator would inject tokens to steer these components in certain directions. For example it may influence certain types of actions from the actor, or for perception to focus on certain properties.</p> <p>The configurator is also responsible for setting the weights of the cost terms. This will allow for the agent to focus on different subgoals at different times. The unanswered question is how the configurator can learn to decompose a complex task into subgoals.</p> <h1 id="world-model">World Model</h1> <p>In JEPA, the purpose of the world model is to predict future representations of the state of the world. There are three main issues</p> <ol> <li>Diversity of the state sequences it is able to observe when training</li> <li>The world isn’t fully predictable, so the model has to predict multiple plausible state representations following an action</li> <li>Predictions must be made at different time scales and abstractions</li> </ol> <h2 id="self-supervised-learning--energy-based-models">Self Supervised Learning / Energy Based Models</h2> <p>In order to train a world model, Yann LeCun proposes an SSL energy based model (EBM).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/ebm-480.webp 480w,/blog/assets/img/blog/jepa/ebm-800.webp 800w,/blog/assets/img/blog/jepa/ebm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/ebm.png" class="image-fluid mx-auto d-block" width="300" height="auto" alt="ebm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>\(x\) and \(y\) can be considered videos, where \(y\) follows x. EBMs learn an energy function \(F(x,y)\) that take low values when \(x\) and \(y\) are compatible and high if not. Compatible in this context means that \(y\) is a plausible continuation of \(x\).</p> <p>This is different from generative models in that \(y\) is not directly predicted from \(x\). There is a large space of values of \(y\) that can follow \(x\). Predicting exactly what will happen is an intractable problem. However, it is feasible to understand what is possible and what is not. Being good at this task requires an understanding of the world and common sense. A value of \(y\) that defies the laws of physics should result in a high energy value.</p> <p>However, planning requires predictions of future states. Although \(y\) can’t be predicted directly, we can predict future representations of \(y\). We can get representations from an encoder: \(s_x = g_x(x)\), \(s_y = g_y(y)\)</p> <p>The encoder will be trained such that the representations are maximally informative about each other, and that \(s_y\) can easily be predicted from \(s_x\). We can make predictions on this representation to enable planning.</p> <p>A latent variable can be introduced to handle uncertainty. A latent variable is just an arbitrary random variable. It is source of randomness that is transformed to a useful distribution. Here we want to map the latent variable to the large space of possible values \(s_y\) can take.</p> <p>A latent-variable EBM (LVEBM) is represented as \(E_w(x, y, z)\).</p> <p>The energy function can be determined by find the \(z\) value that minimizes the energy. \(F_w(x,y) = \min_{z \in \mathcal{Z} }E_w(x,y,z)\)</p> <p>The EBM collapses when all pairs have the same low energy. This can happen when the latent variable has too much information capacity. This happens because \(z\) can vary along a larger space. This means that the space for which the energy of \(y\) is low is correspondingly large. If it is too large then the energies of \(y\) collapse. If the \(z\) dimension is the same as the representation dimension, the model can ignore \(y\) entirely and set \(s_y\) to equal \(z\).</p> <p>The paper describes a high data density region. This refers to \((x, y)\) pairs that are commonly seen in the real data distribution. We want to lower energy in this region, but keep it high outside of it. Collapse is when the energy is low inside and outside of this region which makes the EBM useless.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/regularized_ebm-480.webp 480w,/blog/assets/img/blog/jepa/regularized_ebm-800.webp 800w,/blog/assets/img/blog/jepa/regularized_ebm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/regularized_ebm.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="regularized ebm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>There are two training methods used to prevent collapse.</p> <p>Contrastive methods: Collapse is avoided by increasing the energy with respect to negative examples. It requires some method to generate examples to contrast against. The number of contrastive examples needed grows exponentially with respect to the dimension of the representation.</p> <p>Regularized methods: In these methods the loss is regularized to minimize the space in \(y\) where the energies are lowered. These are less likely to be affected by the curse of dimensionality. Contrastive architectures can be regularized. For example, the latent dimension can be constrained.</p> <h2 id="joint-embedding-predictive-architecture">Joint Embedding Predictive Architecture</h2> <p>JEPA is an EBM that performs predictions in the representation space. The energy is the error in predicting \(s_y\) from \(s_x\).</p> <p>JEPA needs multi-modality, which in this context means to represent multiple possible values of \(y\). There are two ways it can be achieved.</p> <p>Encoder invariance: This means that \(s_y\) will be the same for different values of \(y\). The encoder ignores aspects of the state that may vary.</p> <p>Latent variable predictor: Varying \(z\) will lead to different plausible predictions of \(s_y\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/lv_jepa-480.webp 480w,/blog/assets/img/blog/jepa/lv_jepa-800.webp 800w,/blog/assets/img/blog/jepa/lv_jepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/lv_jepa.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="JEPA with a latent variable" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>There are four criteria that can be used to train this architecture without contrastive loss:</p> <ol> <li>Maximize the information content of \(s_x\) about \(x\): \(-I(s_x)\)</li> <li>Maximize the information content of \(s_x\) about \(y\): \(-I(s_y)\)</li> <li>Make \(s_y\) predictable from \(s_x\): \(D(s_y, \tilde{s_y})\)</li> <li>Minimize the information content of the latent variable with a regularizer: \(R(z)\)</li> </ol> <h3 id="hierarchical-jepa-h-jepa">Hierarchical JEPA (H-JEPA)</h3> <p>There is a trade off between information loss in the encoding and the predictability of the encodings. If a representation contains most of the information of the input, it would be hard to predict. A more abstract and higher level representation would be lower in dimension and more predictable. Higher dimension representations are also more suitable for longer term predictions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/hjepa-480.webp 480w,/blog/assets/img/blog/jepa/hjepa-800.webp 800w,/blog/assets/img/blog/jepa/hjepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/hjepa.png" class="image-fluid mx-auto d-block" width="500" height="auto" alt="H-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>H-JEPA (Hierarchical JEPA) enhances JEPA’s abstraction capabilities by splitting the architecture into two parts. The first JEPA handles low-level representations for short-term predictions, while the second operates at a higher abstraction level for longer-term forecasts. This two-tier structure, though innovative, is arbitrary. True intelligence requires multiple levels of abstraction. However, it is not clear how many levels of abstraction are needed. We may even need variable levels of abstraction. Different situations have different levels of complexity.</p> <p>This architecture can enable higher level planning. In JEPA-2, we can sample from the latent variable for several time steps. Directed search / pruning can be employed in order to efficiently search. This search can be used to determine an optimal action.</p> <p>This kind of search would be different in JEPA-1 or without H-JEPA because the latent dimension would be too large to efficiently sample from. Abstraction is needed to enable this kind of planning.</p> <h2 id="world-model-architecture">World Model Architecture</h2> <p>The world is unpredictable but the agent itself is predictable to the agent. This may motivate a model of self (ego model) that does not have a latent variable.</p> <p>The state of the world varies only slightly between time steps. Rather than regenerating, it can be updated in memory. With this architecture, the world model will only output the change in the state. This can be implemented with an attention like mechanism.</p> <ol> <li>The world model outputs query value pairs: \((q[i], v[i])\)</li> <li>The world model retrieves a value from memory using the query <ul> <li> \[\mathrm{Mem}(q) = \sum_jc_jv_j\] <ul> <li>The value retrieved from memory is a weighted sum of all values.</li> </ul> </li> <li> \[\tilde{c}_j = \mathrm{Match}(k_j,q)\] <ul> <li>Measures dissimilarity between the key and query.</li> </ul> </li> <li> \[c = \mathrm{Normalize}(\tilde{c})\] <ul> <li>This is often a softmax.</li> </ul> </li> <li> \[v_j = \mathrm{Update}(r,v_j,c_j)\] <ul> <li>Value is updated using the current value and new value.</li> <li>The update function can be \(cr+(1-c)v\)</li> </ul> </li> </ul> </li> </ol> <h1 id="data-streams">Data Streams</h1> <p>In building a world model, we have to consider the fundemental differences in the type of data that humans and AI models process. Yann lists 5 modes of information gathering that an agent can use to learn its world model.</p> <ol> <li>Passive observation: sensor stream without control</li> <li>Action foveation: The agent can direct attention within the data stream</li> <li>Passive agency: Observing another agent’s actions and causal effects</li> <li>Active Egomotion: The sensors can be configured, for example moving a camera</li> <li>Active Agency: Sensory streams that are influenced by the agent’s actions</li> </ol> <p>Current AI methods largely focus on passive observation. Others modes may be needed to reach intelligence.</p> <p>AI is trained on internet data. Internet data is not experienced by the agent. Humans train on data that they experience. This is a fundamental difference. This is also why autonomous cars need so much training data. The AI driving systems don’t have other datasets that they have experienced. For example, if they trained on a large dataset of just walking around, they would need less driving data.</p> <p>How can we get a large scale dataset from the perspective of an agent. It won’t reach the scale of internet datasets. A present day example is autonomous car datasets. AV companies have large fleets of vehicles on the road collecting data. These are active data streams.</p> <h1 id="objective-driven-ai">Objective Driven AI</h1> <p>The components of this architecture can be out together to build an intelligent system that follows human defined objectives.</p> <p>Perception is used to generate an initial representation of the state of the world. The actor proposes a sequence of actions. The world model then predicts the state reached if the action sequence is executed. This state is then used in the objectives. The task objective defines what we want the system to do. This could be a task or particular problem. The guardrail objective makes sure the system accomplishes the task without any unwanted behavior. These guardrails would be designed for safety.</p> <p>The action sequence is optimized with respect to the objects. There will be a lot of flexibility in designing the objects to get the system to behave in the way we want.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/objective_driven_ai-480.webp 480w,/blog/assets/img/blog/jepa/objective_driven_ai-800.webp 800w,/blog/assets/img/blog/jepa/objective_driven_ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/objective_driven_ai.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Objective Driven AI" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The system can also be extended to achieve hierarchal planning. The higher levels of planning produce a state that will serve as an objective for the lower level. This state can be considered as a subgoal that is necessary to achieve the higher level goal. We can have unique objectives and guardrails for each level of planning.</p> <p>Latent variables are also introduced to represent the uncertainty in predictions of future states. The latent variables at the higher levels can be thought as imaginary higher level actions. However, only the lower level actions can actually be directly executed.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/h_objective_driven_ai-480.webp 480w,/blog/assets/img/blog/jepa/h_objective_driven_ai-800.webp 800w,/blog/assets/img/blog/jepa/h_objective_driven_ai-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/h_objective_driven_ai.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Hierarchal Objective Driven AI" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="towards-implementing-jepa">Towards Implementing JEPA</h1> <p>The JEPA paper is a position paper that describes a vision for AI that may take decades to materialize. However, since its publication in the summer of 2022, there have been a few steps in advancing the architecture. These papers particularly explore training of JEPAs. They do not explore the other components such as planning. These JEPAs are first steps to creating a world model.</p> <p>These are essentially self supervised pretraining methods. When comparing against other works, these papers cite training speed as their advantage. They can achieve strong downstream performance with fewer pretraining epochs.</p> <h2 id="i-jepa-self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture">I-JEPA: <a href="https://arxiv.org/pdf/2301.08243.pdf">Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</a></h2> <p>Compared to other image SSL approaches, I-JEPA takes advantage of the flexibility of the transformer architecture. ViT is used because it can handle an arbitrary amount of patches in an image, without requiring a strict shape in the input like CNNs</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/ijepa-480.webp 480w,/blog/assets/img/blog/jepa/ijepa-800.webp 800w,/blog/assets/img/blog/jepa/ijepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/ijepa.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="I-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The input image is split into \(N\) non-overlapping patches and fed into a target encoder \(f_{\theta}\) to compute patch representations. \(s_y = \{s_{y1} … s_{yN}\}\)</p> <p>\(M\) possibly overlapping blocks are sampled from these representations. These blocks are basically larger sections of the image that contain multiple patches.</p> <p>Context is generated by sampling a block (larger than the target blocks). When predicting a target from this context, the overlap with the target block is masked from the context. The network is trained to predict the representations of the target blocks given the context block, and position encodings for the target block. The position encodings are added to the input so that the model knows where the target is. It is just tasked with predicting representations at those positions.</p> <p>This architecture avoids collapse by having exponential moving average weights in the target encoder. This is the same approach used in data2vec and BYOL.</p> <p>The main hyperparameters introduced by this work is the scale and aspect ratio of the target and context blocks. Generally a small context is used to make this task difficult, which would force the model to learn higher level and more useful features.</p> <h2 id="v-jepa-revisiting-feature-prediction-for-learning-visual-representations-from-video">V-JEPA: <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/">Revisiting Feature Prediction for Learning Visual Representations from Video</a></h2> <p>V-JEPA is an extension of I-JEPA to videos. This is done by treating videos are 3d images.</p> <ol> <li>A clip of 64 frames (~2.1 seconds of video at 30 frames per second) is extracted from the video and resized to 16 × 224 × 224 × 3.</li> <li>The clip is split into \(L\) spatiotemporal patches of size 16x16x2 (2 is the number of consecutive frames.</li> <li> <p>A random mask is calculated for the context. This is a 2D that is similar to the mask in I-JEPA. This mask is then repeated across the time dimension. This repetition is necessary because the videos are short and there would be too much redundancy for the same patch at different time steps. This redundancy would make the learning task too easy. This masking creates a context image, while the target is the original image.</p> <ol> <li>2 masks are sampled: one short range and one long range. The short range mask covers less area in the image and is more discontinuous. These masks are constructed by different configurations of overlapping blocks, as done in I-JEPA. The target encoder only needs to run once, even is there are multiple masks for the context. Having multiple masks leads to more efficient training.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/vjepa_masking-480.webp 480w,/blog/assets/img/blog/jepa/vjepa_masking-800.webp 800w,/blog/assets/img/blog/jepa/vjepa_masking-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/vjepa_masking.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="V-JEPA masking" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Short-range (left), long-range (right)</figcaption> </figure> <ol> <li>The tokens are processed by a transformer encoder (linear projection of patches + multiple transformer blocks). The masked out patches do not need to be processed. There is a separate encoder for the target and context. The target encoder is an EMA of the context encoder (same as I-JEPA).</li> <li>The predictor predicts the representations of the masked tokens by the unmasked tokens processed by the context encoder. The loss is the L1 distance between the representations of these masked tokens (from the target encoder, and the context encoder + predictor).</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/vjepa-480.webp 480w,/blog/assets/img/blog/jepa/vjepa-800.webp 800w,/blog/assets/img/blog/jepa/vjepa-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/vjepa.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="V-JEPA Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Very similar to I-JEPA but with an added temporal dimension.</figcaption> </figure> <p>Very similar to I-JEPA but with an added temporal dimension.</p> <p>This is predicting gaps in short videos. It does not predict across time. Human learning is across the time dimension.</p> <p>Attentive probing is used to evaluate this model on different finetuning tasks. This is needed in place of linear probing since the input size may vary. This just requires learning a query token specific to the task and a linear classifier on top of the pretrained encoder.</p> <p>V-JEPA processes small sequences of frames. These short videos are essentially images with a little animation. However, that is the current state of video self supervised learning. To achieve a model that is closer to human or even animal level intelligence, this approach needs to scale up significantly. The resolution of the video needs to be increased. Also, the model needs to process longer durations of video and make predictions across time. For example, you should be able to predict what happens in the next 1 minute, based on the previous ten minutes of video input. Such a model could be the basis for an intelligent agent’s world model.</p> <p>V-JEPA is a very interesting model that may be the start of a highly important line of research.</p> <h2 id="mc-jepa-a-joint-embedding-predictive-architecture-for-self-supervised-learning-of-motion-and-content-features">MC-JEPA: <a href="https://arxiv.org/pdf/2307.12698.pdf">A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</a></h2> <p>This is an extension of JEPA to include motion information. It uses an optical flow objective to learn motion from videos, and uses general SSL to learn about the content of the images/videos. Optical flow is estimating the direction in which pixels move between two consecutive frames of a video.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/mcjepa_architecture-480.webp 480w,/blog/assets/img/blog/jepa/mcjepa_architecture-800.webp 800w,/blog/assets/img/blog/jepa/mcjepa_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/mcjepa_architecture.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="MC-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The details of this dense flow estimation are out of the scope of this blog post. Flow estimation and content feature learning are combined as a multitask learning objective. Images are sampled for content learning, while consecutive frames are sampled from videos for flow estimation. The encoder is shared for both tasks. This is a JEPA architecture because the representations from one frame are warped to match the representations from the next frame. The same encoder is used to process both frames.</p> <p>The architecture for flow estimation is hierarchal. This may be the first instantiation of an H-JEPA architecture. This architecture is based on <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_PWC-Net_CNNs_for_CVPR_2018_paper.pdf">PWC-Net</a>. Each level is a different resolution.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/jepa/mcjepa_full_architecture-480.webp 480w,/blog/assets/img/blog/jepa/mcjepa_full_architecture-800.webp 800w,/blog/assets/img/blog/jepa/mcjepa_full_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/jepa/mcjepa_full_architecture.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="MC JEPA full architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The image features are sampled from ImageNet, while a video dataset is used for flow estimation. It is also possible to use frames from video as images for content learning.</p> <p>This work shows that the JEPA framework is generalizable. There are a lot of ways that we could design a world model and it could include many possible objectives.</p> <h2 id="whats-next">Whats next?</h2> <p>The current research in JEPA represents a significant step towards Yann LeCun’s vision of building a world model capable of human-level AI. While the present focus is on creating effective representation learning models for visual data, the ultimate goal is far more ambitious. The holy grail of this research is a V-JEPA model that can predict across extended time horizons, potentially through a Hierarchical JEPA architecture capable of processing complex, lengthy videos like 10-minute YouTube clips.</p> <p>To realize this vision, several crucial advancements are necessary. Firstly, we need to embrace true multimodality, incorporating audio and other modalities that are often overlooked in current video models. Scaling up V-JEPA is also essential, requiring larger video datasets and more sophisticated model architectures that can handle higher resolutions. Additionally, the development of more challenging benchmarks for video understanding is critical, as current standards fall short of the complexity seen in image or language modeling tasks.</p> <p>Future iterations of V-JEPA must evolve beyond spatial masking to make predictions across various time horizons. This capability to forecast future representations based on present information is fundamental to understanding the temporal dynamics of video content. Achieving this may necessitate a hierarchical JEPA structure, where different levels handle predictions at various time scales and abstraction levels. Maybe the next JEPA paper will introduce a hierarchal video JEPA (HV-JEPA).</p>]]></content><author><name></name></author><category term="self-supervised-learning"/><category term="ai"/><summary type="html"><![CDATA[In the AI research community, Yann LeCun has a unique and often controversial perspective. As of 2024, LLMs and Generative AI are the main focus areas of the field of AI. We’ve all been impressed by the performance of LLMs in various contexts, and generative systems like OpenAI’s Sora. However, it is not clear where these advances fit in the long term goal of achieving and surpassing human level intelligence, which many call AGI.]]></summary></entry><entry><title type="html">Scaling Deep Learning</title><link href="https://rohitbandaru.github.io/blog/blog/2023/Scaling-Deep-Learning/" rel="alternate" type="text/html" title="Scaling Deep Learning"/><published>2023-02-21T00:00:00+00:00</published><updated>2023-02-21T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/blog/2023/Scaling-Deep-Learning</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/blog/2023/Scaling-Deep-Learning/"><![CDATA[<p>Many of the state-of-the-art results in deep learning are achieved using multiple GPUs. For some of the largest and most data-intensive ML models, it can take months or even years to train on one CPU or GPU. Training is sped up by scaling to large numbers of GPUs/TPUs. Some neural networks are too large to even fit on one GPU. For example, training large language models like BERT can easily exceed the available memory on a single <a href="https://github.com/google-research/bert/blob/master/README.md#out-of-memory-issues">GPU</a>.</p> <p>If you have the resources it can be easy to speed up training by adding more GPUs. However, it is important to understand the impact this scaling will have on training. Machine learning acceleration is a huge and complex field. I intend to just cover the basic intuitions to keep in mind when training a typical model.</p> <aside> ✍️ We will use GPU and TPU interchangeably. We are treating ML accelerators as generic. </aside> <p>There are two types of machine learning training parallelization: data parallelism and model parallelism.</p> <h1 id="data-parallelism">Data Parallelism</h1> <p>Data parallelism splits a training batch into smaller batches for each GPU. Each GPU has its own copy of the model. Each GPU computes gradients with its own training batch. These gradients are then aggregated across all the GPUs. Each GPU can send its gradients to all other GPUs. For example, if you train with a batch size of 64 and 4 GPUs, each GPU will get a batch size of 16. It will compute gradients for this batch. Once all the GPUs are done with their computations, they can send their gradients to each other. The gradients are then averaged and applied to the model. This allows us to train a model with batch size 64 at the speed of batch size 16. However, there is additional latency in communicating the gradients and synchronizing the GPUs, but it is usually negligible compared to the gradient computations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/scaling_ml/data_parallelism-480.webp 480w,/blog/assets/img/blog/scaling_ml/data_parallelism-800.webp 800w,/blog/assets/img/blog/scaling_ml/data_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/scaling_ml/data_parallelism.png" width="100%" height="auto" alt="Data parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Another option is to skip the gradient aggregation and simply apply the updates to the model separately. This can be done by having an orchestrator take a lock on the model. Or in <a href="https://arxiv.org/abs/1106.5730">Hogwild</a>, you can just update the model without any lock. This will allow some GPU batches to be dropped due to race conditions but minimizes synchronization delays.</p> <p>Adding GPUs doesn’t make training steps faster. It allows you to have larger mini-batch sizes, which in turn trains models faster.</p> <p>There are three variables to consider: mini-batch size, GPU batch size, and number of GPUs. Since the number of GPUs is a function of the other two variables, we will only discuss the two types of batch size and how to optimize them.</p> \[\textrm{Mini batch size} = \textrm{GPU batch size} * \textrm{Number of GPUs}\] <p>The implementation of parallelism can vary between ML frameworks: <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch</a>, <a href="https://www.tensorflow.org/guide/distributed_training">TensorFlow</a>, <a href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html#way-batch-data-parallelism">Jax</a></p> <h2 id="optimal-gpu-batch-size">Optimal GPU Batch Size</h2> <p>With GPUs, we simply want to minimize the training step time. If we operate the GPUs in the optimal GPU batch size range, we can then just set the number of GPUs to get the optimal mini-batch size. Also, note the GPU batch size has an upper limit from the memory available.</p> <p>To see how GPU performance relates to speed, I timed the training steps of a ResNet50 model against ImageNet-sized batches of different sizes. I tested batch sizes of every power of two until the GPU ran out of memory.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/scaling_ml/step_speed_vs_batch_size-480.webp 480w,/blog/assets/img/blog/scaling_ml/step_speed_vs_batch_size-800.webp 800w,/blog/assets/img/blog/scaling_ml/step_speed_vs_batch_size-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/scaling_ml/step_speed_vs_batch_size.jpg" width="100%" height="auto" alt="step speed vs batch size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We see that the throughput (examples per ms) is maximized at the largest possible batch size. We also see that for batch sizes of less than 2^4 or 16, the throughput is lower. GPUs are inefficient with small batches due to overhead. CPUs perform better in some settings. The takeaway is that we want to maximize the GPU utilization by fitting the largest possible batch. Some libraries have the functionality to search for the largest possible batch size given a GPU and dataset. In the flat region, GPU step time increases linearly with batch size.</p> <h2 id="optimal-mini-batch-size">Optimal Mini Batch Size</h2> <p>To optimize the mini-batch size, we will ignore accelerators and just focus on mini-batch gradient descent. Mini batch size is less hardware-dependent and more problem dependent. We will use ImageNet as an example, but the effects of batch size on training should be considered for every new problem.</p> <p>Assuming maximize GPU usage/batch size, optimizing the mini-batch size means selecting the number of GPUs to use. The assumption is that with more GPUs, we can train a model faster. Training on more GPUs means faster training epochs. However, we are interested in the test accuracy of the model, not just completing epochs.</p> <p>Consider this plot from the paper <a href="https://arxiv.org/pdf/1811.03600.pdf">Measuring the Effects of Data Parallelism on Neural Network Training</a> by Shallue et al.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/scaling_ml/effects_of_dp-480.webp 480w,/blog/assets/img/blog/scaling_ml/effects_of_dp-800.webp 800w,/blog/assets/img/blog/scaling_ml/effects_of_dp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/scaling_ml/effects_of_dp.png" width="500" height="auto" alt="Plot of training speed vs batch size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>For points on the dashed line, the number of training steps is halved whenever the batch size is doubled. This means that doubling the GPUs/TPUs would have the training time. This is ideal. In this region, you can happily speed up your model training by adding more GPUs that you may have available. However, this tradeoff changes at batch size 2^13 or 8192. From here, doubling the GPUs still speeds up model training, but the speed will be more than half. This is the point of diminishing returns. If you have the GPUs, you might as well use them but those additional GPUs are not as effective.</p> <p>The paper goes into great detail on this relationship and the effects of other factors such as model architecture, optimizers, and datasets. The takeaway for this blog is that if you set the maximum GPU batch size, up to a point, adding additional GPUs will linearly speed up the training of your model.</p> <h1 id="model-parallelism">Model Parallelism</h1> <p>This type of parallelism is much less commonly used. It can be used along with data parallelism. Model parallelism is when an ML model is too large to fit in the memory of one device. It is partitioned across multiple devices. This has enabled us to train larger and larger networks. For example, the GPT-3 model is about <a href="https://www.reddit.com/r/MachineLearning/comments/gzb5uv/comment/fti44lv/?utm_source=share&amp;utm_medium=web2x&amp;context=3">350 GB</a>. No single GPU can store the whole model in memory.</p> <p>There are different ways of achieving model parallelism. You can split the model vertically by layer, or horizontally by splitting the tensors.</p> <h2 id="pipeline-parallelism">Pipeline Parallelism</h2> <p>The simplest solution is to process different layers of a neural network on different accelerations. A simple illustration of this:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/scaling_ml/pipeline_parallelism-480.webp 480w,/blog/assets/img/blog/scaling_ml/pipeline_parallelism-800.webp 800w,/blog/assets/img/blog/scaling_ml/pipeline_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/scaling_ml/pipeline_parallelism.png" width="100%" height="auto" alt="pipeline parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In <a href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html#speed-up-by-pipelining-inputs">PyTorch</a>, the layers are bucketed into groups of roughly equal memory so that the computations are evenly distributed across the accelerators.</p> <p>A major issue with this approach is that after Layer 0 has a forward pass, it has to wait for the other layers to compute forward and backward passes. The GPU is idle for about 75% of the time. The following diagrams are from the <a href="https://arxiv.org/pdf/1811.06965.pdf">GPipe</a> paper by Huang et al.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-480.webp 480w,/blog/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-800.webp 800w,/blog/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism.png" width="100%" height="auto" alt="batches without pipeline parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The solution is to have the layer compute the next graph while it is waiting for the gradient of the current batch. This essentially combines data parallelism with model parallelism. I explained above that to maximize training speed, we want to maximize the utilization of accelerators. For large models that require model parallelism, we have an additional problem of GPU waiting time. Pipelining GPU batches helps reduce this gap.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/scaling_ml/pipeline_parallelism_batches-480.webp 480w,/blog/assets/img/blog/scaling_ml/pipeline_parallelism_batches-800.webp 800w,/blog/assets/img/blog/scaling_ml/pipeline_parallelism_batches-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/scaling_ml/pipeline_parallelism_batches.png" width="100%" height="auto" alt="Pipeline parallelism batches" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We see that with 4 GPU batches, each GPU is idle for about 6/16 of the time. The variables here are the number of GPUs and number of GPU batches per GPU. With 1 GPU batch (no pipelining), the utilization is: \(\frac{2}{n_{GPU}* 2} = \frac{1}{n_{GPU}}\). With pipelining, we get \(\frac{n_{batches}*2}{n_{batches}*2 + 2* (n_{GPU}-1)}\). This simplifies to the following:</p> \[utilization = \frac{n_{batches}}{n_{batches} + n_{GPU}-1}\] <p>This equation explains the tradeoff. Increasing the number of GPU batches drives the utilization closer to 1, while increasing the number of GPUs reduces the utilization.</p> <p>In an optimal setup, we split the model among as few GPUs as possible, but increase the number of batches that they process in a step. Pipeline parallelism has the added benefit of the speedups of data parallelism. This makes it a very effective solution.</p> <p>In the <a href="https://arxiv.org/pdf/1806.03377.pdf">PipeDream</a> paper, Harlap et al. show that we can further reduce idle time by interleaving forward and backward operations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/scaling_ml/pipedream-480.webp 480w,/blog/assets/img/blog/scaling_ml/pipedream-800.webp 800w,/blog/assets/img/blog/scaling_ml/pipedream-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/scaling_ml/pipedream.png" width="100%" height="auto" alt="pipedream" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>However, this eliminates gradient synchronization. Even eliminating batches above 4, we get the same utilization as GPipe parallelism, just in a different order. For many of the backward passes, a stale version of the model parameters is used. Gradient synchronization is an important tradeoff in all types of ML parallelism.</p> <p>In analyzing utilization, we have been assuming that forward and backward computations are equivalent. Backward passes tend to take more time. If we interleave operations as to always prioritize backward passes, we can get a utilization gain. From AWS Sagemaker <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html">documentation</a>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/scaling_ml/pipedream1-480.webp 480w,/blog/assets/img/blog/scaling_ml/pipedream1-800.webp 800w,/blog/assets/img/blog/scaling_ml/pipedream1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/scaling_ml/pipedream1.png" width="100%" height="auto" alt="without backward prioritization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The idle time here is 1 forward pass and 1 backward pass.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/scaling_ml/pipedream2-480.webp 480w,/blog/assets/img/blog/scaling_ml/pipedream2-800.webp 800w,/blog/assets/img/blog/scaling_ml/pipedream2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/scaling_ml/pipedream2.png" width="100%" height="auto" alt="with backward prioritization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>With backward prioritization, the idle time for GPU 0 is 3 forward passes. This effect will be increased with more GPUs. There are many tradeoffs in parallel ML, such as communication between model layers, the memory overhead of forward and backward passes, different model splits, staleness, etc. We are only covering the high-level intuitions to achieve fast and effective training of large models.</p> <p>What if we want to use more GPUs for data parallelism, but without splitting up the model further? We can simply run pipelines in parallel. For example, we can split the model among four GPUs but duplicate each model split twice. We can then aggregate the gradients of both pipelines in the update. This is often called hybrid model and data parallelism.</p> <h2 id="tensor-parallelism">Tensor Parallelism</h2> <p>Instead of splitting the model into layers, we can split the layers themselves. From the <a href="https://arxiv.org/pdf/1909.08053.pdf">Megatron-LM paper</a> by Shoeybi et al.:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/scaling_ml/tensor_parallelism-480.webp 480w,/blog/assets/img/blog/scaling_ml/tensor_parallelism-800.webp 800w,/blog/assets/img/blog/scaling_ml/tensor_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/scaling_ml/tensor_parallelism.png" width="100%" height="auto" alt="tensor parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The input X has to be completely copied for each split of the model. The layer is split into two halves. The splits of the model are then aggregated in the last layers of the model. Splitting the tensors themselves offers some benefits. The latency is reduced since you can fit more layers on a GPU. This is parallel computation instead of serialized computation. You don’t have to worry about scheduling to minimize idle time.</p> <p>An issue with this approach is that the activations are also separated, so you are learning a different model architecture. There is an additional cost in concatenating \(Y_1\) and \(Y_2\) for both GPUs. The Megatron-LM architecture is designed to reduce the cost of communicating between GPUs.</p> <h1 id="conclusion">Conclusion</h1> <p>We touched the surface on the many tradeoffs, optimizations, and considerations needed for distributed and large scale ML. As models grow larger, it will become more import to understand and keep up to date with this field.</p> <h1 id="additional-resources">Additional Resources</h1> <p><a href="https://www.youtube.com/watch?v=3XUG7cjte2U">https://www.youtube.com/watch?v=3XUG7cjte2U</a></p> <p><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">https://lilianweng.github.io/posts/2021-09-25-train-large/</a></p> <p><a href="https://openai.com/blog/techniques-for-training-large-neural-networks/">https://openai.com/blog/techniques-for-training-large-neural-networks/</a></p> <p><a href="https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/">https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/</a></p> <p><a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">https://huggingface.co/docs/transformers/v4.15.0/parallelism</a></p>]]></content><author><name></name></author><category term="applied-ml"/><summary type="html"><![CDATA[Many of the state-of-the-art results in deep learning are achieved using multiple GPUs. For some of the largest and most data-intensive ML models, it can take months or even years to train on one CPU or GPU. Training is sped up by scaling to large numbers of GPUs/TPUs. Some neural networks are too large to even fit on one GPU. For example, training large language models like BERT can easily exceed the available memory on a single GPU.]]></summary></entry><entry><title type="html">Knowledge Distillation as Self-Supervised Learning</title><link href="https://rohitbandaru.github.io/blog/blog/2022/knowledge-distillation-ssl/" rel="alternate" type="text/html" title="Knowledge Distillation as Self-Supervised Learning"/><published>2022-01-11T00:00:00+00:00</published><updated>2022-01-11T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/blog/2022/knowledge-distillation-ssl</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/blog/2022/knowledge-distillation-ssl/"><![CDATA[<p>Self-supervised learning (SSL) methods have been shown to effectively train large neural networks with unlabeled data. These networks can produce useful image representations that can exceed the performance of supervised pretraining on downstream tasks. However, SSL is not effective with smaller models. This limits applications where computational power is limited, such as edge devices. Knowledge distillation (KD) is a popular method to train a smaller student network from a larger and more powerful teacher network. The <a href="https://arxiv.org/abs/2101.04731">SEED</a> paper by Fang et al., published in ICLR 2021, applies knowledge distillation to self-supervised learning to pretrain smaller neural networks without supervision. In this post, we will discuss self-supervised learning and knowledge distillation and how they are unified in SEED.</p> <h1 id="self-supervised-learning">Self-supervised Learning</h1> <p>Self-supervised learning is a form of unsupervised learning. Self-supervision refers to labels that are generated from the data itself rather than manual annotations (ex: images vs class labels). Different SSL methods have different tasks that are used for the self-supervision.</p> <p>In computer vision, it is very common to pretrain a neural network on ImageNet classification. This is an example of supervised pretraining. This network can then be fine-tuned for various downstream tasks, such as semantic segmentation, object detection, or even medical image classification. Supervised pretraining has been a standard practice in computer vision.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/distillation_ssl/sl_vs_ssl-480.webp 480w,/blog/assets/img/blog/distillation_ssl/sl_vs_ssl-800.webp 800w,/blog/assets/img/blog/distillation_ssl/sl_vs_ssl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/distillation_ssl/sl_vs_ssl.png" width="400" height="auto" alt="Self-supervised vs Supervised Pretraining" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Self-supervised vs Supervised Pretraining</figcaption> </figure> <p>Self-supervised learning provides an alternative to supervised pretraining with two main benefits:</p> <ol> <li> <p>Generalizability: A supervised objective like classification can limit what a model learns about data. This is because not all the information in an image is needed for classification. For example, you can train a network to classify cats and dogs. The color of the animal’s fur is not relevant to this objective. Therefore, representations from this network may not be useful for a downstream task of fur color classification.</p> </li> <li> <p>Unlabeled data: The amount of available unlabeled data dwarfs labeled datasets. SSL is a form of unsupervised learning. It can leverage datasets of billions of images rather than be limited to supervised datasets, such as ImageNet which has about one million images.</p> </li> </ol> <p>There are many methods of SSL. Most of the recent state of art methods implement a form of contrastive learning. This includes <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, <a href="https://arxiv.org/abs/2006.09882">SwAV</a>, and <a href="https://arxiv.org/abs/1911.05722">MoCo</a>. In contrastive learning, representations are pushed towards positive examples and away from negative examples. In SSL, the positive examples are variations of the original image and the negative examples are from other images in the dataset. Contrastive SSL methods share some common steps:</p> <ol> <li> <p>Image augmentation: In supervised learning, augmentations, such as random cropping, flipping, and color distortions are used to generate more training data. In SSL, augmentation is used to produce positive examples. It is needed to avoid the trivial solution of encoding raw pixel values without learning anything about the content of the image.</p> </li> <li> <p>Contrastive loss: The goal of contrastive learning is to push positive examples closer together and negatives apart. It is most common to see a version of the <a href="https://arxiv.org/abs/1807.03748">InfoNCE</a> loss. This loss (defined below) is meant to maximize the similarity of a data point with one positive example, and minimize the similarity with many negative examples. The similarity function \(s\) is usually just the dot product.</p> </li> <li> <p>Negative samples: SSL needs a large amount of negative examples for the best performance. We want to push an image representation away from all other possible image representations from the dataset. This can be accomplished by having a large batch size (SimCLR). All the other images in the batch will be negative examples. An alternative is to keep negative examples in memory through multiple training batches. MoCo does this by keeping a queue of the most recent image representations. It is preferred to keep recent image representations, since the network changes gradually over time. Recent representations are more similar to representations that would be generated from the current network. The queue essentially approximates a large training batch.</p> </li> </ol> \[\begin{equation} \mathcal{L}_{\mathrm{InfoNCE}} = -\mathbb{E} \left[ \mathrm{log} \frac{ \exp(s(x, y)) } { \sum_{y_j} \exp(s(x,y_j)) } \right] \end{equation}\] <h2 id="moco">MoCo</h2> <p><a href="https://arxiv.org/abs/1911.05722">MoCo</a> (momentum contrast) by He et al. implements contrastive SSL by keeping a queue of examples. The queue allows for a large number of negative examples to be used in the contrastive loss The momentum encoder is trained at the same time as the encoder in a bootstrapped fashion. They must have identical architectures for the momentum update to occur.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/distillation_ssl/moco-480.webp 480w,/blog/assets/img/blog/distillation_ssl/moco-800.webp 800w,/blog/assets/img/blog/distillation_ssl/moco-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/distillation_ssl/moco.png" width="700" height="auto" alt="MoCo training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>With a queue of representations encoded by the momentum encoder, the main encoder is trained to contrast the representations. \(q\) is the query or the representation from the encoder. \(k_+\) is the corresponding representation from the momentum encoder. The loss aims to push \(q\) towards \(k_+\) and away from all other representations \(k\) in the queue which serve as negative examples.</p> \[\begin{equation} \mathcal{L}_i = -\log\frac{\exp(q*k_+/\tau)}{\sum_{i=0}^K\exp(q*k_i/\tau)}\\ \end{equation}\] <p>MoCo is very effective in pretraining large neural networks for many downstream tasks. SEED aims to extend this for smaller networks.</p> <h1 id="knowledge-distillation">Knowledge Distillation</h1> <p>In knowledge distillation (<a href="https://arxiv.org/abs/1503.02531">Hinton et al.</a>, <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">Buciluǎ et al.</a>), a large teacher model is used to train a smaller and more efficient student model. It is useful in the case that a large neural network can perform well on a task, but a small network cannot be directly trained to high accuracy. This makes it relevant to SSL, where only large neural networks have strong performance.</p> <p>In supervised learning for classification, the labels are hard targets or one-hot encoded vectors. All of the probability is assigned to one class, and all other classes have a value of zero. The teacher model will have a softmax layer which will return a soft target. The soft target will assign some probability to other classes. Knowledge distillation uses the teacher network to produce these soft targets and uses them to train the student model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/distillation_ssl/soft_vs_hard-480.webp 480w,/blog/assets/img/blog/distillation_ssl/soft_vs_hard-800.webp 800w,/blog/assets/img/blog/distillation_ssl/soft_vs_hard-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/distillation_ssl/soft_vs_hard.png" width="600" height="auto" alt="dark knowledge" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Shiba Inu dogs are known to have cat-like characteristics, soft labels can encode this by assigning some probability to the cat class.</figcaption> </figure> <p>The soft targets encode more information than hard targets. Hinton describes this as “dark knowledge”. For example, from a soft target you can tell which class is the second most likely or the relative probabilities between two classes. This information is not available in a hard target.</p> \[\begin{equation} p_i = \frac{\exp(\frac{z_i}{T})}{ \sum_{j} \exp(\frac{z_j}{T})}\\ \end{equation}\] <p>The soft targets can be made softer by increasing the temperature of the softmax. The temperature \(T\) is typically set to 1. However, in knowledge distillation higher temperatures can yield better results as it increases the magnitude of the non-max values.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/distillation_ssl/kd-480.webp 480w,/blog/assets/img/blog/distillation_ssl/kd-800.webp 800w,/blog/assets/img/blog/distillation_ssl/kd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/distillation_ssl/kd.png" width="700" height="auto" alt="Knowledge Distillation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Knowledge Distillation</figcaption> </figure> <ol> <li> <p>A teacher model is trained for high accuracy. This can be a large neural network or an ensemble.</p> </li> <li> <p>The teacher model generates soft labels for a dataset. This dataset can be the same or different from the hard labeled dataset.</p> </li> <li> <p>The student network is trained to predict the soft labels. It can also be simultaneously trained with hard labels in a separate loss term.</p> </li> </ol> <p>Distillation can use unlabeled data. Once a model is trained, it can be used to produce soft labels for a large unsupervised dataset. This can be larger than the initial labeled dataset and effectively train the student network on a much larger dataset.</p> <h1 id="knowledge-distillation-for-ssl">Knowledge Distillation for SSL</h1> <p>Knowledge distillation aims to transfer dark knowledge between models. Self-supervised learning aims to increase the dark knowledge learned by a model. When training a model on a supervised classification objective, it will not need to learn information that does not help with classification. The objective limits what the model learns. Self-supervised learning methods are designed to be general and not task specific.</p> <p>SSL does not perform well with smaller models which limits its applicability. Also, the downstream task is likely less complex than the SSL task and can be achieved more efficiently with a smaller model. Knowledge distillation offers a way to reduce the size of the model while maintaining accuracy and relevant knowledge.</p> <p>One way to apply KD to SSL is to train the teacher on a SSL objective and then apply KD to train the student on a downstream task. This would require first fine-tuning the teacher network on the downstream test, with a new output layer. It would be more efficient to distill knowledge to a smaller network before training on the downstream task.</p> <p>Although it is simple to apply knowledge distillation on a supervised downstream task, you cannot directly apply it to the self-supervised training objective. This is because SSL models do not output classification predictions. SSL models output feature representations of the input. Training a student network to match these feature representations would not be effective. Self-supervised training involves optimizing with an objective on top of the representations.</p> <h1 id="seed">SEED</h1> <p>In the <a href="https://arxiv.org/abs/2101.04731">SEED</a> paper, the authors propose a self-supervised approach to knowledge distillation. It uses a contrastive objective on the representations.</p> <p>This will allow knowledge distillation to occur before the downstream task. The method produces an SSL trained student network that can be efficiently fine-tuned on downstream tasks. SEED extends self-supervision to smaller models allowing us to compress SSL models to use in more applications.</p> <h2 id="method">Method</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/distillation_ssl/seed-480.webp 480w,/blog/assets/img/blog/distillation_ssl/seed-800.webp 800w,/blog/assets/img/blog/distillation_ssl/seed-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/distillation_ssl/seed.png" width="100%" height="auto" alt="SEED" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SEED</figcaption> </figure> <ol> <li> <p>Train the teacher, independent of the student network. Any of the recent state-of-the-art SSL methods or even supervised models (ResNet trained on ImageNet classification) can be used here. The only requirement is that the model must produce image representations. The teacher networks weights are then frozen.</p> </li> <li> <p>Apply an augmentation to the input image. The same augmentation of the image is used for both the student and the teacher networks. In most other SSL methods, different augmentations would be used. SEED reports better performance when using the same augmentation. This may be because trivial solutions are avoided by the pretraining of the teacher network.</p> </li> <li> <p>Input the image to both the student and teacher networks to get two vector representations: \(Z^S\) and \(Z^T\).</p> </li> <li> <p>Add teacher vector \(Z^T\) to the instance queue \(D\) which is a fixed size FIFO queue that persists between training batches. Self-supervised learning in general benefits from a large number of negative examples.</p> </li> <li> <p>Apply the self-supervised SEED loss, using the student and teacher vectors, and the instance queue. The student and teacher vectors are each compared to every embedding in the queue, to produce two similarity vectors. A cross-entropy loss is applied between the similarity vectors The student network is trained to produce vectors that have the same similarities as the teacher. We will further explain the loss used in SEED.</p> </li> </ol> <h2 id="loss">Loss</h2> <p>In self-supervised learning, a supervised objective is formed from the input rather than human annotations. In this case, the supervised objective is predicting the current image representation from a queue containing the current representation and negative examples. Knowledge distillation is applied with respect to this objective. The scores from applying the softmax to the teacher similarity vector form the soft label.</p> <p>The cross-entropy loss is used like the contrastive InfoNCE loss in SSL. The student vector is pushed towards the teacher vector and away from the vectors in the queue. However, some of the negative vectors are closer than others. The student network is also trained to match this information. This is where the dark knowledge of KD is applied.</p> <p>Unlike the InfoNCE loss, there are no hard positive and negative examples in this objective. The teacher network creates a soft probability distribution. Each example is assigned a continuous score between 0 and 1 that indicates how positive the example is. SEED can be viewed as a <em>soft contrastive learning</em> method.</p> \[\begin{align} \mathcal{L}_{SEED} &amp;= - \sum_i^N \textbf{p}^T(\textbf{x}_i; \theta_T, \textbf{D}^+) * \log \textbf{p}^S(\textbf{x}_i; \theta_S, \textbf{D}^+) \\ &amp;= - \sum_i^N \sum_j^{K + 1} \frac{\exp(\textbf{z}_i^T * \textbf{d}_j / \tau^T)}{\sum_{d\sim\textbf{D}^+}\exp(\textbf{z}_i^T * \textbf{d} / \tau^T)} * \log \frac{\exp(\textbf{z}_i^S * \textbf{d}_j / \tau^S)}{\sum_{d\sim\textbf{D}^+}\exp(\textbf{z}_i^S * \textbf{d} / \tau^S)} \end{align}\] <p>For each example in the batch (size \(N\)), two similarity functions are applied: one using the teacher network \(p^T\) and one using the student network \(p^S\). The similarity function is applying an inner product and softmax with the vectors to the instance queues. This produces a probability distribution with more probability on examples in the queue that are close to the input. Since we want these probability distributions to match, a cross entropy loss is applied between the two probability distributions.</p> \[\mathcal{L}_{cross-entropy} = \sum_{i} y_i * \log(\hat{y}_i)\] <p>Referring to the formula for cross-entropy. \(\textbf{p}^T(..)\) corresponds to the label \(y_i\). In classification, \(y_i\) would be binary or a one-hot encoded vector. In this case, \(\textbf{p}^T(..)\) is a score between 0 and 1. As in KD, this is a soft label. With hard labels and standard contrastive learning, the scores would be binary with a 1 for the current datapoint. \(\textbf{p}^S(..)\) corresponds to the prediction \(\hat{y}_i\). Here the prediction is the student similarity score. We want the student to produce similarity scores matching the teacher.</p> <h2 id="seed-vs-moco">SEED vs MoCo</h2> <p>SEED is trained very similarly to MoCo. The differences are the lack of momentum weight updates and the soft contrastive loss.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/distillation_ssl/seed2-480.webp 480w,/blog/assets/img/blog/distillation_ssl/seed2-800.webp 800w,/blog/assets/img/blog/distillation_ssl/seed2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/distillation_ssl/seed2.png" width="700" height="auto" alt="SEED training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SEED training</figcaption> </figure> <h1 id="self-supervised-vs-supervised-knowledge-distillation">Self-supervised vs Supervised Knowledge Distillation</h1> <p>SEED or self-supervised distillation in general does not aim to replace supervised knowledge distillation. The authors report their best results when training models with both self-supervised and supervised knowledge distillation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/distillation_ssl/s_vs_sl_kd-480.webp 480w,/blog/assets/img/blog/distillation_ssl/s_vs_sl_kd-800.webp 800w,/blog/assets/img/blog/distillation_ssl/s_vs_sl_kd-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/distillation_ssl/s_vs_sl_kd.png" width="900" height="auto" alt="Self-supervised KD with Supervised KD" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Self-supervised KD with Supervised KD</figcaption> </figure> <p>SEED allows for more effective self-supervised training of smaller models. It is better to train a large model with SSL and distill it to a small model than to train the small model directly. After SEED pretraining, the model can be fine-tuned with supervised knowledge distillation with the downstream task. In this step, the student is initialized from the self-supervised KD trained model, instead of initializing from scratch.</p> <h1 id="conclusion">Conclusion</h1> <p>Self-supervised knowledge distillation allows the impressive gains of large SSL models to be transferred to smaller neural networks. This allows for more applications of these models. We can even view knowledge distillation as a form of self-supervised learning. Hard labels are not used in SSL. The soft labels provide self-supervision since they are produced from the data.</p> <p>SEED essentially adapts momentum contrast to be used as knowledge distillation. An interesting future direction would be adapting other SSL methods such as SimCLR to be used as knowledge distillation. Nearly every contrastive SSL method can be adapted in this way.</p>]]></content><author><name></name></author><category term="paper-review"/><category term="self-supervised-learning"/><category term="knowledge-distillation"/><category term="computer-vision"/><summary type="html"><![CDATA[Self-supervised learning (SSL) methods have been shown to effectively train large neural networks with unlabeled data. These networks can produce useful image representations that can exceed the performance of supervised pretraining on downstream tasks. However, SSL is not effective with smaller models. This limits applications where computational power is limited, such as edge devices. Knowledge distillation (KD) is a popular method to train a smaller student network from a larger and more powerful teacher network. The SEED paper by Fang et al., published in ICLR 2021, applies knowledge distillation to self-supervised learning to pretrain smaller neural networks without supervision. In this post, we will discuss self-supervised learning and knowledge distillation and how they are unified in SEED.]]></summary></entry><entry><title type="html">Self-Supervised Learning  -  Getting more out of data</title><link href="https://rohitbandaru.github.io/blog/blog/2021/Self-Supervised-Learning/" rel="alternate" type="text/html" title="Self-Supervised Learning  -  Getting more out of data"/><published>2021-08-14T00:00:00+00:00</published><updated>2021-08-14T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/blog/2021/Self-Supervised-Learning</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/blog/2021/Self-Supervised-Learning/"><![CDATA[<p>Yann LeCun <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">describes</a> self-supervised learning as the next big challenge in the field of AI. How does it work? Self-supervised learning (SSL) is a specific type of unsupervised learning. It aims to learn from large datasets of unlabeled data to enable building more robust models in different domains such as vision and NLP.</p> <p>For many computer vision problems, it is a common practice to pretrain the model on a supervised learning task. For example, there are <a href="https://keras.io/api/applications/">many</a> neural networks that are pretrained to do image classification on ImageNet. However, self-supervised learning has recently been shown to outperform supervised pretraining learning on certain tasks. SSL is an active area of research with heavy involvement from top AI labs in Google, Facebook, Deepmind, and academia. Rather than focusing on the details of SSL architectures, we will explore the intuitions on why it works and what is needed.</p> <h1 id="code">Code</h1> <p>In order to make it easy to directly interact with SSL, I wrote a Colab notebook showing a few algorithms. This notebook demonstrates transfer learning on CPC, SwAV, and SimCLR pretrained models on the CIFAR10 classification task. This uses PyTorch Lightning’s implementations of these algorithms.</p> <p><a href="include.url"> <img src="../../../assets/img/colab.svg" alt="Open In Colab"/> </a></p> <p>We are experimenting with the simple example of pretraining on ImageNet and evaluating on CIFAR10 classification. SSL can be effective on other datasets and learning tasks (object detection, segmentation, etc.), but these won’t be the focus of this post.</p> <h1 id="motivations">Motivations</h1> <h2 id="data">Data</h2> <p>Self-supervised learning does not need labels. The amount of unlabeled data generally far exceeds the amount of labeled data. SSL can leverage large amounts of unlabeled data to build powerful models. Although most research does not use datasets larger than ImageNet, there are real world applications of using larger unlabeled datasets. For example, Facebook/Meta can train the <a href="https://ai.facebook.com/blog/seer-the-start-of-a-more-powerful-flexible-and-accessible-era-for-computer-vision/">SEER</a> model on billions of Instagram images.</p> <h2 id="generalizability">Generalizability</h2> <p>If you train a model on image classification, it may not perform as well on non-classification tasks. This is because only part of the image’s information is needed to classify it. A self-supervised learning algorithm may be able to use more of the information in the data.</p> <p>The reason for the generalization gap is that the classification task does not always require a strong understanding of the object. For example, if you trained a supervised model to classify dog breeds, it may only look at the texture and color of the dog’s fur. In order to classify the breeds, the network may not need to understand other characteristics of the dog, such as size and facial features. This model would then not generalize well if you want to add a new dog breed with an indistinctive skin pattern. It will also not generalize well to new tasks like classifying the size or shape of the dog.</p> <h2 id="better-performance">Better Performance</h2> <p>It is common to think that unsupervised / self-supervised learning is only useful when you lack labels to do supervised learning. However, these approaches can actually increase performance compared to a fully supervised approach. The ability to learn more accurate and robust models is what gives self-supervised learning the potential to shift the field of AI.</p> <p>In research, there are comparisons between training on ImageNet images and labels with supervised learning and ImageNet with only images for self-supervised learning. Although the motivation for SSL is often framed as being able to use more data, in this case, the size of the dataset is the same. The ability to use larger unlabeled datasets is just a side benefit of SSL.</p> <h1 id="vision-vs-nlp">Vision vs NLP</h1> <p>Self-supervised learning has been long applied in NLP, but as Yann LeCun and Ishan Misra point <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">out</a>, it is much harder to apply to vision. In NLP, language models are often trained with self supervision. Given a some text, you can mask a word and try to predict it given the rest of the text. There is a limited vocabulary, so you can assign a probability to each word. This is the basis of many popular NLP methods.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/self_supervised_learning/nlp-480.webp 480w,/blog/assets/img/blog/self_supervised_learning/nlp-800.webp 800w,/blog/assets/img/blog/self_supervised_learning/nlp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/self_supervised_learning/nlp.png" class="img-fluid mx-auto d-block" width="400" height="auto" alt="Predicting masked words in NLP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Predicting masked words in NLP</figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/self_supervised_learning/image_patch-480.webp 480w,/blog/assets/img/blog/self_supervised_learning/image_patch-800.webp 800w,/blog/assets/img/blog/self_supervised_learning/image_patch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/self_supervised_learning/image_patch.png" class="img-fluid mx-auto d-block" width="300" height="auto" alt="Image SSL with patches" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Predicting patches of an image is much harder.</figcaption> </figure> <p>The analogue for vision is to mask a patch of an image and try to fill it in. However, because there is an intractable number of possible ways to fill in an image, you can’t compute a probability for each one. There can also be a large number of possible solutions. For example, in the image above, there is many facial expressions the dog could have. The NLP approach is straight forward but cannot be directly applied to vision.</p> <h1 id="pretext-task">Pretext Task</h1> <p>The earlier approaches to self-supervised learning focused on training the network on a pretext task. This task would not require labels in the label. The labels will be made up through the task. In <a href="https://arxiv.org/abs/2012.01985">RotNet</a>, each image is rotated by 0, 90, 180, or 270 degrees, and a network is trained to predict the rotation. In <a href="https://arxiv.org/abs/1603.09246">Jigsaw</a>, the image is split up into patches and scrambled like a jigsaw puzzle. A network is then trained to solve the puzzle by predicting the permutation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/self_supervised_learning/rotnet-480.webp 480w,/blog/assets/img/blog/self_supervised_learning/rotnet-800.webp 800w,/blog/assets/img/blog/self_supervised_learning/rotnet-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/self_supervised_learning/rotnet.png" width="100%" height="auto" alt="RotNet, SSL by predicting rotations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The problem with pretext task-based SSL is the same as supervised learning. There can be shortcuts to achieve high accuracy on the task. There have been attempts to avoid this. For example, in Jigsaw, each path is randomly cropped, so the task can’t be solved by simply lining up edges. However, the limitations still exist regardless, so more recent research has focused on contrastive learning.</p> <h1 id="contrastive-learning">Contrastive Learning</h1> <p>A neural network outputs a vector representation for every image. The goal of contrastive learning is to push these vectors closer for similar images and pull them apart unrelated images. This in different ways in different research papers.</p> <h2 id="cpc"><a href="https://arxiv.org/abs/1807.03748">CPC</a></h2> <p>Contrastive Predictive Coding is a method developed by Deepmind. It is a generic approach that can be applied to any data modality. In the paper, it is applied to images, audio, and text. It is a very general framework with two main components: an encoder, and an autoregressive model. These can be anything and are designed to fit the domain.</p> <p>The encoder simply encodes the data into a lower-dimensional vector \(z_t\). This can be any model. For images, this can be a convolutional neural network.</p> <p>Autoregressive models the variables in the data are given an order. In images, the pixels can be ordered from left to right and top to bottom. We can imagine unrolling each datapoint (ex: image, audio clip) into a list. We can call each element of this list an observation. CPC encodes a sequence of observations \(X\) into a sequence of encodings \(Z\).</p> \[X = [x_1, x_2, x_3, x_4 ... x_N]\\ Z = [z_1, z_2, z_3, z_4 ... z_N]\\ z_t = g_{enc}(x_t)\] <p>The prediction of an observation in the sequence depends only on the previous observations. This similar to predicting the future from the past in a time series. In CPC, the autoregressive model is used to generate context vectors from the encodings \(z_t\). Context vector \(c_t\) is a function of encodings \(z_{\leq t}\), but not any encoding after \(z_t\). Note that the autoregressive model is trying to predict the encodings of the observations, but not the observations themselves. The architecture of this autoregressive model depends on the application.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/self_supervised_learning/cpc-480.webp 480w,/blog/assets/img/blog/self_supervised_learning/cpc-800.webp 800w,/blog/assets/img/blog/self_supervised_learning/cpc-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/self_supervised_learning/cpc.png" width="100%" height="auto" alt="CPC" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">CPC applied to audio, \(g_{enc}\) is the encoder, \(g_{ar}\) is the autoregressive model</figcaption> </figure> <p>With these two models, we can generate an encoding of the data and context vectors. These vectors can be used as representations of the data. But how are these models trained? The self-supervised task is essentially predicting the input from the context. For example, given \(c_t\), we want to be able to go backwards and identify that it was generated from \(x_t/z_t\). The models are trained on a contrastive InfoNCE loss.</p> \[\begin{equation} \mathcal{L}_{\mathrm{InfoNCE}} = -\mathbb{E} \left[ \mathrm{log} \frac{ s(x, y) } { \sum_{y_j} s(x,y_j) } \right] \end{equation}\] <p>\(x\) is the sample we are trying to predict. \(c\) is the correct context. \(c_j\) are the context vectors for the negative samples. The negative samples come from other observations of the same datapoint and other datapoints in the batch. We want to maximize \(s(x,c)\) and minimize the sum of \(s(x, y_j)\). This is contrastive in that we are pushing \(y\) to be close to \(x\), and all other \(y_j\) to be far from \(x\).</p> \[\begin{equation} f_k(x_{t+k},c_t) = \mathrm{exp} \left( (g_{enc}(x_{t+k}))^TW_kc_t \right) = \mathrm{exp} \left( z_{t+k}^TW_kc_t \right) \end{equation}\] <p>The \(s\) function is modeled by \(f_k\) a log bilinear model. \(W_k\) is linear transforms the context vector, which can then be compared with the encoding \(z\).</p> <p>To apply this to vision, the image is split up into 7x7 patches (with 50% overlap) which will be considered the observations. Each patch is encoded by a CNN (ResNet without pretraining). If the encoding returns at 1024 dimensional vector, the encoded image will have a size of 7x7x1024. An autoregressive model (<a href="https://arxiv.org/abs/1606.05328">PixelCNN</a> or <a href="https://arxiv.org/abs/1601.06759">PixelRNN</a>) is applied to the encodings of the patches. For 1D data like audio, an RNN/LSTM scan be used. The self-supervised task in this case is predicting which patch generated each context vector. Refer to the PixelRNN paper for more information on autoregressive models and PixelCNN. The final representation is computed by mean pooling the encodings into a single 1024 dimensional vector. This can then be used for downstream tasks, like image classification.</p> <p>Why do we need the autoregressive model? We could optimize the InfoNCE loss using the 7x7 encodings. The self supervised task here is predicting the next context vector given a sequence of context vectors. This is similar to predicting the next patch of an image given all the previous patches. But rather predict the patch, which as we discussed is too difficult, we just predict a lower dimensional vector. Without this autoregressive constraint, we are just optimizing for generating unique embeddings for each patch and ignoring the relation between the patches. The InfoNCE loss is just ensuring that the predictions are correct.</p> <p>Why not just mask out the current context / observation? The architecture for this may be a masked fully connected layer that learns the context vector for each observation, while excluding the connection to the observation itself. Or there could be two PixelCNNs, one from the left and one from the right. We can then concatenate these two context vectors and possibly add additional neural network layers on top of it. Both methods would be more computationally expensive and complex, but likely still feasible. This would be bidirectional model for images similar to <a href="https://arxiv.org/abs/1810.04805">BERT</a>. This idea may be explored in other research papers or, it may be an open idea to try.</p> <h2 id="simclr"><a href="https://arxiv.org/abs/2002.05709">SimCLR</a></h2> <p>SimCLR is a method from Google Brain which takes a different approach for self-supervised learning of image representations. The basis of SimCLR is image augmentations. Image augmentation has long been used in supervised learning. The augmentations are transformations applied to the image and cropping, color change, and rotation. The idea is that these transformations do not change the content of the image and the network will learn to ignore and be invariant to these transformations. In supervised learning, data augmentation is used to just increase the size of the dataset for a supervised task like classification. Many SSL methods including SimCLR make invariance to the augmentation the actual learning objective. The augmented images are fed into an encoder to get the representation. These representations are then learned to be close of augmentations of the same image.</p> <p>However, the problem with just comparing within the same image is collapse. The network would learn the trivial solution of a constant vector for all representations (ex: a vector of all zeros). This would maximize the similarity between augmentations but obviously not contain any useful images. We need negative samples to minimize similarity with. In SimCLR, the negative samples are augmentations of other images from the same training batch. The assumption made here is that the other images are unrelated to the current image and the representations should be far apart.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/self_supervised_learning/simclr_arch-480.webp 480w,/blog/assets/img/blog/self_supervised_learning/simclr_arch-800.webp 800w,/blog/assets/img/blog/self_supervised_learning/simclr_arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/self_supervised_learning/simclr_arch.png" width="500" height="auto" alt="SimCLR architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The architecture of SimCLR. Diagram by Author, but dog images from SimCLR paper</figcaption> </figure> <p>Why do we need a projection head? It would not change the architecture much by optimizing the similarity losses on the representations themselves. The encoder may even include the same fully connected layers that would have been in the projection head. The projection head allows for a more complex and nonlinear similarity relationship between the encodings. Without it, the representations would have to have a high cosine similarity. This may restrict the expressivity of the vectors. The projection head can also ignore some information in the representations. For example, SimCLR may train to make the representations invariant to rotations. The rotation angle may be encoded in the representation but ignored by the projection head. If the rotation is encoded in the first 5 values of the vector, the projection MLP may have zero weights for those values. This may be desirable in a variant of the architecture in which the self-supervised learning happens simultaneously with a downstream task. The SimCLR architecture itself has no reason to include unnecessary information in the representation. It is unclear whether having “extra” information in the representation is desirable or not.</p> <p>Projection heads are very common in self-supervised learning. The autoregressive model in CPC can be viewed as a projection head.</p> <p>Aggressive augmentation yields the best results. This means applying multiple augmentations at a time. This makes the contrastive learning more challenging and forces the network to learn more about the image. The augmentations also avoid trivial solution to the contrastive objective. Without cropping, the network can match two augmented images by their local features (edges in the same place), instead of learning global features. Without color distortion, images can be matched by their color distribution. These augmentations can be composed with others, such as rotation and blur.</p> \[\begin{equation} \ell_{i,j} = \log{\frac{\exp(\mathrm{sim}(z_i,z_j)/\tau))}{\sum_{k=1}^{2N}\mathbb{1}_{[k\neq i]}\exp(\mathrm{sim}(z_i,z_k)/\tau)}} \end{equation}\] <p>The loss is referred to as NT-Xent (the normalized temperature-scaled cross entropy loss). The similarity function \(s\) can simply be cosine similarity (\(\frac{u^\top v}{\|u\|\|v\|}\)). This loss is similar to the InfoNCE loss. The main difference is the temperature \(\tau\). The temperature essentially controls how strongly should attract and repel the other vectors in the loss.</p> <h2 id="scaling">Scaling</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/self_supervised_learning/scaling-480.webp 480w,/blog/assets/img/blog/self_supervised_learning/scaling-800.webp 800w,/blog/assets/img/blog/self_supervised_learning/scaling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/self_supervised_learning/scaling.png" width="500" height="auto" alt="SSL scaling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Self-supervised learning is often evaluated on ImageNet classification. The projection head is replaced with a linear layer. The network is then trained to classify ImageNet with the encoder weights frozen. The encodings learned with self-supervision must be useful enough for a linear layer to classify them.</p> <p>An interesting property of self-supervised trained encoders, is how the scale in terms of depth and width. We see that only SimCLR(4x) is able to match the accuracy of fully supervised learning. “4x” means the network is 4 times as wide and as deep. It is not necessarily a bad thing that SSL requires a much larger network for ImageNet classification. This likely means the network is learning more information from the data than what is needed for supervised learning. Although this doesn’t help with ImageNet classification, the vectors may be more effective in other downstream tasks.</p> <p>One issue with SimCLR is its reliance on huge batch sizes. The best results come from a batch size of 4096. It needs many negative samples to be effective. This makes the network inefficient to train. Other approaches attempt to address this problem.</p> <h2 id="byol"><a href="https://arxiv.org/abs/2006.07733">BYOL</a></h2> <p>BYOL is a paper from Deepmind that aims to remove the need for negative samples. There are two networks: a target network and an online network. The target network’s weights are an exponential moving average of the online encoder. Similar to SimCLR, augmented versions of an image are passed through the encoders. Unlike SimCLR, the loss does not use negative examples so there is no need for large batch sizes. There is a projection head on top of the online encoder. The online encoder is used for downstream tasks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/self_supervised_learning/byol-480.webp 480w,/blog/assets/img/blog/self_supervised_learning/byol-800.webp 800w,/blog/assets/img/blog/self_supervised_learning/byol-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/self_supervised_learning/byol.png" width="100%" height="auto" alt="BYOL architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">BYOL architecture</figcaption> </figure> <p>Bootstrapping is a poorly defined word used in machine learning. It can mean simultaneously optimizing two objectives that depend on each. In BYOL, that refers to the two encoders.</p> <p>BYOL is able to learn useful representations without collapse because only the parameters of the online encoder are optimized. The online encoder can’t learn to output a constant because it is following the representations of the target encoder. The bootstrapping ensures that the trivial solution is avoided.</p> <p>BYOL is a non-contrastive method of SSL. However one criticism of BYOL is that batch normalization causes implicit contrastive learning by leaking information between batch elements. However, in a <a href="https://arxiv.org/pdf/2010.10241.pdf">follow up paper</a>, the authors show that replacing batch normalization with group normalization and weight standardization leads to comparable performance.</p> <h1 id="clustering">Clustering</h1> <p>Clustering is an important class of unsupervised learning algorithms. Although more often used outside of deep learning, clustering can be applied to self supervised learning. Feature vectors can be clustered. Clusters can indicate a group of related images. In this sense clusters are similar to classes and can be used as labels in SSL.</p> <h2 id="deepcluster"><a href="https://arxiv.org/abs/1807.05520">DeepCluster</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/self_supervised_learning/deepcluster-480.webp 480w,/blog/assets/img/blog/self_supervised_learning/deepcluster-800.webp 800w,/blog/assets/img/blog/self_supervised_learning/deepcluster-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/self_supervised_learning/deepcluster.png" width="100%" height="auto" alt="DeepCluster Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">DeepCluster algorithm</figcaption> </figure> <p>DeepCluster trains a neural network in two alternating steps: clustering and classification. In the clustering step, each image is assigned a cluster as a pseudolabel by clustering the feature vectors from the network. K-means is used for clustering. There are \(k\) clusters of the same dimension as the feature vectors. The network is then trained to predict the clusters from the images. After training on this classification objective, the features improve. The dataset is reclustered with better clusters. This iterative training procedure improves the clusters and the representations.</p> <p>The main problem with DeepCluster is that it requires periodically clustering the entire dataset. This limits this method in scaling to extremely large datasets. This is addressed by SwAV with an online approach to clustering based SSL.</p> <h2 id="swav"><a href="https://arxiv.org/abs/2006.09882">SwAV</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/self_supervised_learning/swav-480.webp 480w,/blog/assets/img/blog/self_supervised_learning/swav-800.webp 800w,/blog/assets/img/blog/self_supervised_learning/swav-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/self_supervised_learning/swav.png" width="500" height="auto" alt="SwAV" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SwAV</figcaption> </figure> <p>SwAV extends on DeepCluster to be online, while also taking inspiration from contrastive SSL methods. Two augmentations of an image are passed to an encoder. These representations are then assigned prototypes. There are K prototypes, which are vectors of the same representation as the encoding.</p> <h1 id="conclusion">Conclusion</h1> <p>There are many approaches to self supervised learning, however there are common elements. There are contrastive losses, data augmentation, bootstrapping, projection heads, and sometimes negative samples.</p>]]></content><author><name></name></author><category term="computer-vision"/><category term="self-supervised-learning"/><summary type="html"><![CDATA[Yann LeCun describes self-supervised learning as the next big challenge in the field of AI. How does it work? Self-supervised learning (SSL) is a specific type of unsupervised learning. It aims to learn from large datasets of unlabeled data to enable building more robust models in different domains such as vision and NLP.]]></summary></entry><entry><title type="html">Domain Adaptation</title><link href="https://rohitbandaru.github.io/blog/blog/2021/Domain-Adaptation/" rel="alternate" type="text/html" title="Domain Adaptation"/><published>2021-08-09T00:00:00+00:00</published><updated>2021-08-09T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/blog/2021/Domain-Adaptation</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/blog/2021/Domain-Adaptation/"><![CDATA[<p>Machine learning performance depends on the dataset that it is trained on. Datasets are imperfect, so problems in the data affect the models. One type of problem is domain shift. This means that a model trained to learn a task on one dataset, may not be able to perform the same task on a slightly different dataset.</p> <p>Say you train a model to detect dogs in outdoor settings like public parks. It may perform very well on test images of dogs in outdoor places. However, that model may not function well when trying to detect dogs indoors, although the task itself is identical. This is a problem because the background of the image should not matter since you are just trying to detect dogs. We will explore four different research papers that address this problem.</p> <h1 id="vocabulary">Vocabulary</h1> <p>There are two datasets: a source dataset and a target dataset. The dataset that the model is trained on is the source dataset. The target dataset is the one that it will be tested on.</p> <p>For domain generalization, a similar problem, the target dataset is not available during training. The network is trained on the source dataset to not overfit to the domain-specific features.</p> <p>In domain adaptation, both the source and target datasets are available during training, but labels for the target dataset are not always available. For unsupervised domain adaptation, there are no labels available for the target dataset during training time. Semi-supervised domain adaptation involves a few labeled examples from the target dataset. With supervised domain adaptation, all the data from both the source and target datasets have labels.</p> <p>Unsupervised domain adaptation is the most commonly studied problem, as it has the most applications. Supervised DA can be useful when you have a labeled dataset, but it is too small to directly train on.</p> <p>These methods can be applied to many ML problems. However, a common application is image classification. I will focus on image classification on two common benchmark datasets: MNIST and SVHN. A model trained on handwritten digits (MNIST) often performs poorly on printed house number digits (SVHN).</p> <h1 id="adversarial-methods">Adversarial Methods</h1> <p>The most common approaches to the domain adaptation method follow an adversarial approach. For some context, I would suggest reading about <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29">Generative Adversarial Networks (GANs)</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/domain_adaptation/framework-480.webp 480w,/blog/assets/img/blog/domain_adaptation/framework-800.webp 800w,/blog/assets/img/blog/domain_adaptation/framework-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/domain_adaptation/framework.png" width="100%" height="auto" alt="Framework for Adversarial Domain Adaptation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Framework for Adversarial Domain Adaptation</figcaption> </figure> <p>There are two encoders, which learn to produce a vector representation of each input. There is a classifier to classify the inputs and a discriminator that is trained to differentiate between the datasets. The goal is to eliminate differences in the domain from the encodings. This is similar to the GAN objective in that we want the encoders to fool the discriminator by generating encodings that are difficult to differentiate. However, this needs to be done such that the classifier is also effective for both datasets. The same classifier can then be applied to both datasets.</p> <p>There are many approaches to this with different training methods, architectures, and losses. The high-level goal is consistent. We want the encoders to generate encodings that contain the useful information needed for classification but remove the shift in domains.</p> <p>The key difference between the many algorithms is what the discriminator is and how it is trained. In simple cases, it is just an additional loss term. For example, maximum mean discrepancy (MMD) measures the difference between the encodings of the source and target datasets. Training the networks while minimizing the discrepancy can reduce domain shift. This may be useful for simple DA problems but does not work well for larger disparities.</p> <h2 id="adda"><a href="https://arxiv.org/abs/1702.05464">ADDA</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/domain_adaptation/adda-480.webp 480w,/blog/assets/img/blog/domain_adaptation/adda-800.webp 800w,/blog/assets/img/blog/domain_adaptation/adda-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/domain_adaptation/adda.png" width="100%" height="auto" alt="The steps of ADDA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The steps of ADDA</figcaption> </figure> <p>Adversarial Discriminative Domain Adaptation (ADDA) applies a simple approach to discriminative DA. There is only one encoder shared between the source and target datasets. The networks are trained in two steps.</p> <ol> <li> <p>The encoder and classifier are first trained to achieve high classification accuracy on the source dataset.</p> </li> <li> <p>The encoder is trained with the discriminator to lose domain discriminability. The discriminator is trained to classify the two domains with an adversarial loss. The encoder is trained with the negation of this loss since it is adversarial with respect to the discriminator. This negative is done through gradient reversal, which means in backpropagation, the gradients are negated before going to the encoder.</p> </li> </ol> <p>One major shortcoming of this approach is that the classification performance can be lost or forgotten in the adaptation step. This is because the labels are not used in this step.</p> <h2 id="dann"><a href="https://arxiv.org/abs/1505.07818">DANN</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/domain_adaptation/dann-480.webp 480w,/blog/assets/img/blog/domain_adaptation/dann-800.webp 800w,/blog/assets/img/blog/domain_adaptation/dann-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/domain_adaptation/dann.png" width="100%" height="auto" alt="DANN" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">DANN</figcaption> </figure> <p>Domain-Adversarial Training of Neural Networks (DANN) is very similar to ADDA. Rather than have a separate adaptation step, the domain discriminator is trained alongside the classier. A gradient reversal layer is used because the domain discriminator and the classier have adversarial loss functions. This allows classification and discrimination to be trained together and avoid the network from forgetting the task.</p> <h1 id="image-translation">Image Translation</h1> <p>Another approach to addressing the domain gap is to convert examples from one domain to another. An example of this is transforming street-view digits (SVHN) to look like handwritten MNIST (digits). After this translation, you can apply an MNIST trained image classifier. The architectures are more complex because, in addition to the main task (image classification), the networks must translate images to and from the source and target domains.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/domain_adaptation/translation-480.webp 480w,/blog/assets/img/blog/domain_adaptation/translation-800.webp 800w,/blog/assets/img/blog/domain_adaptation/translation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/domain_adaptation/translation.png" width="100%" height="auto" alt="SVHN DA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="image-to-image-translation"><a href="https://arxiv.org/abs/1712.00479">Image to Image translation</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/domain_adaptation/i2i-480.webp 480w,/blog/assets/img/blog/domain_adaptation/i2i-800.webp 800w,/blog/assets/img/blog/domain_adaptation/i2i-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/domain_adaptation/i2i.png" width="100%" height="auto" alt="I2I" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Like the adversarial methods Image to Image Translation (I2I) aims to learn a domain invariant encoding (Z) for the images. There are six networks in this architecture: the source encoder, source decoder, target encoder, target decoder, domain discriminator, and task network (ex: classifier). The decoders aim to reconstruct the images from the encoding. This also includes adversarial learning with the domain discriminator.</p> <p>The network is trained on a weighted combination of six different losses. The paper studies which combination of losses yields the best performance.</p> <ol> <li> <p>Qc is the classification loss on the source domain. We cannot get this loss for the target domain since there are no labels. However, the loss can be extended to include the target domain if labels exist.</p> </li> <li> <p>Qid is the loss of encoding an image and decoding it back into the same domain. Encoding an image into Z and decoding it back to the original domain should ideally return the same image. This loss can be the L1 norm of the difference between the original and decoded image.</p> </li> <li> <p>Qz is the domain discriminator’s loss. This is similar to ADDA in that it is trying to determine the domain of the encoding. We want this loss to increase as the encodings improve.</p> </li> <li> <p>Qtr is another discrimination loss in which the image is translated into the other domain before going to the domain discriminator.</p> </li> <li> <p>Qcyc is the cycle consistency loss. This loss is similar to Qid. The difference is that the images are decoded in the other domain before being encoding and decoded in the original domain. The image from the source domain is encoded into Z. This is decoded into the target domain and encoded back to Z. This is then decoded into the source domain and compared with the original image. A loss with the source and target switched is also applied. This aims to ensure encodings from similar images in different domains have similar encodings.</p> </li> <li> <p>Qtrc is similar to Qcyc, but instead of decoding back into the original domain, the encoding is classified. Unlike Qcyc, this is not symmetric since it involves labels. An image from the source domain is translated into the target domain and then classified.</p> </li> </ol> <h2 id="cycada"><a href="https://arxiv.org/abs/1711.03213">CyCADA</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/domain_adaptation/cycada-480.webp 480w,/blog/assets/img/blog/domain_adaptation/cycada-800.webp 800w,/blog/assets/img/blog/domain_adaptation/cycada-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/domain_adaptation/cycada.png" width="100%" height="auto" alt="CyCADA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>CyCADA is similar to I2I. Many of the I2I losses and networks have counterparts here. The main difference is that the target images are not translated to the source domain. Also, the GAN losses can be applied to both the images and the encodings.</p> <p>The source images are translated into the target domain. They are translated back into the source domain to apply the cycle consistency loss (L1 difference with the original image).</p> <p>The fs network is trained on the supervised learning task in the source domain. The semantic consistency loss ensures that the features from this network remain close before and after translation into the target domain. This ensures that the images retain the semantic information after translation.</p> <p>A GAN loss is then applied to the images and features (from fT) for the translated images and the target images. This loss is needed to train the translations to be similar to the target domain. There are two GAN losses to ensure that both the images and the features are similar.</p> <p>Finally, a task loss is applied to the translated images. This applies the task to the original target images.</p> <h1 id="other-domains">Other Domains</h1> <p>Image classification is the primary problem used to benchmark domain adaptation methods. However, domain adaptation can also be applied to other computer vision problems, such as image segmentation. It can also be applied in different research areas, such as natural language processing (NLP).</p> <p>One particularly interesting application of domain adaptation is self-driving cars and robotics. It is a common practice to train deep neural networks for these applications using data from simulated environments. It is much easier to collect large amounts of data in a simulation rather than in the real world. However, in order for a model trained on simulation data to function in a real-world environment, domain adaptation is often required to achieve good performance.</p> <p>There are also many variants to the problem, including few-shot domain adaptation, domain generalization, and multiclass domain adaptation.</p> <h1 id="conclusion">Conclusion</h1> <p>There are several approaches to domain adaptation but they often share some common characteristics. Adversarial learning with a domain discrimination network is common. There is also a lot of work using image to image translation with a cycle consistency loss. Apply domain adaptation to new problems will likely involve some combination of these components.</p> <h1 id="references">References</h1> <p>[1] Long, Mingsheng, et al. “Learning transferable features with deep adaptation networks.” International conference on machine learning. PMLR, 2015.</p> <p>[2] Eric Tzeng et al. “Adversarial discriminative domain adaptation”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017, pp. 7167-7176.</p> <p>[3] Yaroslav Ganin and Victor Lempitsky. “Unsupervised domain adaptation by backpropagation”. In: arXiv preprint arXiv:1409.7495 (2014).</p> <p>[4] Zak Murez et al. “Image to image translation for domain adaptation”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018, pp. 4500-4509.</p> <p>[5] Hoffman, Judy, et al. “Cycada: Cycle-consistent adversarial domain adaptation”. In: arXiv preprint arXiv:1711.03213 (2017).</p>]]></content><author><name></name></author><category term="computer-vision"/><summary type="html"><![CDATA[Machine learning performance depends on the dataset that it is trained on. Datasets are imperfect, so problems in the data affect the models. One type of problem is domain shift. This means that a model trained to learn a task on one dataset, may not be able to perform the same task on a slightly different dataset.]]></summary></entry><entry><title type="html">Pruning Neural Networks</title><link href="https://rohitbandaru.github.io/blog/blog/2020/Neural-Network-Pruning/" rel="alternate" type="text/html" title="Pruning Neural Networks"/><published>2020-09-01T00:00:00+00:00</published><updated>2020-09-01T00:00:00+00:00</updated><id>https://rohitbandaru.github.io/blog/blog/2020/Neural-Network-Pruning</id><content type="html" xml:base="https://rohitbandaru.github.io/blog/blog/2020/Neural-Network-Pruning/"><![CDATA[<p>Much of the success of deep learning has come from building larger and larger neural networks. This allows these models to perform better on various tasks, but also makes them more expensive to use. Larger models take more storage space which makes them harder to distribute. Larger models also take more time to run and can require more expensive hardware. This is especially a concern if you are productionizing a model for a real-world application.</p> <p>Model compression aims to reduce the size of models while minimizing loss in accuracy or performance. Neural network pruning is a method of compression that involves removing weights from a trained model. In agriculture, pruning is cutting off unnecessary branches or stems of a plant. In machine learning, pruning is removing unnecessary neurons or weights. We will go over some basic concepts and methods of neural network pruning.</p> <h1 id="remove-weights-or-neurons">Remove weights or neurons?</h1> <p>There are different ways to prune a neural network.</p> <ol> <li>You can prune weights.</li> </ol> <p>This is done by setting individual parameters to zero and making the network sparse. This would lower the number of parameters in the model while keeping the architecture the same.</p> <ol> <li>You can remove entire nodes from the network.</li> </ol> <p>This would make the network architecture itself smaller, while aiming to keep the accuracy of the initial larger network.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/pruning/weights_vs_neurons-480.webp 480w,/blog/assets/img/blog/pruning/weights_vs_neurons-800.webp 800w,/blog/assets/img/blog/pruning/weights_vs_neurons-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/pruning/weights_vs_neurons.png" width="100%" height="auto" alt="pruning weights vs nodes" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Visualization of pruning weights/synapses vs nodes/neurons</figcaption> </figure> <p>Weight-based pruning is more popular as it is easier to do without hurting the performance of the network. However, it requires sparse computations to be effective. This requires hardware support and a certain amount of sparsity to be efficient. Pruning nodes will allow dense computation which is more optimized. This allows the network to be run normally without sparse computation. This dense computation is more often better supported on hardware. However, removing entire neurons can more easily hurt the accuracy of the neural network.</p> <h1 id="what-to-prune">What to prune?</h1> <p>A major challenge in pruning is determining what to prune. If you are removing weights or nodes from a model, you want the parameters you remove to be less useful. There are different heuristics and methods of determining which nodes are less important and can be removed with minimal effect on accuracy. You can use heuristics based on the weights or activations of a neuron to determine how important it is for the model’s performance. The goal is to remove more of the less important parameters.</p> <p>One of the simplest ways to prune is based on the magnitude of the weight. Removing a weight is essentially setting it to zero. You can minimize the effect on the network by removing weights that are already close to zero, meaning low in magnitude. This can be implemented by removing all weights below a certain threshold. To prune a neuron based on weight magnitude you can use the L2 norm of the neuron’s weights.</p> <p>Rather than just weights, activations on training data can be used as a criteria for pruning. When running a dataset through a network, certain statistics of the activations can be observed. You may observe that some neurons always outputs near-zero values. Those neurons can likely be removed with little impact on the model. The intuition is that if a neuron rarely activates with a high value, then it is rarely used in the model’s task.</p> <p>In addition to the magnitude of weights or activations, redundancy of parameters can mean a neuron can be removed. If two neurons in a layer have very similar weights or activations, it can mean they are doing the same thing. By this intuition, we can remove one of the neurons and preserve the same functionality.</p> <p>Ideally in a neural network, all the neurons have unique parameters and output activations that are significant in magnitude and not redundant. We want all the neurons are doing something unique, and remove those that are not.</p> <h1 id="when-to-prune">When to prune?</h1> <p>A major consideration in pruning is where to put it in the training/testing machine learning timeline. If you are using a weight magnitude-based pruning approach, as described in the previous section, you would want to prune after training. However, after pruning, you may observe that the model performance has suffered. This can be fixed by fine-tuning, meaning retraining the model after pruning to restore accuracy.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/pruning/pruning_flow-480.webp 480w,/blog/assets/img/blog/pruning/pruning_flow-800.webp 800w,/blog/assets/img/blog/pruning/pruning_flow-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/assets/img/blog/pruning/pruning_flow.png" width="300" height="auto" alt="flow of iterative pruning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The usage of pruning can change depending on the application and methods used. Sometimes fine-tuning or multiple iterations of pruning are not necessary. This depends on how much of the network is pruned.</p> <h1 id="how-to-evaluate-pruning">How to evaluate pruning?</h1> <p>There multiple metrics to consider when evaluating a pruning method: accuracy, size, and computation time. Accuracy is needed to determine how the model performs on its task. Model size is how much bytes of storage the model takes. To determine computation time, you can use FLOPs (Floating point operations) as a metric. This is more consistent to measure than inference time and it does not depend on what system the model runs on.</p> <p>With pruning, there is a tradeoff between model performance and efficiency. You can prune heavily and have a smaller more efficient network, but also less accurate. Or you could prune lightly and have a highly performant network, that is also large and expensive to operate. This trade-off needs to be considered for different applications of the neural network.</p> <h1 id="conclusion">Conclusion</h1> <p>Pruning is an effective method of making neural networks more efficient. There are plenty of choices and areas of research in this area. We want to continue to make advances in deep learning while also keeping our models energy, time, and space-efficient.</p> <h1 id="references">References</h1> <p>[1] Blalock, Davis, et al. “What is the state of neural network pruning?.” arXiv preprint arXiv:2003.03033 (2020).</p> <p>[2] Han, Song, et al. “Learning both weights and connections for efficient neural network.” Advances in neural information processing systems. 2015.</p> <p>[3] PyTorch Pruning Tutorial <a href="https://pytorch.org/tutorials/intermediate/pruning_tutorial.html">https://pytorch.org/tutorials/intermediate/pruning_tutorial.html</a></p> <p>[4] Keras / Tensorflow Pruning Tutorial <a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras">https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras</a></p> <p>[5] Molchanov, Pavlo, et al. “Pruning convolutional neural networks for resource efficient inference.” arXiv preprint arXiv:1611.06440 (2016).</p> <p>[6] Babaeizadeh, Mohammad, Paris Smaragdis, and Roy H. Campbell. “Noiseout: A simple way to prune neural networks.” arXiv preprint arXiv:1611.06211 (2016).</p>]]></content><author><name></name></author><category term="applied-ml"/><summary type="html"><![CDATA[Much of the success of deep learning has come from building larger and larger neural networks. This allows these models to perform better on various tasks, but also makes them more expensive to use. Larger models take more storage space which makes them harder to distribute. Larger models also take more time to run and can require more expensive hardware. This is especially a concern if you are productionizing a model for a real-world application.]]></summary></entry></feed>