<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="4zEwdc_gZtK7GAY69eUGNqwH6-hIgaXtASEwUuEDEho"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Scaling Deep Learning | Rohit Bandaru </title> <meta name="author" content="Rohit Bandaru"> <meta name="description" content="ML blog."> <meta name="keywords" content="machine-learning, ai, artificial-intelligence, research"> <meta property="og:site_name" content="Rohit Bandaru"> <meta property="og:type" content="website"> <meta property="og:title" content="Rohit Bandaru | Scaling Deep Learning"> <meta property="og:url" content="https://rohitbandaru.github.io/Scaling-Deep-Learning/"> <meta property="og:description" content="ML blog."> <meta property="og:image" content="assets/img/blog/scaling_ml/data_parallelism.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Scaling Deep Learning"> <meta name="twitter:description" content="ML blog."> <meta name="twitter:image" content="assets/img/blog/scaling_ml/data_parallelism.png"> <meta name="twitter:site" content="@rohit_bandaru"> <meta name="twitter:creator" content="@rohit_bandaru"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Rohit Bandaru"
        },
        "url": "https://rohitbandaru.github.io/Scaling-Deep-Learning/",
        "@type": "WebSite",
        "description": "ML blog.",
        "headline": "Scaling Deep Learning",
        
        "sameAs": ["https://www.linkedin.com/in/rohit-bandaru", "https://twitter.com/rohit_bandaru"],
        
        "name": "Rohit Bandaru",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rohitbandaru.github.io/Scaling-Deep-Learning/"> <script src="/assets/js/theme.js?bd888c560287cd675855c7662a167c4a"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rohit</span> Bandaru </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">notes </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Scaling Deep Learning</h1> <p class="post-meta"> Created in February 21, 2023 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2023   ·   <i class="fa-solid fa-hashtag fa-sm"></i> applied-ml </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Many of the state-of-the-art results in deep learning are achieved using multiple GPUs. For some of the largest and most data-intensive ML models, it can take months or even years to train on one CPU or GPU. Training is sped up by scaling to large numbers of GPUs/TPUs. Some neural networks are too large to even fit on one GPU. For example, training large language models like BERT can easily exceed the available memory on a single <a href="https://github.com/google-research/bert/blob/master/README.md#out-of-memory-issues" rel="external nofollow noopener" target="_blank">GPU</a>.</p> <p>If you have the resources it can be easy to speed up training by adding more GPUs. However, it is important to understand the impact this scaling will have on training. Machine learning acceleration is a huge and complex field. I intend to just cover the basic intuitions to keep in mind when training a typical model.</p> <aside> ✍️ We will use GPU and TPU interchangeably. We are treating ML accelerators as generic. </aside> <p>There are two types of machine learning training parallelization: data parallelism and model parallelism.</p> <h1 id="data-parallelism">Data Parallelism</h1> <p>Data parallelism splits a training batch into smaller batches for each GPU. Each GPU has its own copy of the model. Each GPU computes gradients with its own training batch. These gradients are then aggregated across all the GPUs. Each GPU can send its gradients to all other GPUs. For example, if you train with a batch size of 64 and 4 GPUs, each GPU will get a batch size of 16. It will compute gradients for this batch. Once all the GPUs are done with their computations, they can send their gradients to each other. The gradients are then averaged and applied to the model. This allows us to train a model with batch size 64 at the speed of batch size 16. However, there is additional latency in communicating the gradients and synchronizing the GPUs, but it is usually negligible compared to the gradient computations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/data_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/data_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/data_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/scaling_ml/data_parallelism.png" width="100%" height="auto" alt="Data parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Another option is to skip the gradient aggregation and simply apply the updates to the model separately. This can be done by having an orchestrator take a lock on the model. Or in <a href="https://arxiv.org/abs/1106.5730" rel="external nofollow noopener" target="_blank">Hogwild</a>, you can just update the model without any lock. This will allow some GPU batches to be dropped due to race conditions but minimizes synchronization delays.</p> <p>Adding GPUs doesn’t make training steps faster. It allows you to have larger mini-batch sizes, which in turn trains models faster.</p> <p>There are three variables to consider: mini-batch size, GPU batch size, and number of GPUs. Since the number of GPUs is a function of the other two variables, we will only discuss the two types of batch size and how to optimize them.</p> \[\textrm{Mini batch size} = \textrm{GPU batch size} * \textrm{Number of GPUs}\] <p>The implementation of parallelism can vary between ML frameworks: <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" rel="external nofollow noopener" target="_blank">PyTorch</a>, <a href="https://www.tensorflow.org/guide/distributed_training" rel="external nofollow noopener" target="_blank">TensorFlow</a>, <a href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html#way-batch-data-parallelism" rel="external nofollow noopener" target="_blank">Jax</a></p> <h2 id="optimal-gpu-batch-size">Optimal GPU Batch Size</h2> <p>With GPUs, we simply want to minimize the training step time. If we operate the GPUs in the optimal GPU batch size range, we can then just set the number of GPUs to get the optimal mini-batch size. Also, note the GPU batch size has an upper limit from the memory available.</p> <p>To see how GPU performance relates to speed, I timed the training steps of a ResNet50 model against ImageNet-sized batches of different sizes. I tested batch sizes of every power of two until the GPU ran out of memory.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/step_speed_vs_batch_size-480.webp 480w,/assets/img/blog/scaling_ml/step_speed_vs_batch_size-800.webp 800w,/assets/img/blog/scaling_ml/step_speed_vs_batch_size-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/scaling_ml/step_speed_vs_batch_size.jpg" width="100%" height="auto" alt="step speed vs batch size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We see that the throughput (examples per ms) is maximized at the largest possible batch size. We also see that for batch sizes of less than 2^4 or 16, the throughput is lower. GPUs are inefficient with small batches due to overhead. CPUs perform better in some settings. The takeaway is that we want to maximize the GPU utilization by fitting the largest possible batch. Some libraries have the functionality to search for the largest possible batch size given a GPU and dataset. In the flat region, GPU step time increases linearly with batch size.</p> <h2 id="optimal-mini-batch-size">Optimal Mini Batch Size</h2> <p>To optimize the mini-batch size, we will ignore accelerators and just focus on mini-batch gradient descent. Mini batch size is less hardware-dependent and more problem dependent. We will use ImageNet as an example, but the effects of batch size on training should be considered for every new problem.</p> <p>Assuming maximize GPU usage/batch size, optimizing the mini-batch size means selecting the number of GPUs to use. The assumption is that with more GPUs, we can train a model faster. Training on more GPUs means faster training epochs. However, we are interested in the test accuracy of the model, not just completing epochs.</p> <p>Consider this plot from the paper <a href="https://arxiv.org/abs/1811.03600" rel="external nofollow noopener" target="_blank">Measuring the Effects of Data Parallelism on Neural Network Training</a> by Shallue et al.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/effects_of_dp-480.webp 480w,/assets/img/blog/scaling_ml/effects_of_dp-800.webp 800w,/assets/img/blog/scaling_ml/effects_of_dp-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/scaling_ml/effects_of_dp.png" width="500" height="auto" alt="Plot of training speed vs batch size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1811.03600" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>For points on the dashed line, the number of training steps is halved whenever the batch size is doubled. This means that doubling the GPUs/TPUs would have the training time. This is ideal. In this region, you can happily speed up your model training by adding more GPUs that you may have available. However, this tradeoff changes at batch size 2^13 or 8192. From here, doubling the GPUs still speeds up model training, but the speed will be more than half. This is the point of diminishing returns. If you have the GPUs, you might as well use them but those additional GPUs are not as effective.</p> <p>The paper goes into great detail on this relationship and the effects of other factors such as model architecture, optimizers, and datasets. The takeaway for this blog is that if you set the maximum GPU batch size, up to a point, adding additional GPUs will linearly speed up the training of your model.</p> <h1 id="model-parallelism">Model Parallelism</h1> <p>This type of parallelism is much less commonly used. It can be used along with data parallelism. Model parallelism is when an ML model is too large to fit in the memory of one device. It is partitioned across multiple devices. This has enabled us to train larger and larger networks. For example, the GPT-3 model is about <a href="https://www.reddit.com/r/MachineLearning/comments/gzb5uv/comment/fti44lv/?utm_source=share&amp;utm_medium=web2x&amp;context=3" rel="external nofollow noopener" target="_blank">350 GB</a>. No single GPU can store the whole model in memory.</p> <p>There are different ways of achieving model parallelism. You can split the model vertically by layer, or horizontally by splitting the tensors.</p> <h2 id="pipeline-parallelism">Pipeline Parallelism</h2> <p>The simplest solution is to process different layers of a neural network on different accelerations. A simple illustration of this:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipeline_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/pipeline_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/pipeline_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/scaling_ml/pipeline_parallelism.png" width="100%" height="auto" alt="pipeline parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In <a href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html#speed-up-by-pipelining-inputs" rel="external nofollow noopener" target="_blank">PyTorch</a>, the layers are bucketed into groups of roughly equal memory so that the computations are evenly distributed across the accelerators.</p> <p>A major issue with this approach is that after Layer 0 has a forward pass, it has to wait for the other layers to compute forward and backward passes. The GPU is idle for about 75% of the time. The following diagrams are from the <a href="https://arxiv.org/abs/1811.06965" rel="external nofollow noopener" target="_blank">GPipe</a> paper by Huang et al.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/scaling_ml/batches_without_pipeline_parallelism.png" width="100%" height="auto" alt="batches without pipeline parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1811.06965" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>The solution is to have the layer compute the next graph while it is waiting for the gradient of the current batch. This essentially combines data parallelism with model parallelism. I explained above that to maximize training speed, we want to maximize the utilization of accelerators. For large models that require model parallelism, we have an additional problem of GPU waiting time. Pipelining GPU batches helps reduce this gap.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipeline_parallelism_batches-480.webp 480w,/assets/img/blog/scaling_ml/pipeline_parallelism_batches-800.webp 800w,/assets/img/blog/scaling_ml/pipeline_parallelism_batches-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/scaling_ml/pipeline_parallelism_batches.png" width="100%" height="auto" alt="Pipeline parallelism batches" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1811.06965" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>We see that with 4 GPU batches, each GPU is idle for about 6/16 of the time. The variables here are the number of GPUs and number of GPU batches per GPU. With 1 GPU batch (no pipelining), the utilization is: \(\frac{2}{n_{GPU}* 2} = \frac{1}{n_{GPU}}\). With pipelining, we get \(\frac{n_{batches}*2}{n_{batches}*2 + 2* (n_{GPU}-1)}\). This simplifies to the following:</p> \[utilization = \frac{n_{batches}}{n_{batches} + n_{GPU}-1}\] <p>This equation explains the tradeoff. Increasing the number of GPU batches drives the utilization closer to 1, while increasing the number of GPUs reduces the utilization.</p> <p>In an optimal setup, we split the model among as few GPUs as possible, but increase the number of batches that they process in a step. Pipeline parallelism has the added benefit of the speedups of data parallelism. This makes it a very effective solution.</p> <p>In the <a href="https://arxiv.org/abs/1806.03377" rel="external nofollow noopener" target="_blank">PipeDream</a> paper, Harlap et al. show that we can further reduce idle time by interleaving forward and backward operations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipedream-480.webp 480w,/assets/img/blog/scaling_ml/pipedream-800.webp 800w,/assets/img/blog/scaling_ml/pipedream-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/scaling_ml/pipedream.png" width="100%" height="auto" alt="pipedream" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1806.03377" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>However, this eliminates gradient synchronization. Even eliminating batches above 4, we get the same utilization as GPipe parallelism, just in a different order. For many of the backward passes, a stale version of the model parameters is used. Gradient synchronization is an important tradeoff in all types of ML parallelism.</p> <p>In analyzing utilization, we have been assuming that forward and backward computations are equivalent. Backward passes tend to take more time. If we interleave operations as to always prioritize backward passes, we can get a utilization gain. From AWS Sagemaker <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html" rel="external nofollow noopener" target="_blank">documentation</a>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipedream1-480.webp 480w,/assets/img/blog/scaling_ml/pipedream1-800.webp 800w,/assets/img/blog/scaling_ml/pipedream1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/scaling_ml/pipedream1.png" width="100%" height="auto" alt="without backward prioritization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>The idle time here is 1 forward pass and 1 backward pass.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/pipedream2-480.webp 480w,/assets/img/blog/scaling_ml/pipedream2-800.webp 800w,/assets/img/blog/scaling_ml/pipedream2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/scaling_ml/pipedream2.png" width="100%" height="auto" alt="with backward prioritization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>With backward prioritization, the idle time for GPU 0 is 3 forward passes. This effect will be increased with more GPUs. There are many tradeoffs in parallel ML, such as communication between model layers, the memory overhead of forward and backward passes, different model splits, staleness, etc. We are only covering the high-level intuitions to achieve fast and effective training of large models.</p> <p>What if we want to use more GPUs for data parallelism, but without splitting up the model further? We can simply run pipelines in parallel. For example, we can split the model among four GPUs but duplicate each model split twice. We can then aggregate the gradients of both pipelines in the update. This is often called hybrid model and data parallelism.</p> <h2 id="tensor-parallelism">Tensor Parallelism</h2> <p>Instead of splitting the model into layers, we can split the layers themselves. From the <a href="https://arxiv.org/abs/1909.08053" rel="external nofollow noopener" target="_blank">Megatron-LM paper</a> by Shoeybi et al.:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/scaling_ml/tensor_parallelism-480.webp 480w,/assets/img/blog/scaling_ml/tensor_parallelism-800.webp 800w,/assets/img/blog/scaling_ml/tensor_parallelism-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/scaling_ml/tensor_parallelism.png" width="100%" height="auto" alt="tensor parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1909.08053" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>The input X has to be completely copied for each split of the model. The layer is split into two halves. The splits of the model are then aggregated in the last layers of the model. Splitting the tensors themselves offers some benefits. The latency is reduced since you can fit more layers on a GPU. This is parallel computation instead of serialized computation. You don’t have to worry about scheduling to minimize idle time.</p> <p>An issue with this approach is that the activations are also separated, so you are learning a different model architecture. There is an additional cost in concatenating \(Y_1\) and \(Y_2\) for both GPUs. The Megatron-LM architecture is designed to reduce the cost of communicating between GPUs.</p> <h1 id="conclusion">Conclusion</h1> <p>We touched the surface on the many tradeoffs, optimizations, and considerations needed for distributed and large scale ML. As models grow larger, it will become more import to understand and keep up to date with this field.</p> <h1 id="additional-resources">Additional Resources</h1> <p><a href="https://www.youtube.com/watch?v=3XUG7cjte2U" rel="external nofollow noopener" target="_blank">https://www.youtube.com/watch?v=3XUG7cjte2U</a></p> <p><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/" rel="external nofollow noopener" target="_blank">https://lilianweng.github.io/posts/2021-09-25-train-large/</a></p> <p><a href="https://openai.com/blog/techniques-for-training-large-neural-networks/" rel="external nofollow noopener" target="_blank">https://openai.com/blog/techniques-for-training-large-neural-networks/</a></p> <p><a href="https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/" rel="external nofollow noopener" target="_blank">https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/</a></p> <p><a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism" rel="external nofollow noopener" target="_blank">https://huggingface.co/docs/transformers/v4.15.0/parallelism</a></p> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Bandaru, Rohit (Feb 2023). Scaling Deep Learning. https://rohitbandaru.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">bandaru2023scaling-deep-learning</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Scaling Deep Learning}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Bandaru, Rohit}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Feb}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://rohitbandaru.github.io/Scaling-Deep-Learning/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Rohit Bandaru. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?3e7054dc4d3e3dd8f0731a48453e618e"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?3577194613afa04501eb52f8f4164de9" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-blog",title:"blog",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-about",title:"about",description:"",section:"Navigation",handler:()=>{window.location.href="/about/"}},{id:"nav-notes",title:"notes",description:"",section:"Navigation",handler:()=>{window.location.href="/notes/"}},{id:"post-transformer-design-guide-part-1-vanilla",title:"Transformer Design Guide (Part 1: Vanilla)",description:"",section:"Posts",handler:()=>{window.location.href="/Transformer-Design-Guide-Pt1/"}},{id:"post-self-supervision-from-videos",title:"Self-Supervision from Videos",description:"",section:"Posts",handler:()=>{window.location.href="/Self-Supervision-from-Videos/"}},{id:"post-ssl-with-vision-transformers",title:"SSL with Vision Transformers",description:"",section:"Posts",handler:()=>{window.location.href="/SSL-with-Vision-Transformers/"}},{id:"post-deep-dive-into-yann-lecun-s-jepa",title:"Deep Dive into Yann LeCun\u2019s JEPA",description:"",section:"Posts",handler:()=>{window.location.href="/JEPA-Deep-Dive/"}},{id:"post-scaling-deep-learning",title:"Scaling Deep Learning",description:"",section:"Posts",handler:()=>{window.location.href="/Scaling-Deep-Learning/"}},{id:"post-knowledge-distillation-as-self-supervised-learning",title:"Knowledge Distillation as Self-Supervised Learning",description:"",section:"Posts",handler:()=>{window.location.href="/knowledge-distillation-ssl/"}},{id:"post-self-supervised-learning-getting-more-out-of-data",title:"Self-Supervised Learning\u200a -\u200a Getting more out of\xa0data",description:"",section:"Posts",handler:()=>{window.location.href="/Self-Supervised-Learning/"}},{id:"post-domain-adaptation",title:"Domain Adaptation",description:"",section:"Posts",handler:()=>{window.location.href="/Domain-Adaptation/"}},{id:"post-pruning-neural-networks",title:"Pruning Neural Networks",description:"",section:"Posts",handler:()=>{window.location.href="/Neural-Network-Pruning/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%72%6F%68.%62%61%6E%64%61%72%75@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/rohit-bandaru","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/rohit_bandaru","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>