<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="4zEwdc_gZtK7GAY69eUGNqwH6-hIgaXtASEwUuEDEho"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deep Dive into Yann LeCun’s JEPA | Rohit Bandaru </title> <meta name="author" content="Rohit Bandaru"> <meta name="description" content="ML blog."> <meta name="keywords" content="machine-learning, ai, artificial-intelligence, research"> <meta property="og:site_name" content="Rohit Bandaru"> <meta property="og:type" content="website"> <meta property="og:title" content="Rohit Bandaru | Deep Dive into Yann LeCun’s JEPA"> <meta property="og:url" content="https://rohitbandaru.github.io/JEPA-Deep-Dive/"> <meta property="og:description" content="ML blog."> <meta property="og:image" content="assets/img/blog/jepa/jepa_brain.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Deep Dive into Yann LeCun’s JEPA"> <meta name="twitter:description" content="ML blog."> <meta name="twitter:image" content="assets/img/blog/jepa/jepa_brain.png"> <meta name="twitter:site" content="@rohit_bandaru"> <meta name="twitter:creator" content="@rohit_bandaru"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Rohit Bandaru"
        },
        "url": "https://rohitbandaru.github.io/JEPA-Deep-Dive/",
        "@type": "WebSite",
        "description": "ML blog.",
        "headline": "Deep Dive into Yann LeCun’s JEPA",
        
        "sameAs": ["https://www.linkedin.com/in/rohit-bandaru", "https://twitter.com/rohit_bandaru"],
        
        "name": "Rohit Bandaru",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rohitbandaru.github.io/JEPA-Deep-Dive/"> <script src="/assets/js/theme.js?bd888c560287cd675855c7662a167c4a"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rohit</span> Bandaru </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">notes </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deep Dive into Yann LeCun’s JEPA</h1> <p class="post-meta"> Created in July 31, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> self-supervised-learning   <i class="fa-solid fa-hashtag fa-sm"></i> ai   <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"><a href="#relevant-talks-by-yann-lecun">Relevant Talks by Yann LeCun</a></li> <li class="toc-entry toc-h1"> <a href="#problems-with-current-ai">Problems with Current AI</a> <ul> <li class="toc-entry toc-h2"> <a href="#common-sense">Common Sense</a> <ul> <li class="toc-entry toc-h3"><a href="#how-humans-learn">How Humans Learn</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#learning-to-think">Learning to Think</a></li> </ul> </li> <li class="toc-entry toc-h1"><a href="#modality">Modality</a></li> <li class="toc-entry toc-h1"><a href="#a-framework-for-building-human-level-ai">A Framework for Building Human-Level AI</a></li> <li class="toc-entry toc-h1"><a href="#actor">Actor</a></li> <li class="toc-entry toc-h1"><a href="#cost">Cost</a></li> <li class="toc-entry toc-h1"><a href="#configurator">Configurator</a></li> <li class="toc-entry toc-h1"> <a href="#world-model">World Model</a> <ul> <li class="toc-entry toc-h2"><a href="#self-supervised-learning--energy-based-models">Self-Supervised Learning / Energy-Based Models</a></li> <li class="toc-entry toc-h2"> <a href="#joint-embedding-predictive-architecture">Joint Embedding Predictive Architecture</a> <ul> <li class="toc-entry toc-h3"><a href="#hierarchical-jepa-h-jepa">Hierarchical JEPA (H-JEPA)</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#world-model-architecture">World Model Architecture</a></li> </ul> </li> <li class="toc-entry toc-h1"><a href="#data-streams">Data Streams</a></li> <li class="toc-entry toc-h1"><a href="#objective-driven-ai">Objective Driven AI</a></li> <li class="toc-entry toc-h1"> <a href="#towards-implementing-jepa">Towards Implementing JEPA</a> <ul> <li class="toc-entry toc-h2"><a href="#i-jepa-self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture">I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</a></li> <li class="toc-entry toc-h2"><a href="#v-jepa-revisiting-feature-prediction-for-learning-visual-representations-from-video">V-JEPA: Revisiting Feature Prediction for Learning Visual Representations from Video</a></li> <li class="toc-entry toc-h2"><a href="#mc-jepa-a-joint-embedding-predictive-architecture-for-self-supervised-learning-of-motion-and-content-features">MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</a></li> <li class="toc-entry toc-h2"><a href="#whats-next">What’s Next?</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <p>In the AI research community, Yann LeCun has a unique and often controversial perspective. As of 2024, LLMs and Generative AI are the main focus areas of the field of AI. We’ve all been impressed by the performance of LLMs in various contexts, and generative systems like OpenAI’s <a href="https://openai.com/sora" rel="external nofollow noopener" target="_blank">Sora</a>. However, it is not clear where these advances fit in the long term goal of achieving and surpassing human level intelligence, which many call AGI.</p> <p>In his position paper <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf" rel="external nofollow noopener" target="_blank">A Path Towards Autonomous Machine Intelligence</a> and his many recent talks (linked below), Yann presents an alternative framework for achieving artificial intelligence. He also proposes a new architecture for a predictive world model: Joint Embedding Predictive Architecture (JEPA).</p> <p>This blog post will dive deep into Yann’s vision for AI, the JEPA architecture, current research, and energy-based models. We will go deep into the technical aspects of these ideas, as well as give my opinions, along with interesting references. I will also cover recent research advances such as <em>V-JEPA</em></p> <p>This is a long post, feel free to jump to the sections about JEPA, I-JEPA, and V-JEPA.</p> <h3 id="relevant-talks-by-yann-lecun">Relevant Talks by Yann LeCun</h3> <p><a href="https://drive.google.com/file/d/1RVYBVi_bWyz-4sZSsu4rSWzDwQBLsvHL/view" rel="external nofollow noopener" target="_blank"><em>From Machine Learning to Autonomous Intelligence</em></a></p> <div class="video"> <figure> <iframe width="560" height="315" src="https://www.youtube.com/embed/VRzvpV9DZ8Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </figure> </div> <p><a href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf" rel="external nofollow noopener" target="_blank"><em>Objective-Driven AI: Towards Machines that can Learn, Reason, and Plan”</em></a></p> <div class="video"> <figure> <iframe width="560" height="315" src="https://www.youtube.com/embed/d_bdU3LsLzE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </figure> </div> <h1 id="problems-with-current-ai">Problems with Current AI</h1> <p>The JEPA architecture aims to address current AI challenges. To contextualize these issues, we’ll examine Yann LeCun’s criticisms of popular AI trends as of 2024.</p> <p>Recent years have seen tremendous excitement around Large Language Models (LLMs) and Generative AI. LLMs are pretrained using autoregressive self-supervised learning, predicting the next token given preceding ones. They’re trained on vast datasets of text and code from the internet and books, often fine-tuned with supervised learning or reinforcement learning. Generative AI broadly refers to creation of multimodal media from inputs, such as text-to-image generation.</p> <p>However, these models face significant limitations:</p> <ol> <li>Factuality / Hallucinations: When uncertain, models often generate plausible-sounding but false information. They’re optimized for probabilistic likelihood, not factual accuracy.</li> <li>Limited Reasoning: While techniques like <a href="https://arxiv.org/abs/2201.11903" rel="external nofollow noopener" target="_blank">Chain of Thought</a> prompting improve LLM’s ability to reason, they’re restricted to solving the selected type of problem and approaches to solving them without improving generalized reasoning abilities.</li> <li>Lack of Planning: LLMs predict one step at a time, lacking effective long-term planning crucial for tasks requiring sustained goal-oriented behavior.</li> </ol> <p>Despite impressive advancements, the challenge of autonomous driving illustrates the gap between current AI and human-level intelligence. As LeCun notes, humans can learn driving basics in about 20 hours. In contrast, self-driving car development has consumed billions of dollars, extensive data collection, and decades of effort, yet still hasn’t achieved human-level performance.</p> <p>Even achieving Level 5 autonomy wouldn’t signify true human-level AI or Artificial General Intelligence (AGI). Such intelligence would involve learning to drive from scratch within a day, using only data collected during that experience, without relying on massive pre-existing datasets for finetuning. Realizing this level of adaptable intelligence might require several more decades of research.</p> <h2 id="common-sense">Common Sense</h2> <p>The limitations in AI models can often be attributed to a lack of common sense. Common sense can be defined as thinking and acting in a reasonable manner. Humans and many animals have this ability. This includes avoiding egregiously dangerous or incorrect actions. Expanding on the autonomous driving example, AV systems need to be trained to deal with new situations safely. When learning to drive, humans utilize their common sense to know to not do dangerous things like driving off the road or into other cars. This is not obvious to current AV systems, so they require a large amount of training data to avoid these actions.</p> <p>LLMs similarly demonstrate a lack of common sense through nonsensical or illogical outputs. Common sense is a vague term. One definition is that it is a lower bound on the types of errors an agent makes. For AI to be trustworthy, it needs this foundational level of understanding.</p> <p>Common sense can also be viewed as a collection of world models. These models enable quick learning of new skills, avoidance of dangerous mistakes in novel situations, and prediction of outcomes in unfamiliar scenarios. Essentially, we use world models to generalize our experiences.</p> <h3 id="how-humans-learn">How Humans Learn</h3> <p>Humans acquire a basic understanding of the world during early infancy, but we’re also born with some innate knowledge. The brain isn’t randomly initialized; it’s evolved, pre-trained, and fine-tuned throughout life. This differs significantly from artificial neural networks, which start with random initializations and have far weaker inductive biases than humans or animals. Life is generally pre-programmed to behave in a certain way from birth. More intelligent life is able to learn more and not purely rely on innate knowledge.</p> <p>Understanding the extent to which babies acquire common sense during infancy is crucial for AI development. If common sense is largely innate, the focus should be on massive datasets mimicking evolutionary timescales. If it’s primarily learned, priority should be given to models that excel at quick learning from limited data.</p> <p>A baby’s experience, while not comparable to evolutionary timescales, still represents a substantial dataset. If a baby is awake for <a href="https://intuitiveparentingdc.com/blog/2018/7/6/developmentally-appropriate-sleep-expectations-birth-to-age-5" rel="external nofollow noopener" target="_blank">8 hours</a> a day, in four months they have seen about 960 hours of data. This data is also augmented by other sensory signals and dense biological supervision (pain, hunger, emotions). This is around the same length as the <a href="https://arxiv.org/abs/1705.06950" rel="external nofollow noopener" target="_blank">Kinetics 400</a> video dataset. This is still dwarfed by the millions of hours of video that self driving cars are using.</p> <p>This Nature <a href="https://www.nature.com/articles/s42256-024-00802-0" rel="external nofollow noopener" target="_blank">paper</a> by Orhan and Lake explores learning from infant-perspective data. They demonstrate that computer vision models can be trained on noisy, less diverse datasets collected from infant headcams. These egocentric datasets are far noisier and less diverse than standard image/video datasets, but AI models without strong inductive biases can learn from them.</p> <p>Emmanuel Dupoux’s diagram, presented by Yann LeCun, suggests that babies often understand concepts like object permanence, solidity, and biological motion by around four months. While presented as quick learning, it’s important to note the significant amount of data processing that occurs during this time.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/dupoux-480.webp 480w,/assets/img/blog/jepa/dupoux-800.webp 800w,/assets/img/blog/jepa/dupoux-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/dupoux.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Dupoux diagram on cognitive development" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>We don’t yet know precisely how much data AI systems would need to learn the same concepts as babies. It’s likely that the data efficiency gap is relatively small for basic concepts that babies learn. For instance, object permanence could probably be learned from 960 hours of video data. However, it becomes evident that this gap grows substantially with age and with the complexity of the knowledge being assessed. The challenges in developing fully autonomous vehicles clearly demonstrate how large this data efficiency gap can become.</p> <p>In addition to the lack of common sense, we mention three other fundamental gaps in the ability of current AI: hallucinations, lack of planning, and lack of reasoning.</p> <h2 id="learning-to-think">Learning to Think</h2> <p>The question of whether Large Language Models (LLMs) can truly reason and plan is a contentious topic in the AI community. While these models exhibit behaviors that resemble <a href="https://arxiv.org/abs/2201.11903" rel="external nofollow noopener" target="_blank">reasoning</a> and planning, skeptics argue that they merely replicate patterns from their training data.</p> <p>To frame this discussion, let’s consider reasoning and planning as forms of “thinking”, which we will define as a variable length internal process that precedes any outputs.. Current deep learning models employ two primary mechanisms for this kind of processing:</p> <ol> <li>Depth: Each layer in a neural network can be viewed as a step in the thinking process. However, this depth is typically fixed, with some recent <a href="https://arxiv.org/abs/2404.02258" rel="external nofollow noopener" target="_blank">work</a> exploring dynamic depth adjustment based on input complexity. Despite these advances, maximum depth and other constraints still limit the model’s flexibility.</li> <li>Sequential Generation: Decoder-based LLMs, such as GPT, generate text one token at a time. Each step in this process involves some degree of computation that could be interpreted as thinking. Prompt engineering techniques leverage this sequential nature to guide the model towards desired outputs. A key limitation of this approach is that the model must produce a token at each step, preventing purely internal information processing.</li> </ol> <p>While these properties enable models to create the illusion of thought, significant advancements are necessary to achieve more effective reasoning and planning capabilities.</p> <p>Many researchers draw parallels between AI and the two-system model of thinking <a href="https://www.google.com/books/edition/Thinking_Fast_and_Slow/ZuKTvERuPG8C?hl=en&amp;gbpv=1&amp;printsec=frontcover" rel="external nofollow noopener" target="_blank">proposed</a> by Daniel Kahneman. System 1 thinking is fast and intuitive, providing immediate responses without conscious deliberation. System 2, in contrast, is slower and more deliberate, engaging in deeper cognitive processing. Current machine learning models, including LLMs, primarily operate in a System 1 mode by processing information in a single pass without the ability to plan ahead. While they excel at pattern recognition, they lack true reasoning or planning capabilities.</p> <p>This inability to plan contributes to factual errors in LLM outputs. Each generated word carries a risk of inaccuracy, with the probability of errors increasing exponentially as the output length grows. The sequential nature of token generation means that early mistakes can compound, potentially invalidating the entire output. This stands in stark contrast to human speech, where we typically plan our utterances at a higher level before vocalization, minimizing such errors. In this context, reasoning can be viewed as the planning of speech. Without the capacity to reason or plan effectively, LLMs essentially “speak without thinking.”</p> <p>In the JEPA paper, Yann LeCun proposes frameworks for models that can think. Learning to think may address the fundamental problems in current AI models and represent a crucial step towards achieving more human-like intelligence in AI.</p> <h1 id="modality">Modality</h1> <p>Recent advancements have expanded LLMs to include multimodal processing and outputs, but they remain primarily language-centric. This raises questions about the sufficiency of language alone for AI and the investment needed in visual understanding. Could visual comprehension help ground AI in reality, improving common sense and reducing hallucinations?</p> <p>Language serves as a compressed representation of the complex concepts humans experience. Its expressive power is vast, capable of describing intricate scientific theories and nuanced emotions. Yet, language alone may not suffice for complete understanding.</p> <p>Humans interpret language within the context of shared reality. It functions as a highly efficient medium for transmitting information through the relatively narrow bandwidth of speech. When we process language, our brains rely on prior knowledge and experiences. While some of this prior information can be acquired through text, a significant portion stems from visual and physical interactions with the world.</p> <p>Currently, it does seem that language models are more capable than vision models. Language models currently outperform visual models due to information density, data requirements, and data availability.</p> <p>In a given data point there is a certain amount of explicit information in the form of bits. But then there is relevant information that is useful. For example, if you take an image of the park, a lot of bits are used to represent the position of every blade of grass. But that is not useful in most scenarios. Language is very compressed. While there are some filler words that don’t add much <a href="https://www.youtube.com/watch?v=VvPaEsuz-tY&amp;ab_channel=Argonaut57" rel="external nofollow noopener" target="_blank">information</a>, the ratio of knowledge to bits is high. However, for images, most of the bits are not useful. This means you need orders of magnitude more bits of data to learn equivalent knowledge. Video models are further behind because you need another order of magnitude more bits since consecutive frames in video are mostly redundant.</p> <p>While language-based AI leads, scenarios exist where visual learning could catch up. One scenario in which visual learning could overtake language is that we will have a large number of robots / autonomous vehicles interacting with the world while collecting visual data. Language will be data constrained with the rate of new text generation limiting scaling. In a world with a lot of robots, the knowledge gained from the visual world and the size of the available datasets may exceed that of text. However, this is all very speculative. We don’t know how important vision or grounding is for intelligence.</p> <h1 id="a-framework-for-building-human-level-ai">A Framework for Building Human-Level AI</h1> <p>Yann proposes a high level architecture for building an AI system that is aimed at addressing the problems we outlined. This is a design for an intelligent agent that can perceive the world,</p> <p>We will then explore the various challenges that must be addressed to construct such an architecture. Currently, this is merely a theoretical architecture. Building certain components remains an open problem, and assembling all the modules will pose an additional challenge.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/jepa_brain-480.webp 480w,/assets/img/blog/jepa/jepa_brain-800.webp 800w,/assets/img/blog/jepa/jepa_brain-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/jepa_brain.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="High Level View of LeCun's Architecture for Intelligence" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> High Level View of LeCun's Architecture for Intelligence <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>This architecture contains different proposed components. We will explain these components and their relationships.</p> <p><strong>Configurator</strong>: Configures input from all other modules and configures them for the task at hand. It tells the perception module what information to extract.</p> <p><strong>Perception:</strong> Estimates the current state of the world from different sensory signals.</p> <p><strong>World module</strong>: Estimates missing information about the state of the world and predicts future states. It simulates the world and extracts relevant information as determined by the configurator.</p> <p><strong>Cost module</strong>: Measures the level of discomfort as energy. This energy is the sum of the intrinsic cost module and the trainable critic module.</p> <p><strong>Intrinsic cost</strong>: Computes a cost given the current state of the world and predicted future states. This cost can be imagined as hunger, pain, or general discomfort. This cost can be hardwired in AI agents, as done with rewards in RL.</p> <p><strong>Trainable Critic</strong>: Predicts future intrinsic energy. It has the same input as the intrinsic cost. This estimate is dependent on the intrinsic cost and cannot be hardwired. It is trained from past states and subsequent intrinsic cost, retrieved from memory.</p> <p><strong>Short term memory</strong>: Stores relevant information about past present and future states of the world along with intrinsic cost.</p> <p><strong>Actor</strong>: Proposes sequences of actions. These sequences are executed by the effectors. The world model predicts future states from the sequence which then generates a cost.</p> <h1 id="actor">Actor</h1> <p>The actor proposes an optimal action or sequence of actions.</p> <p>If the world model and cost are well behaved, gradient based optimization can be used to determine an optimal action sequence. If actions are discrete then dynamic programming methods such as beam search can be used.</p> <p>There are two different modes in the actor. These align with Kahneman’s System 1 and 2, which we mentioned earlier.</p> <p><strong>Mode 1 Reactive Behavior</strong>: A policy module that computes an action from the state generated by perception and short-term memory. This module acts fast and produces simple decisions. A world model is needed to estimate the cost of an action. Without a world model the agent would have to perturb their actions which is not feasible. The world model can be adjusted after observing the next state.</p> <p><strong>Mode 2 Reasoning and Planning</strong>: A sequence of actions along with predicted corresponding states is generated. From this sequence of states, a cost can be computed. Planning is done by optimizing the action sequence to minimize total cost. The action sequence is then sent to the effectors which execute at least the beginning of the sequence. The states and costs are stored in short-term memory. The sequence can be optimized through gradients since the cost and world model are differentiable. Dynamic programming can also be used. Planning in this setup is essentially inference time cost optimization.</p> <p>Agents may have multiple policy modules executing mode 1. In this design, the agent only has one world model, so mode 2 can only be run once. However, AIs could be designed to have multiple world models and mode 2 processes at the same time. This is similar to having multiple thoughts at the same time. However, this would be very complicated in that the different modules would have to coordinate with the effectors and other modules to avoid conflicts. Also, this may be why humans don’t think like this.</p> <p>Policy modules can be learned to approximate actions from mode 2 reasoning. This is the process of learning a new skill. In humans, system 2 thinking can be done through system 1 after enough learning. For example, in chess, inexperienced players plan steps explicitly and simulate outcomes. Experienced players can instantly recognize patterns and make optimal moves.</p> <h1 id="cost">Cost</h1> <p>Cost is the sum of an immutable intrinsic cost and a trainable cost or critic.</p> \[C(s) = \mathrm{IC}(s) + \mathrm{TC}(s)\] <p>Each of these costs are the sum of different sub-costs generated by submodules. The weights of the sub-cost at each state \(u\) and \(v\) are determined by the configurator. This allows the agent to focus on different goals at different times.</p> \[\mathrm{IC}(s) = \sum_{i=1}^ku_i\mathrm{IC_i}(s)\\ \mathrm{TC}(s) = \sum_{i=1}^kv_i\mathrm{TC_i}(s)\] <p>The IC being immutable prevents the agent from drifting towards bad behaviors. It constrains the behavior of the agent.</p> <p>\(\mathrm{TC}\) or the critic is trained to predict future intrinsic cost values. The intrinsic cost only considers the current state. The critic can be trained to predict the future cost so the agent can minimize cost in the future. The short term memory stores triplets of (time, state, intrinsic energy): \((\tau, s_{\tau}, IC(s_{\tau}))\). The critic can be trained to predict the cost of a future state or a discounted sum of future intrinsic costs. For example, the loss function of the critic could be \(\|\|\mathrm{IC}(s_{\tau+\delta}) - \mathrm{TC}(s_{\tau})\|\|^2\). This formulation trains the critic to predict the intrinsic cost of a state \(\delta\) steps in the future. \(\mathrm{IC}(s_{\tau+\delta})\) can be replaced with other targets that can be extracted from the sequence of triplets. However, it cannot depend on the future trainable cost itself.</p> <h1 id="configurator">Configurator</h1> <p>The configurator controls the other components of the system. If these components are implemented as transformers, they can be easily configured by adding tokens. The configurator would inject tokens to steer these components in certain directions. For example, it may influence certain types of actions from the actor, or for perception to focus on certain properties.</p> <p>The configurator is also responsible for setting the weights of the cost terms. This will allow for the agent to focus on different subgoals at different times. The unanswered question is how the configurator can learn to decompose a complex task into subgoals.</p> <h1 id="world-model">World Model</h1> <p>In JEPA, the purpose of the world model is to predict future representations of the state of the world. There are three main issues</p> <ol> <li>Diversity of the state sequences the model is able to observe during training</li> <li>The world isn’t fully predictable, so the model has to predict multiple plausible state representations following an action</li> <li>Predictions must be made at different time scales and abstractions</li> </ol> <h2 id="self-supervised-learning--energy-based-models">Self-Supervised Learning / Energy-Based Models</h2> <p>In order to train a world model, Yann LeCun proposes an SSL energy-based model (EBM).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/ebm-480.webp 480w,/assets/img/blog/jepa/ebm-800.webp 800w,/assets/img/blog/jepa/ebm-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/ebm.png" class="image-fluid mx-auto d-block" width="300" height="auto" alt="ebm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>\(x\) and \(y\) can be considered videos, where \(y\) follows x. EBMs learn an energy function \(F(x,y)\) that take low values when \(x\) and \(y\) are compatible and high if not. Compatible in this context means that \(y\) is a plausible continuation of \(x\).</p> <p>This is different from generative models in that \(y\) is not directly predicted from \(x\). There is a large space of values of \(y\) that can follow \(x\). Predicting exactly what will happen is an intractable problem. However, it is feasible to understand what is possible and what is not. Being good at this task requires an understanding of the world and common sense. A value of \(y\) that defies the laws of physics should result in a high energy value.</p> <p>However, planning requires predictions of future states. Although \(y\) can’t be predicted directly, we can predict future representations of \(y\). We can get representations from an encoder: \(s_x = g_x(x)\), \(s_y = g_y(y)\)</p> <p>The encoder will be trained such that the representations are maximally informative about each other, and that \(s_y\) can easily be predicted from \(s_x\). We can make predictions on this representation to enable planning.</p> <p>A latent variable can be introduced to handle uncertainty. A latent variable is just an arbitrary random variable. It is the source of randomness that is transformed to a useful distribution. Here we want to map the latent variable to the large space of possible values \(s_y\) can take.</p> <p>A latent-variable EBM (LVEBM) is represented as \(E_w(x, y, z)\).</p> <p>The energy function can be determined by find the \(z\) value that minimizes the energy. \(F_w(x,y) = \min_{z \in \mathcal{Z} }E_w(x,y,z)\)</p> <p>The EBM collapses when all pairs have the same low energy. This can happen when the latent variable has too much information capacity. This happens because \(z\) can vary along a larger space. This means that the space for which the energy of \(y\) is low is correspondingly large. If it is too large then the energies of \(y\) collapse. If the \(z\) dimension is the same as the representation dimension, the model can ignore \(y\) entirely and set \(s_y\) to equal \(z\).</p> <p>The paper describes a high data density region. This refers to \((x, y)\) pairs that are commonly seen in the real data distribution. We want to lower energy in this region, but keep it high outside of it. Collapse is when the energy is low inside and outside of this region which makes the EBM useless.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/regularized_ebm-480.webp 480w,/assets/img/blog/jepa/regularized_ebm-800.webp 800w,/assets/img/blog/jepa/regularized_ebm-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/regularized_ebm.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="regularized ebm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>There are two training methods used to prevent collapse.</p> <p>Contrastive methods: Collapse is avoided by increasing the energy with respect to negative examples. It requires some method to generate examples to contrast against. The number of contrastive examples needed grows exponentially with respect to the dimension of the representation.</p> <p>Regularized methods: In these methods, the loss is regularized to minimize the space in \(y\) where the energies are lowered. These are less likely to be affected by the curse of dimensionality. Contrastive architectures can be regularized. For example, the latent dimension can be constrained.</p> <h2 id="joint-embedding-predictive-architecture">Joint Embedding Predictive Architecture</h2> <p>JEPA is an EBM that performs predictions in the representation space. The energy is the error in predicting \(s_y\) from \(s_x\).</p> <p>JEPA needs multi-modality, which in this context means to represent multiple possible values of \(y\). There are two ways it can be achieved.</p> <p>Encoder invariance: This means that \(s_y\) will be the same for different values of \(y\). The encoder ignores aspects of the state that may vary.</p> <p>Latent variable predictor: Varying \(z\) will lead to different plausible predictions of \(s_y\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/lv_jepa-480.webp 480w,/assets/img/blog/jepa/lv_jepa-800.webp 800w,/assets/img/blog/jepa/lv_jepa-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/lv_jepa.png" class="image-fluid mx-auto d-block" width="400" height="auto" alt="JEPA with a latent variable" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>There are four criteria that can be used to train this architecture without contrastive loss:</p> <ol> <li>Maximize the information content of \(s_x\) about \(x\): \(-I(s_x)\)</li> <li>Maximize the information content of \(s_x\) about \(y\): \(-I(s_y)\)</li> <li>Make \(s_y\) predictable from \(s_x\): \(D(s_y, \tilde{s_y})\)</li> <li>Minimize the information content of the latent variable with a regularizer: \(R(z)\)</li> </ol> <h3 id="hierarchical-jepa-h-jepa">Hierarchical JEPA (H-JEPA)</h3> <p>There is a trade off between information loss in the encoding and the predictability of the encodings. If a representation contains most of the information of the input, it would be hard to predict. A more abstract and higher level representation would be lower in dimension and more predictable. Higher dimension representations are also more suitable for longer term predictions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/hjepa-480.webp 480w,/assets/img/blog/jepa/hjepa-800.webp 800w,/assets/img/blog/jepa/hjepa-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/hjepa.png" class="image-fluid mx-auto d-block" width="500" height="auto" alt="H-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>H-JEPA (Hierarchical JEPA) enhances JEPA’s abstraction capabilities by splitting the architecture into two parts. The first JEPA handles low-level representations for short-term predictions, while the second operates at a higher abstraction level for longer-term forecasts. This two-tier structure, though innovative, is arbitrary. True intelligence requires multiple levels of abstraction. However, it is not clear how many levels of abstraction are needed. We may even need variable levels of abstraction. Different situations have different levels of complexity.</p> <p>This architecture can enable higher level planning. In JEPA-2, we can sample from the latent variable for several time steps. Directed search / pruning can be employed in order to efficiently search. This search can be used to determine an optimal action.</p> <p>This kind of search would be different in JEPA-1 or without H-JEPA because the latent dimension would be too large to efficiently sample from. Abstraction is needed to enable this kind of planning.</p> <h2 id="world-model-architecture">World Model Architecture</h2> <p>The world is unpredictable but the agent itself is predictable to the agent. This may motivate a model of self (ego model) that does not have a latent variable.</p> <p>The state of the world varies only slightly between time steps. Rather than regenerating, it can be updated in memory. With this architecture, the world model will only output the change in the state. This can be implemented with an attention-like mechanism.</p> <ol> <li>The world model outputs query value pairs: \((q[i], v[i])\)</li> <li>The world model retrieves a value from memory using the query <ul> <li> \[\mathrm{Mem}(q) = \sum_jc_jv_j\] <ul> <li>The value retrieved from memory is a weighted sum of all values.</li> </ul> </li> <li> \[\tilde{c}_j = \mathrm{Match}(k_j,q)\] <ul> <li>Measures dissimilarity between the key and query.</li> </ul> </li> <li> \[c = \mathrm{Normalize}(\tilde{c})\] <ul> <li>This is often a softmax.</li> </ul> </li> <li> \[v_j = \mathrm{Update}(r,v_j,c_j)\] <ul> <li>Value is updated using the current value and new value.</li> <li>The update function can be \(cr+(1-c)v\)</li> </ul> </li> </ul> </li> </ol> <h1 id="data-streams">Data Streams</h1> <p>In building a world model, we have to consider the fundamental differences in the type of data that humans and AI models process. Yann lists 5 modes of information gathering that an agent can use to learn its world model.</p> <ol> <li>Passive observation: sensor stream without control</li> <li>Action foveation: The agent can direct attention within the data stream</li> <li>Passive agency: Observing another agent’s actions and causal effects</li> <li>Active Egomotion: The sensors can be configured, for example moving a camera</li> <li>Active Agency: Sensory streams that are influenced by the agent’s actions</li> </ol> <p>Current AI methods largely focus on passive observation. Other modes may be needed to reach intelligence.</p> <p>AI is trained on internet data. Internet data is not experienced by the agent. Humans train on data that they experience. This is a fundamental difference. This is also why autonomous cars need so much training data. The AI driving systems don’t have other datasets that they have experienced. For example, if they trained on a large dataset of just walking around, they would need less driving data.</p> <p>It is challenging to create large-scale datasets from the perspective of an agent, especially reaching the scale of internet datasets. A present-day example is autonomous car datasets. AV companies have large fleets of vehicles on the road collecting data. These are active data streams.</p> <h1 id="objective-driven-ai">Objective Driven AI</h1> <p>The components of this architecture can be put together to build an intelligent system that follows human defined objectives.</p> <p>Perception is used to generate an initial representation of the state of the world. The actor proposes a sequence of actions. The world model then predicts the state reached if the action sequence is executed. This state is then used in the objectives. The task objective defines what we want the system to do. This could be a task or particular problem. The guardrail objective makes sure the system accomplishes the task without any unwanted behavior. These guardrails would be designed for safety.</p> <p>The action sequence is optimized with respect to the objects. There will be a lot of flexibility in designing the objects to get the system to behave in the way we want.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/objective_driven_ai-480.webp 480w,/assets/img/blog/jepa/objective_driven_ai-800.webp 800w,/assets/img/blog/jepa/objective_driven_ai-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/objective_driven_ai.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Objective Driven AI" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>The system can also be extended to achieve hierarchal planning. The higher levels of planning produce a state that will serve as an objective for the lower level. This state can be considered as a subgoal that is necessary to achieve the higher level goal. We can have unique objectives and guardrails for each level of planning.</p> <p>Latent variables are also introduced to represent the uncertainty in predictions of future states. The latent variables at the higher levels can be thought as imaginary higher level actions. However, only the lower level actions can actually be directly executed.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/h_objective_driven_ai-480.webp 480w,/assets/img/blog/jepa/h_objective_driven_ai-800.webp 800w,/assets/img/blog/jepa/h_objective_driven_ai-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/h_objective_driven_ai.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="Hierarchal Objective Driven AI" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <h1 id="towards-implementing-jepa">Towards Implementing JEPA</h1> <p>The JEPA paper is a position paper that describes a vision for AI that may take decades to materialize. However, since its publication in the summer of 2022, there have been a few steps in advancing the architecture. These papers particularly explore the training of JEPAs. They do not explore the other components such as planning. These JEPAs are the first steps to creating a world model.</p> <p>These are essentially self supervised pretraining methods. When comparing against other works, these papers cite training speed as their advantage. They can achieve strong downstream performance with fewer pretraining epochs.</p> <h2 id="i-jepa-self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture">I-JEPA: <a href="https://arxiv.org/abs/2301.08243" rel="external nofollow noopener" target="_blank">Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</a> </h2> <p>Compared to other image SSL approaches, I-JEPA takes advantage of the flexibility of the transformer architecture. ViT is used because it can handle an arbitrary amount of patches in an image, without requiring a strict shape in the input like CNNs</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/ijepa-480.webp 480w,/assets/img/blog/jepa/ijepa-800.webp 800w,/assets/img/blog/jepa/ijepa-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/ijepa.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="I-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2301.08243" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>The input image is split into \(N\) non-overlapping patches and fed into a target encoder \(f_{\theta}\) to compute patch representations. \(s_y = \{s_{y1} … s_{yN}\}\)</p> <p>\(M\) possibly overlapping blocks are sampled from these representations. These blocks are basically larger sections of the image that contain multiple patches.</p> <p>Context is generated by sampling a block (larger than the target blocks). When predicting a target from this context, the overlap with the target block is masked from the context. The network is trained to predict the representations of the target blocks given the context block, and position encodings for the target block. The position encodings are added to the input so that the model knows where the target is. It is just tasked with predicting representations at those positions.</p> <p>This architecture avoids collapse by having exponential moving average weights in the target encoder. This is the same approach used in data2vec and BYOL.</p> <p>The main hyperparameters introduced by this work is the scale and aspect ratio of the target and context blocks. Generally, a small context is used to make this task difficult, which would force the model to learn higher level and more useful features.</p> <h2 id="v-jepa-revisiting-feature-prediction-for-learning-visual-representations-from-video">V-JEPA: <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/" rel="external nofollow noopener" target="_blank">Revisiting Feature Prediction for Learning Visual Representations from Video</a> </h2> <p>V-JEPA is an extension of I-JEPA to videos. This is done by treating videos are 3d images.</p> <ol> <li>A clip of 64 frames (~2.1 seconds of video at 30 frames per second) is extracted from the video and resized to 16 × 224 × 224 × 3.</li> <li>The clip is split into \(L\) spatiotemporal patches of size 16x16x2 (2 is the number of consecutive frames.</li> <li> <p>A random mask is calculated for the context. This is a 2D that is similar to the mask in I-JEPA. This mask is then repeated across the time dimension. This repetition is necessary because the videos are short and there would be too much redundancy for the same patch at different time steps. This redundancy would make the learning task too easy. This masking creates a context image, while the target is the original image.</p> <ol> <li>2 masks are sampled: one short range and one long range. The short range mask covers less area in the image and is more discontinuous. These masks are constructed by different configurations of overlapping blocks, as done in I-JEPA. The target encoder only needs to run once, even if there are multiple masks for the context. Having multiple masks leads to more efficient training.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/vjepa_masking-480.webp 480w,/assets/img/blog/jepa/vjepa_masking-800.webp 800w,/assets/img/blog/jepa/vjepa_masking-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/vjepa_masking.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="V-JEPA masking" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> Short-range (left), long-range (right) <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <ol> <li>The tokens are processed by a transformer encoder (linear projection of patches + multiple transformer blocks). The masked out patches do not need to be processed. There is a separate encoder for the target and context. The target encoder is an EMA of the context encoder (same as I-JEPA).</li> <li>The predictor predicts the representations of the masked tokens by the unmasked tokens processed by the context encoder. The loss is the L1 distance between the representations of these masked tokens (from the target encoder, and the context encoder + predictor).</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/vjepa-480.webp 480w,/assets/img/blog/jepa/vjepa-800.webp 800w,/assets/img/blog/jepa/vjepa-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/vjepa.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="V-JEPA Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> Very similar to I-JEPA but with an added temporal dimension. <a href="https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>This is predicting gaps in short videos. It does not predict across time. Human learning is across the time dimension.</p> <p>Attentive probing is used to evaluate this model on different finetuning tasks. This is needed in place of linear probing since the input size may vary. This just requires learning a query token specific to the task and a linear classifier on top of the pretrained encoder.</p> <p>V-JEPA processes small sequences of frames. These short videos are essentially images with a little animation. However, that is the current state of video self-supervised learning. To achieve a model that is closer to human or even animal-level intelligence, this approach needs to scale up significantly. The resolution of the video needs to be increased. Also, the model needs to process longer durations of video and make predictions across time. For example, you should be able to predict what happens in the next 1 minute, based on the previous ten minutes of video input. Such a model could be the basis for an intelligent agent’s world model.</p> <p>V-JEPA is a very interesting model that may be the start of a highly important line of research.</p> <h2 id="mc-jepa-a-joint-embedding-predictive-architecture-for-self-supervised-learning-of-motion-and-content-features">MC-JEPA: <a href="https://arxiv.org/abs/2307.12698" rel="external nofollow noopener" target="_blank">A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</a> </h2> <p>This is an extension of JEPA to include motion information. It uses an optical flow objective to learn motion from videos and uses general SSL to learn about the content of the images/videos. Optical flow is estimating the direction in which pixels move between two consecutive frames of a video.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/mcjepa_architecture-480.webp 480w,/assets/img/blog/jepa/mcjepa_architecture-800.webp 800w,/assets/img/blog/jepa/mcjepa_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/mcjepa_architecture.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="MC-JEPA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2307.12698" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>The details of this dense flow estimation are out of the scope of this blog post. Flow estimation and content feature learning are combined as a multitask learning objective. Images are sampled for content learning, while consecutive frames are sampled from videos for flow estimation. The encoder is shared for both tasks. This is a JEPA architecture because the representations from one frame are warped to match the representations from the next frame. The same encoder is used to process both frames.</p> <p>The architecture for flow estimation is hierarchal. This may be the first instantiation of an H-JEPA architecture. This architecture is based on <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_PWC-Net_CNNs_for_CVPR_2018_paper.pdf" rel="external nofollow noopener" target="_blank">PWC-Net</a>. Each level has a different resolution.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/jepa/mcjepa_full_architecture-480.webp 480w,/assets/img/blog/jepa/mcjepa_full_architecture-800.webp 800w,/assets/img/blog/jepa/mcjepa_full_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/jepa/mcjepa_full_architecture.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="MC JEPA full architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2307.12698" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>The image features are sampled from ImageNet, while a video dataset is used for flow estimation. It is also possible to use frames from video as images for content learning.</p> <p>This work shows that the JEPA framework is generalizable. There are a lot of ways that we could design a world model and it could include many possible objectives.</p> <h2 id="whats-next">What’s Next?</h2> <p>The current research in JEPA represents a significant step towards Yann LeCun’s vision of building a world model capable of human-level AI. While the present focus is on creating effective representation learning models for visual data, the ultimate goal is far more ambitious. The holy grail of this research is a V-JEPA model that can predict across extended time horizons, potentially through a Hierarchical JEPA architecture capable of processing complex, lengthy videos like 10-minute YouTube clips.</p> <p>To realize this vision, several crucial advancements are necessary. Firstly, we need to embrace true multimodality, incorporating audio and other modalities that are often overlooked in current video models. Scaling up V-JEPA is also essential, requiring larger video datasets and more sophisticated model architectures that can handle higher resolutions. Additionally, the development of more challenging benchmarks for video understanding is critical, as current standards fall short of the complexity seen in image or language modeling tasks.</p> <p>Future iterations of V-JEPA must evolve beyond spatial masking to make predictions across various time horizons. This capability to forecast future representations based on present information is fundamental to understanding the temporal dynamics of video content. Achieving this may necessitate a hierarchical JEPA structure, where different levels handle predictions at various time scales and abstraction levels. Maybe the next JEPA paper will introduce a hierarchal video JEPA (HV-JEPA).</p> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Bandaru, Rohit (Jul 2024). Deep Dive into Yann LeCun’s JEPA. https://rohitbandaru.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">bandaru2024deep-dive-into-yann-lecun-s-jepa</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Deep Dive into Yann LeCun’s JEPA}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Bandaru, Rohit}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Jul}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://rohitbandaru.github.io/JEPA-Deep-Dive/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Rohit Bandaru. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?3e7054dc4d3e3dd8f0731a48453e618e"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?3577194613afa04501eb52f8f4164de9" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-blog",title:"blog",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-about",title:"about",description:"",section:"Navigation",handler:()=>{window.location.href="/about/"}},{id:"nav-notes",title:"notes",description:"",section:"Navigation",handler:()=>{window.location.href="/notes/"}},{id:"post-transformer-design-guide-part-1-vanilla",title:"Transformer Design Guide (Part 1: Vanilla)",description:"",section:"Posts",handler:()=>{window.location.href="/Transformer-Design-Guide-Pt1/"}},{id:"post-self-supervision-from-videos",title:"Self-Supervision from Videos",description:"",section:"Posts",handler:()=>{window.location.href="/Self-Supervision-from-Videos/"}},{id:"post-ssl-with-vision-transformers",title:"SSL with Vision Transformers",description:"",section:"Posts",handler:()=>{window.location.href="/SSL-with-Vision-Transformers/"}},{id:"post-deep-dive-into-yann-lecun-s-jepa",title:"Deep Dive into Yann LeCun\u2019s JEPA",description:"",section:"Posts",handler:()=>{window.location.href="/JEPA-Deep-Dive/"}},{id:"post-scaling-deep-learning",title:"Scaling Deep Learning",description:"",section:"Posts",handler:()=>{window.location.href="/Scaling-Deep-Learning/"}},{id:"post-knowledge-distillation-as-self-supervised-learning",title:"Knowledge Distillation as Self-Supervised Learning",description:"",section:"Posts",handler:()=>{window.location.href="/knowledge-distillation-ssl/"}},{id:"post-self-supervised-learning-getting-more-out-of-data",title:"Self-Supervised Learning\u200a -\u200a Getting more out of\xa0data",description:"",section:"Posts",handler:()=>{window.location.href="/Self-Supervised-Learning/"}},{id:"post-domain-adaptation",title:"Domain Adaptation",description:"",section:"Posts",handler:()=>{window.location.href="/Domain-Adaptation/"}},{id:"post-pruning-neural-networks",title:"Pruning Neural Networks",description:"",section:"Posts",handler:()=>{window.location.href="/Neural-Network-Pruning/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%72%6F%68.%62%61%6E%64%61%72%75@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/rohit-bandaru","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/rohit_bandaru","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>