<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="4zEwdc_gZtK7GAY69eUGNqwH6-hIgaXtASEwUuEDEho"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Variational Autoencoders: VAE to VQ-VAE / dVAE | Rohit Bandaru </title> <meta name="author" content="Rohit Bandaru"> <meta name="description" content="ML blog."> <meta name="keywords" content="vae, variational autoencoder, vq-vae, vector quantized vae, dvae, discrete vae, autoencoder, generative model, latent space, variational inference, elbo, evidence lower bound, kl divergence, reparameterization trick, gumbel-softmax, discrete latent variables, image generation, deep learning, machine learning, neural networks, representation learning, beta-vae, vqgan, dalle, openai, image compression, generative ai, diffusion models, ai art, deep generative models, latent embeddings, encoder, decoder, reconstruction loss, codebook, quantization, straight-through estimator, autoregressive models, pixelcnn, image synthesis, disentangled representations, posterior collapse, blurry images, continuous latent variables, discrete latent variables, categorical distribution, gumbel distribution, temperature annealing, hierarchical vae, text-to-image, conditional image generation, image editing, generative modeling, deep learning architectures, neural network training, model optimization, machine learning algorithms, ai research, deep learning research, generative ai research, variational inference tutorial, vae tutorial, vq-vae tutorial, dvae tutorial, image generation tutorial, deep learning blog, machine learning blog, ai blog, computer vision"> <meta property="og:site_name" content="Rohit Bandaru"> <meta property="og:type" content="article"> <meta property="og:title" content="Rohit Bandaru | Variational Autoencoders: VAE to VQ-VAE / dVAE"> <meta property="og:url" content="https://rohitbandaru.github.io/blog/VAEs/"> <meta property="og:description" content="ML blog."> <meta property="og:image" content="https://rohitbandaru.github.io/assets/img/blog/vae/vae.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Variational Autoencoders: VAE to VQ-VAE / dVAE"> <meta name="twitter:description" content="ML blog."> <meta name="twitter:image" content="https://rohitbandaru.github.io/assets/img/blog/vae/vae.png"> <meta name="twitter:site" content="@rohit_bandaru"> <meta name="twitter:creator" content="@rohit_bandaru"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Rohit Bandaru"
        },
        "url": "https://rohitbandaru.github.io/blog/VAEs/",
        "@type": "BlogPosting",
        "description": "ML blog.",
        "headline": "Variational Autoencoders: VAE to VQ-VAE / dVAE",
        
        "sameAs": ["https://www.linkedin.com/in/rohit-bandaru", "https://twitter.com/rohit_bandaru"],
        
        "name": "Rohit Bandaru",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/blog/favicon.ico?51e8729a60eb374d0910b53e1033a010"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rohitbandaru.github.io/blog/VAEs/"> <script src="/assets/js/theme.js?bd888c560287cd675855c7662a167c4a"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rohit</span> Bandaru </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">notes </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Variational Autoencoders: VAE to VQ-VAE / dVAE</h1> <p class="post-meta"> Created in February 10, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision,</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The autoencoder is a simple and intuitive machine learning architecture. It takes a high-dimensional input, uses an encoder to transform it into a lower-dimensional embedding, and then employs a decoder to learn the reverse transformation. The entire model is trained end-to-end with a reconstruction loss, such as mean squared error. We’ll refer to this as a vanilla autoencoder.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/vae/ae-480.webp 480w,/assets/img/blog/vae/ae-800.webp 800w,/assets/img/blog/vae/ae-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/vae/ae.png" class="image-fluid mx-auto d-block" width="450" height="auto" alt="Vanilla Autoencoder" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> </figcaption> </figure> <p>The vanilla autoencoder excels at compression and representation learning but faces challenges when used as a generative model. Due to the curse of dimensionality, we can’t effectively sample from the latent space of a vanilla autoencoder. The space of all possible latent representations is vast, with only a small portion occurring in the dataset. Consequently, most random points in this space would decode into noise.</p> <p>While it might be possible to sample from a low-dimensional latent space, if the dimension is too small, the model loses expressiveness and doesn’t scale well. This type of autoencoder is valuable for representation learning but falls short as an effective generative model. Variational autoencoders seek to address this.</p> <p>For the vanilla autoencoder, the loss is defined as \(L_{reconstruction}(x, g(f(x)))\). Where \(x\) is the input, \(f\) is the encoder, and \(g\) is the decoder. \(L\) is a reconstruction loss that is often set to mean squared error (MSE). The latent embedding can be represented as \(z = f(x)\). All autoencoders map observed data to latent embeddings where the latent dimension is a hyperparameter.</p> <p>This architecture is simple to implement and is useful for compression and representation learning. Variational autoencoders were introduced to address different deficiencies of this architecture, which we will cover.</p> <h1 id="variational-autoencoders-vae">Variational Autoencoders (VAE)</h1> <p>The goal of variational autoencoders is to constrain the latent space of an autoencoder so that it can be sampled from. VAEs use variational inference to create a probabilistic latent space. VAE was introduced in <a href="https://arxiv.org/abs/1312.6114" rel="external nofollow noopener" target="_blank">Auto-Encoding Variational Bayes</a> in 2013. Although the acronym VAE isn’t present in this paper.</p> <p>A simple way to construct this would be to have the encoder output the parameters to a Gaussian distribution. A mean and standard deviation are predicted for each latent dimension. Using these parameters, we can sample from the distribution as such.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/vae/naivevae-480.webp 480w,/assets/img/blog/vae/naivevae-800.webp 800w,/assets/img/blog/vae/naivevae-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/vae/naivevae.png" class="image-fluid mx-auto d-block" width="600" height="auto" alt="Naive Variational Autoencoder" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> </figcaption> </figure> <h2 id="reparameterization-trick">Reparameterization Trick</h2> <p>However, the architecture defined above is not trainable. The sample operation is not differentiable, so it would not be possible to train the encoder. A “reparameterization trick” is used to solve this.</p> \[z=\mu+\sigma\epsilon\] <p>This trick is a simple and clever implementation detail, since Gaussians are normally sampled in this way. We are sampling \(\epsilon\) and then treating the sampling of \(z\) as a deterministic process. This can be done only because \(\epsilon\) is independent of any parameters. The stochastic component is refactored to be a fixed distribution that has no dependencies for backpropagation.</p> <p>Although Gaussians are commonly used, any distribution that can be reparameterized like this, can be used in a VAE.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/vae/vae-480.webp 480w,/assets/img/blog/vae/vae-800.webp 800w,/assets/img/blog/vae/vae-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/vae/vae.png" class="image-fluid mx-auto d-block" width="600" height="auto" alt="Variational Autoencoder" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> </figcaption> </figure> <h2 id="forward-pass">Forward Pass</h2> <p>In order to understand how the VAE is trained, we need to introduce some mathematical notation.</p> <p>Encoder network \(q(z\mid x)\): For each input \(x_i\), this outputs vectors for mean \(\mu_i\) and log variance \(logvar_i\). These parameters are used to generate a latent embedding.</p> <p>Latent variable \(z\): This is computed using the reparameterization trick. \(z_i = μ_i + σ_i * ε\), where \(σ_i = \sqrt{\exp(logvar_i)}\) and \(ε ∼ N(0, I)\)</p> <p>Decoder network \(p(x\mid z)\): Given the latent embedding this outputs the parameters of the output distribution. This distribution can be a Gaussian distribution for continuous data (like natural images) or a Bernoulli distribution for binary data.</p> <p>This notation describes the forward pass of the VAE. We will now explain how this is trained.</p> <h2 id="loss">Loss</h2> <p>During training, we sample from the conditional distribution\(q(z\mid x)\). At test time, however, we need to use the decoder to generate new samples by sampling from the prior \(p(z)\). For this to work, the model’s marginal posterior \(q(z)\) must approximate a standard Gaussian prior \(p(z) = N(0, I)\). To achieve this while sampling from \(q(z\mid x)\), we add a KL divergence term to the loss function: \(\text{KL}(q(z\mid x) \mid\mid p(z))\). We use the reverse KL divergence specifically to ensure the approximate posterior places probability mass only where the prior does. This KL term has a closed form for Gaussian distributions:</p> \[\begin{aligned} \mathcal{L}_{KL}(x,z) &amp;= \text{KL}(q(z\mid x) \mid\mid p(z)) \\&amp;= \frac{1}{2} \sum_{i=1}^{d} (1 + \log \sigma_i^2 - \mu_i^2 - \sigma_i^2)\end{aligned}\] <p>The reconstruction loss is defined differently for Gaussian and Bernoulli output distributions. We will focus on the Gaussian form.</p> \[\mathcal{L}_{\text{reconstruction}}(x,z) = -\mathbb{E}_{z \sim q(z\mid x)} \left[ \log p(x\mid z) \right]\] <p>In practice this is implemented as the MSE between the mean of the output \(p(x\mid z)\) and the input \(x\). We can expand the log probability as follows (\(\mu_q\) and \(\sigma_q\) represents the outputs of the decoder):</p> \[\log p(x\mid z) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma_q(z)^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu_q(z))^2\] <p>We can take the mean and drop the constants and optimize the MSE:</p> \[- \sum_{i=1}^n (x_i - \mu_q(z))^2\] <p>The optimization process combines both losses as follows:</p> <ol> <li>Take a batch of input examples (images): \(x_i\).</li> <li>Use the encoder to generate mean and log variance parameters: \(\mu_i\) , \(logvar_i\).</li> <li>Sample \(\epsilon\) to generate latent embeddings \(z_i\) for each example using the reparameterization trick.</li> <li>Pass the latent embeddings through the decoder and take the mean of the output distribution or reconstruction: \(\hat{x}_i\).</li> <li>Calculate the reconstruction loss between the decoder’s output mean and the inputs \(x_i\). Calculate the KL loss using the mean and variance from the encoder. Sum these losses and backpropagate through both networks.</li> </ol> <h2 id="generation">Generation</h2> <p>To generate examples with a VAE, we only use the decoder. First, we sample \(z\) from the prior \(p(z)\), which is set to the unit Gaussian. This sampled value passes through the decoder to produce the distribution \(p(x\mid z)\), which can be either Gaussian or Bernoulli. For images, we typically don’t sample from \(p(x\mid z)\) since it represents pixel-level probabilities and sampling from it would only add noise. By sampling different values of \(z\), we can generate diverse images.</p> <h2 id="elbo">ELBO</h2> <p>The loss we defined for VAE is known as the Expected Lower Bound (ELBO). This can be formulated as follows:</p> \[\text{ELBO} = -\mathcal{L}_{\text{reconstruction}} + -\mathcal{L}_{\text{KL}} = \mathbb{E}_{z \sim q(z\mid x_i)}[\log p(x_i\mid z)] - \text{KL}(q(z\mid x_i) \mid\mid p(z))\] <p>We take the negations of the losses for ELBO because it is a lower bound that we want to maximize through gradient ascent. However, in practice we may just minimize negative ELBO.</p> <p>We have arrived at ELBO from a practical standpoint. This can also be derived through variational inference. We will now go deeper into the mathematical intuitions of ELBO and VAE.</p> <p>We can start by considering training the decoder by maximizing the likelihood:</p> \[\mathbb{E}_{z \sim q(z\mid x_i)} \left[ \log p (x_i, z) \right]\] <p>This is intractable because it requires taking an integral over the continuous latent variable. Instead of optimizing the intractable likelihood, we can formulate a lower bound. Maximizing this lower bound is a proxy to optimizing the likelihood. Variational inference is a mathematical framework that approximates complex probability distributions by optimizing a simpler distribution to be as close as possible to the target distribution.</p> <h3 id="derivations">Derivations</h3> <p>In unsupervised learning we want to increase the probability of the data, which we express as \(\mathrm{log}(p(x_i))\).</p> <p>We can first express the log probability as an expectation with respect to the latent variable. This is done by expanding \(p(x)\) as an integral over \(z\). We then multiply the numerator and denominator by \(q(z)\) and rearrange these terms:</p> \[\begin{aligned}\mathrm{log}(p(x_i)) &amp;=\log\int p(x_i\mid z) p(z) dz\\ &amp;= \log\int p(x_i\mid z) p(z) \frac{q_i(z\mid x_i)}{q_i(z\mid x_i)} dz \\ &amp;= \mathbb{E}_{z \sim q(z\mid x_i)} [\frac{p(x_i\mid z)p_i(z)}{q_i(z\mid x_i)}] \end{aligned}\] <p>Jensen’s Inequality states that for any concave function \(f\): \(f(E[y]) \geq E[f(y)]\). This applies to the log likelihood because \(\log\) is a concave function. We can apply Jensen’s inequality to the expectation we have derived.</p> \[\log \mathbb{E}_{z \sim q(z\mid x_i)} [\frac{p(x_i\mid z)p_i(z)}{q(z\mid x_i)}] \geq \mathbb{E}_{z \sim q(z\mid x_i)} \log[\frac{p(x_i\mid z)p_i(z)}{q(z\mid x_i)}]\] <p>The quantity on the right represents the lower bound on the log probability. We can rearrange the terms on the right for a more interpretable formula for ELBO.</p> \[\begin{aligned} &amp;\geq \mathbb{E}_{z \sim q(z\mid x_i)} [\log(p(x_i\mid z)) + \log(\frac{p(z)}{q_i(z\mid x_i)})] \\ &amp;\geq \mathbb{E}_{z \sim q(z\mid x_i)} \log(p(x_i\mid z)) + \mathbb{E}_{z \sim q(z\mid x_i)} \log(\frac{p(z)}{q_i(z\mid x_i)}) \\ &amp;\geq \mathbb{E}_{z \sim q(z\mid x_i)} \log(p(x_i\mid z)) - \text{KL}(q_i(z\mid x_i) \mid\mid p(z)) \end{aligned}\] <p>The first term is the reconstruction loss; the second is the KL term.</p> <p><strong>Alternative</strong></p> <p>It is also possible to derive ELBO from the definition of KL divergence. The first step to this derivation is the same. However, we continue by applying Bayes’ rule instead.</p> <p>We start with the definition of KL divergence between the posterior and approximate posterior:</p> \[\mathrm{KL}(q(z\mid x_i) \mid\mid p(z\mid x_i)) = \mathbb{E}_{z \sim q(z\mid x_i)}[\log \frac{q(z\mid x_i)}{p(z\mid x_i)}]\] <p>Using Bayes’ rule, we can express the posterior as:</p> \[p(z\mid x_i) = \frac{p(x_i\mid z)p(z)}{p(x_i)}\] <p>Substituting this into the KL divergence:</p> \[\begin{aligned} \mathrm{KL}(q(z\mid x_i) \mid\mid p(z\mid x_i)) &amp;= \mathbb{E}_{z \sim q(z\mid x_i)}[\log \frac{q(z\mid x_i)p(x_i)}{p(x_i\mid z)p(z)}] \\ &amp;= \mathbb{E}_{z \sim q(z\mid x_i)}[\log q(z\mid x_i) - \log p(x_i\mid z) - \log p(z) + \log p(x_i)] \\ &amp;= \mathbb{E}_{z \sim q(z\mid x_i)}[\log q(z\mid x_i) - \log p(x_i\mid z) - \log p(z)] + \log p(x_i) \end{aligned}\] <p>We are able to remove \(p(x)\) from the expectation since it is the only term that doesn’t depend on \(z\). We can now rearrange the terms to isolate \(\log p(x)\). We want to place a bound on the quantity, like we did in the first derivation.</p> \[\begin{aligned} \log p(x_i) &amp;= \mathbb{E}_{z \sim q(z\mid x_i)}[\log p(x_i\mid z)] - \mathrm{KL}(q(z\mid x_i) \mid\mid p(z)) + \mathrm{KL}(q(z\mid x_i) \mid\mid p(z\mid x_i)) \\ &amp;\geq \mathbb{E}_{z \sim q(z\mid x_i)}[\log p(x_i\mid z)] - \mathrm{KL}(q(z\mid x_i) \mid\mid p(z)) \end{aligned}\] <p>We drop the last term since KL is non-negative. We are able to get to the same lower bound.</p> <h3 id="intuitions-of-elbo">Intuitions of ELBO</h3> <p>Now that we have derived ELBO, we will explain the intuition of its terms. We can first decompose the KL term.</p> \[\begin{aligned} \text{KL}(q(z\mid x_i) \mid\mid p(z)) &amp;= \mathbb{E}_{z \sim q(z\mid x_i)} \left[ \log \frac{q(z\mid x_i)}{p(z)} \right] \\ &amp;= \mathbb{E}_{z \sim q(z\mid x_i)} \left[ \log q(z\mid x_i) - \log p(z) \right] \\ &amp;= \mathbb{E}_{z \sim q(z\mid x_i)} \left[ \log q(z\mid x_i) \right] - \mathbb{E}_{z \sim q(z\mid x_i)} \left[ \log p(z) \right] \end{aligned}\] <p>The first term, \(\mathbb{E}_{z \sim q(z\mid x_i)} \left[ \log q(z\mid x_i) \right]\), is the negative entropy of the approximate posterior distribution. We want to increase the entropy of the decoder distribution so that we get diverse latent embeddings. This encourages the model to give probability to a wider range of latent embeddings and not overfit to precise values. This makes the VAE able to generalize better and make it easier to sample from.</p> <p>The second term, \(\mathbb{E}_{z \sim q(z\mid x_i)} \left[ \log p(z) \right]\), is the cross-entropy between the approximate posterior and the prior. This term encourages the approximate posterior to be similar to the prior distribution. Since we typically use a unit Gaussian as the prior, this term pushes the approximate posterior toward a standard normal distribution. It is important that this distribution is close to the distribution we will be sampling from.</p> <p>We use the reverse KL to strongly penalize \(q\) taking non zero values when \(p\) is zero. This encourages the supports of each distribution to be more aligned. We don’t want to generate latent embeddings during training that would be low probability during inference / generation.</p> <h2 id="problems-in-vae">Problems in VAE</h2> <p>Although it is a powerful architecture, there are several weaknesses to the VAE that follow up research seeks to address.</p> <ul> <li> <strong>Blurry images</strong>: VAEs tend to produce blurry output images due to their continuous latent embeddings.</li> <li> <strong>Posterior collapse</strong>: This occurs when the decoder ignores the latent embedding. If a decoder is too powerful, it can generate high-quality images without using information from the latent space. While this doesn’t harm image generation itself, it renders the latent embedding meaningless as a representation.</li> <li> <strong>Entanglement of the latent embedding</strong>: The latent dimensions in a VAE often become entangled, meaning that individual dimensions do not correspond to interpretable features. This makes it difficult to control specific attributes during generation or perform meaningful latent space manipulation.</li> </ul> <h1 id="β-vae"><a href="https://openreview.net/forum?id=Sy2fzU9gl" rel="external nofollow noopener" target="_blank"><strong>β-VAE</strong></a></h1> <p>This work by <a href="https://openreview.net/forum?id=Sy2fzU9gl" rel="external nofollow noopener" target="_blank">Higgins et al. 2017</a> explores a simple change to the VAE ELBO loss, by simply adding a weight \(\beta\) to the KL term of the loss:</p> \[\text{ELBO}_{\beta} = \mathbb{E}_{q(z\mid x_i)}[\log p(x_i\mid z)] - \beta\text{KL}(q(z\mid x_i) \mid\mid p(z))\] <p>In the paper, they set \(\beta &gt; 1\) and observe that it encourages more disentangled representations. This is because the prior \(p(z)\) is a multivariate distribution where each latent variable is completely independent of others. This also constrains the information capacity of the embedding, which forces the encoder to learn more of the underlying factors of the data. This is built on the assumption that disentangled representations are more efficient than entangled ones.</p> <p>The authors point out that a latent embedding with independent factors isn’t necessarily disentangled. Entangled representations can be expressed as independent factors through PCA. We want these factors to be semantically interpretable.</p> <h1 id="vector-quantized-vae-vq-vae"><a href="https://arxiv.org/abs/1711.00937" rel="external nofollow noopener" target="_blank">Vector-Quantized VAE (VQ-VAE)</a></h1> <p>The latent embedding of the VAE is continuous. However, for some applications, we want to have a discrete latent representation. This can be used as categorical features in a downstream model. This can also be used for autoregressive modeling if the discrete values can be used to create a vocabulary.</p> <p><a href="https://arxiv.org/abs/1711.00937" rel="external nofollow noopener" target="_blank">van den Oord et al. (2017)</a> introduce VQ-VAE to address this problem.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/vae/vqvae_paper-480.webp 480w,/assets/img/blog/vae/vqvae_paper-800.webp 800w,/assets/img/blog/vae/vqvae_paper-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/vae/vqvae_paper.png" class="image-fluid mx-auto d-block" width="600" height="auto" alt="Vector-Quantized Variational Autoencoder" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1711.00937" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p><strong>Forward Pass</strong></p> <ol> <li>A CNN encoder maps the input image to a grid of latent embeddings. These are deterministic and do not require sampling through reparameterization.</li> <li>For each embedding, we find the closest embedding in the codebook. We replace the embedding with this codebook embedding.</li> <li>A CNN decoder reconstructs the image from the codebook embeddings.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/vae/vqvae-480.webp 480w,/assets/img/blog/vae/vqvae-800.webp 800w,/assets/img/blog/vae/vqvae-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/vae/vqvae.png" class="image-fluid mx-auto d-block" width="600" height="auto" alt="VQ-VAE" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> </figcaption> </figure> <p>The encoder generates an embedding \(z_e(x)\). This is matched to a codebook embedding \(e_j\). \(z_q(e)\) represents the quantized embedding. The encoder and quantization can be interpreted as a categorical probability distribution:</p> \[q(z=k\mid x) = \begin{cases} 1 &amp; \text{for } k = \text{argmin}_j \mid\mid z_e(x) - e_j\mid\mid_2, \\0 &amp; \text{otherwise}.\end{cases}\] <p>The decoder reconstructs the input from the quantized embedding: \(p(x\mid z_q(x))\)</p> <p><strong>Training</strong></p> <p>The VQ-VAE is trained with three loss terms:</p> \[L = \log p(x\mid z_q(x)) + \mid\mid\text{sg}[z_e(x)] - e\mid\mid_2^2 + \beta \mid\mid z_e(x) - \text{sg}[e]\mid\mid_2^2.\] <ol> <li>The first term is the reconstruction loss, which is used to train the decoder.</li> <li>The second term is the codebook loss which pushes the codebook embedding closer to the encoder’s output embedding.</li> <li>The third term is the commitment loss that pushes the encoder’s output embedding closer to the quantized embedding.</li> </ol> <p>This loss term is meant to optimize reconstruction while also improving the quality of the quantization. There is no KL term as in the standard VAE. The second and third terms replace the KL term as it forces the encoder to match the prior of a categorical distribution.</p> <p>We also want the quantization to be close to the prior, which is a uniform categorical distribution of the latent codes. We can consider the KL divergence of the quantized encoder with this prior:</p> \[\text{KL}(q(z\mid x) \mid\mid p(z)) = \sum_{z} q(z\mid x) \log \left( \frac{q(z\mid x)}{p(z)} \right)\] <p>From the prior we get \(p(z=k) = \frac{1}{K}\). From the quantization operation we get \(q(z=k^*\mid x) = 1\) for some code \(k^*\), and 0 for all other values. We can use this to remove the summation.</p> \[\begin{aligned} \text{KL}(q(z\mid x) \mid\mid p(z)) &amp;= \sum_{z} q(z\mid x) \log \left( \frac{q(z\mid x)}{\frac{1}{K}} \right) \\ \text{KL}(q(z\mid x) \mid\mid p(z)) &amp;= q(z=k^*\mid x) \log \left( \frac{q(z=k^*\mid x)}{\frac{1}{K}} \right) \\\text{KL}(q(z\mid x) \mid\mid p(z)) &amp;= 1 \cdot \log \left( \frac{1}{\frac{1}{K}} \right) \\\text{KL}(q(z\mid x) \mid\mid p(z)) &amp;= \log K\end{aligned}\] <p>We arrive at a constant value which we can ignore in the optimization. However, we still have a potential issue with VQ-VAE in code imbalance, where certain codes are used more often than others. This could lead to posterior collapse.</p> <p>One important detail to note is that the quantization operation is non-differentiable. The authors use a <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/1308.3432&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=6295853094428885492&amp;ei=yv6gZ5P8L5-_6rQPqLWgoAM&amp;scisig=AFWwaeZCpj0kVnGHwUPWSs36ai3N" rel="external nofollow noopener" target="_blank">straight through estimator</a> to address this. This simply involves copying the gradients from the codebook embedding \(z_q(x)\) to the unquantized embedding \(z_e(x)\). As the model trains, these two embeddings become closer together and the gradients become more accurate.</p> <p><strong>Generation</strong></p> <p>In VQ-VAE we don’t sample from the categorical distribution of codes directly. The encoder maps each image to a fixed number of codes (ex: 128) with an order. We can use these codes to train an autoregressive model like <a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper_files/paper/2016/file/b1301141feffabac455e1f90a7de2054-Paper.pdf&amp;hl=en&amp;sa=T&amp;oi=gsr-r-gga&amp;ct=res&amp;cd=0&amp;d=8587297613215686995&amp;ei=QQShZ-LSK7ml6rQPn6KtOA&amp;scisig=AFWwaeYETmh4x2FTe4m5xariXaxF" rel="external nofollow noopener" target="_blank">PixelCNN</a>. We can sample latent codes from this model through ancestral sampling. These codes are then fed into the VQ-VAE decoder to generate an image. VQ-VAE is powerful in that it enables usage of powerful autoregressive models. More recently it is common to use transforms to model in the latent space.</p> <h1 id="discrete-vae">Discrete VAE</h1> <p>The discrete variational autoencoder (dVAE), used in OpenAI’s image generation model <a href="https://arxiv.org/abs/2102.12092" rel="external nofollow noopener" target="_blank">DALLE</a>, provides an alternative to vector quantization. In VQ-VAE the encoder maps the images to a grid of vectors which are then quantized. In dVAE, the encoder maps the inputs directly to the discrete encodings. Instead of a grid of vectors, we get a grid of scalars.</p> <p>The encoder first maps inputs to grids of vectors, as done in VQ-VAE. However, these are treated as one-hot encoding from a categorical distribution. The argmax is then taken to discretize this encoding into a scalar. These scalars can be auto-regressively modeled and then sampled from.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/vae/dvae-480.webp 480w,/assets/img/blog/vae/dvae-800.webp 800w,/assets/img/blog/vae/dvae-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/vae/dvae.png" class="image-fluid mx-auto d-block" width="600" height="auto" alt="Discrete VAE" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> </figcaption> </figure> <h2 id="gumbel-softmax">Gumbel-Softmax</h2> <p>In order to make this trainable, we use the Gumbel-Softmax relaxation. This relaxation for VAEs is introduced concurrently in the papers <a href="https://arxiv.org/abs/1611.01144" rel="external nofollow noopener" target="_blank">Jang et al. 2016</a> and <a href="https://arxiv.org/abs/1611.00712" rel="external nofollow noopener" target="_blank">Maddison et al. 2016</a> (<a href="https://blog.evjang.com/2016/11/tutorial-categorical-variational.html" rel="external nofollow noopener" target="_blank">Eric Jang blog post</a>). The Gumbel-Softmax can be defined as follows:</p> \[z = \text{one_hot}(\text{argmax}_i[g_i+\log\pi_i])\] <p>\(i\) is the index in the one-hot encoding, which corresponds to the number of discrete codes. \(\pi_i\) is the probability of code \(i\) in the encoding.</p> <p>\(g_i\) is a sample from the Gumbel distribution, also referred to as Gumbel noise. It is sampled as follows: \(g = -\log(-\log(u))\) where \(u \sim \text{Uniform}(0,1)\). This is applying the reparameterization trick to sampling from a categorical distribution. \(g_i\) is analogous to \(\sigma \epsilon\) in VAE. This noise also makes sure that codes that aren’t the max are selected and trained on.</p> <p>This argmax can be approximated as a softmax:</p> \[y_i = \frac{\exp((\log(\pi_i) + g_i)/\tau)}{\sum_{j=1}^{k} \exp((\log(\pi_j) + g_j)/\tau)} \quad \text{for } i = 1, \dots, k.\] <p>During training the temperature \(\tau\) is gradually reduced. As it approaches 0, the softmax approaches the argmax. We can then use an argmax for inference / generation. The temperature is annealed during training and not used during inference. At inference, we also do not need to add the Gumbel noise.</p> <p>dVAE and VQ-VAE are similar architectures with a key difference in how they discretize the data.</p> <h1 id="conclusion">Conclusion</h1> <p>This blog post explained the evolution of autoencoders to variational autoencoders to vector quantized variational autoencoders. This just covers the fundamental mathematical intuitions. The VAE and vector quantization are malleable frameworks that can be used in a variety of settings and within many variations of architecture. The <a href="https://arxiv.org/abs/2012.09841" rel="external nofollow noopener" target="_blank">VQGAN</a> builds upon VQ-VAE by adding a discriminator to improve the image generation. <a href="https://arxiv.org/abs/1906.00446v1" rel="external nofollow noopener" target="_blank">VQ-VAE-2</a> improves upon the original by using a hierarchical approach with multiple levels of latent codes to capture both fine and coarse details. Although these models are most known for image generation, they are also very useful for discrete representation learning. This blog post focuses on the VAE and the encoding of images. There are also methods of additional conditions to the image generation, such as text prompts.</p> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Bandaru, Rohit (Feb 2025). Variational Autoencoders: VAE to VQ-VAE / dVAE. https://rohitbandaru.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">bandaru2025variational-autoencoders-vae-to-vq-vae-dvae</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Variational Autoencoders: VAE to VQ-VAE / dVAE}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Bandaru, Rohit}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Feb}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://rohitbandaru.github.io/blog/VAEs/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Rohit Bandaru. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?3e7054dc4d3e3dd8f0731a48453e618e"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?3577194613afa04501eb52f8f4164de9" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-X48NHDB5RV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-X48NHDB5RV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-notes",title:"notes",description:"",section:"Navigation",handler:()=>{window.location.href="/notes/"}},{id:"post-variational-autoencoders-vae-to-vq-vae-dvae",title:"Variational Autoencoders: VAE to VQ-VAE / dVAE",description:"",section:"Posts",handler:()=>{window.location.href="/blog/VAEs/"}},{id:"post-transformer-design-guide-part-2-modern-architecture",title:"Transformer Design Guide (Part 2: Modern Architecture)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Transformer-Design-Guide-Pt2/"}},{id:"post-transformer-design-guide-part-1-vanilla",title:"Transformer Design Guide (Part 1: Vanilla)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Transformer-Design-Guide-Pt1/"}},{id:"post-self-supervision-from-videos",title:"Self-Supervision from Videos",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Self-Supervision-from-Videos/"}},{id:"post-ssl-with-vision-transformers",title:"SSL with Vision Transformers",description:"",section:"Posts",handler:()=>{window.location.href="/blog/SSL-with-Vision-Transformers/"}},{id:"post-deep-dive-into-yann-lecun-s-jepa",title:"Deep Dive into Yann LeCun\u2019s JEPA",description:"",section:"Posts",handler:()=>{window.location.href="/blog/JEPA-Deep-Dive/"}},{id:"post-scaling-deep-learning",title:"Scaling Deep Learning",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Scaling-Deep-Learning/"}},{id:"post-knowledge-distillation-as-self-supervised-learning",title:"Knowledge Distillation as Self-Supervised Learning",description:"",section:"Posts",handler:()=>{window.location.href="/blog/knowledge-distillation-ssl/"}},{id:"post-self-supervised-learning-getting-more-out-of-data",title:"Self-Supervised Learning\u200a -\u200a Getting more out of\xa0data",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Self-Supervised-Learning/"}},{id:"post-domain-adaptation",title:"Domain Adaptation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Domain-Adaptation/"}},{id:"post-pruning-neural-networks",title:"Pruning Neural Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Neural-Network-Pruning/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%72%6F%68.%62%61%6E%64%61%72%75@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/rohit-bandaru","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/rohit_bandaru","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>