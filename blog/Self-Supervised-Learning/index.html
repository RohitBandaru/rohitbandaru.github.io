<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="4zEwdc_gZtK7GAY69eUGNqwH6-hIgaXtASEwUuEDEho"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Self-Supervised Learning  -  Getting more out of data | Rohit Bandaru </title> <meta name="author" content="Rohit Bandaru"> <meta name="description" content="ML blog."> <meta name="keywords" content="machine-learning, ai, artificial-intelligence, research"> <meta property="og:site_name" content="Rohit Bandaru"> <meta property="og:type" content="article"> <meta property="og:title" content="Rohit Bandaru | Self-Supervised Learning  -  Getting more out of data"> <meta property="og:url" content="https://rohitbandaru.github.io/blog/Self-Supervised-Learning/"> <meta property="og:description" content="ML blog."> <meta property="og:image" content="https://rohitbandaru.github.io/assets/img/blog/self_supervised_learning/byol.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Self-Supervised Learning  -  Getting more out of data"> <meta name="twitter:description" content="ML blog."> <meta name="twitter:image" content="https://rohitbandaru.github.io/assets/img/blog/self_supervised_learning/byol.png"> <meta name="twitter:site" content="@rohit_bandaru"> <meta name="twitter:creator" content="@rohit_bandaru"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Rohit Bandaru"
        },
        "url": "https://rohitbandaru.github.io/blog/Self-Supervised-Learning/",
        "@type": "BlogPosting",
        "description": "ML blog.",
        "headline": "Self-Supervised Learning  -  Getting more out of data",
        
        "sameAs": ["https://www.linkedin.com/in/rohit-bandaru", "https://twitter.com/rohit_bandaru"],
        
        "name": "Rohit Bandaru",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rohitbandaru.github.io/blog/Self-Supervised-Learning/"> <script src="/assets/js/theme.js?bd888c560287cd675855c7662a167c4a"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rohit</span> Bandaru </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">notes </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Self-Supervised Learning  -  Getting more out of data</h1> <p class="post-meta"> Created in August 14, 2021 </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a>   <a href="/blog/tag/self-supervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> self-supervised-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Yann LeCun <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/" rel="external nofollow noopener" target="_blank">describes</a> self-supervised learning as the next big challenge in the field of AI. How does it work? Self-supervised learning (SSL) is a specific type of unsupervised learning. It aims to learn from large datasets of unlabeled data to enable building more robust models in different domains such as vision and NLP.</p> <p>For many computer vision problems, it is a common practice to pretrain the model on a supervised learning task. For example, there are <a href="https://keras.io/api/applications/" rel="external nofollow noopener" target="_blank">many</a> neural networks that are pretrained to do image classification on ImageNet. However, self-supervised learning has recently been shown to outperform supervised pretraining learning on certain tasks. SSL is an active area of research with heavy involvement from top AI labs in Google, Facebook, Deepmind, and academia. Rather than focusing on the details of SSL architectures, we will explore the intuitions on why it works and what is needed.</p> <h1 id="code">Code</h1> <p>In order to make it easy to directly interact with SSL, I wrote a Colab notebook showing a few algorithms. This notebook demonstrates transfer learning on CPC, SwAV, and SimCLR pretrained models on the CIFAR10 classification task. This uses PyTorch Lightning’s implementations of these algorithms.</p> <p><a href="https://colab.research.google.com/drive/1PDCTe5dIQgYyiuLQw3WGyCuT03gX1qZR?usp=sharing" rel="external nofollow noopener" target="_blank"> <img src="/blog/assets/img/colab.svg" alt="Open In Colab"> </a></p> <p>We are experimenting with the simple example of pretraining on ImageNet and evaluating on CIFAR10 classification. SSL can be effective on other datasets and learning tasks (object detection, segmentation, etc.), but these won’t be the focus of this post.</p> <h1 id="motivations">Motivations</h1> <h2 id="data">Data</h2> <p>Self-supervised learning does not need labels. The amount of unlabeled data generally far exceeds the amount of labeled data. SSL can leverage large amounts of unlabeled data to build powerful models. Although most research does not use datasets larger than ImageNet, there are real world applications of using larger unlabeled datasets. For example, Facebook/Meta can train the <a href="https://ai.facebook.com/blog/seer-the-start-of-a-more-powerful-flexible-and-accessible-era-for-computer-vision/" rel="external nofollow noopener" target="_blank">SEER</a> model on billions of Instagram images.</p> <h2 id="generalizability">Generalizability</h2> <p>If you train a model on image classification, it may not perform as well on non-classification tasks. This is because only part of the image’s information is needed to classify it. A self-supervised learning algorithm may be able to use more of the information in the data.</p> <p>The reason for the generalization gap is that the classification task does not always require a strong understanding of the object. For example, if you trained a supervised model to classify dog breeds, it may only look at the texture and color of the dog’s fur. In order to classify the breeds, the network may not need to understand other characteristics of the dog, such as size and facial features. This model would then not generalize well if you want to add a new dog breed with an indistinctive skin pattern. It will also not generalize well to new tasks like classifying the size or shape of the dog.</p> <h2 id="better-performance">Better Performance</h2> <p>It is common to think that unsupervised / self-supervised learning is only useful when you lack labels to do supervised learning. However, these approaches can actually increase performance compared to a fully supervised approach. The ability to learn more accurate and robust models is what gives self-supervised learning the potential to shift the field of AI.</p> <p>In research, there are comparisons between training on ImageNet images and labels with supervised learning and ImageNet with only images for self-supervised learning. Although the motivation for SSL is often framed as being able to use more data, in this case, the size of the dataset is the same. The ability to use larger unlabeled datasets is just a side benefit of SSL.</p> <h1 id="vision-vs-nlp">Vision vs NLP</h1> <p>Self-supervised learning has been long applied in NLP, but as Yann LeCun and Ishan Misra point <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/" rel="external nofollow noopener" target="_blank">out</a>, it is much harder to apply to vision. In NLP, language models are often trained with self supervision. Given a some text, you can mask a word and try to predict it given the rest of the text. There is a limited vocabulary, so you can assign a probability to each word. This is the basis of many popular NLP methods.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/nlp-480.webp 480w,/assets/img/blog/self_supervised_learning/nlp-800.webp 800w,/assets/img/blog/self_supervised_learning/nlp-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/self_supervised_learning/nlp.png" class="img-fluid mx-auto d-block" width="400" height="auto" alt="Predicting masked words in NLP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> Predicting masked words in NLP </figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/image_patch-480.webp 480w,/assets/img/blog/self_supervised_learning/image_patch-800.webp 800w,/assets/img/blog/self_supervised_learning/image_patch-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/self_supervised_learning/image_patch.png" class="img-fluid mx-auto d-block" width="300" height="auto" alt="Image SSL with patches" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> Predicting patches of an image is much harder. </figcaption> </figure> <p>The analogue for vision is to mask a patch of an image and try to fill it in. However, because there is an intractable number of possible ways to fill in an image, you can’t compute a probability for each one. There can also be a large number of possible solutions. For example, in the image above, there is many facial expressions the dog could have. The NLP approach is straight forward but cannot be directly applied to vision.</p> <h1 id="pretext-task">Pretext Task</h1> <p>The earlier approaches to self-supervised learning focused on training the network on a pretext task. This task would not require labels in the label. The labels will be made up through the task. In <a href="https://arxiv.org/abs/2012.01985" rel="external nofollow noopener" target="_blank">RotNet</a>, each image is rotated by 0, 90, 180, or 270 degrees, and a network is trained to predict the rotation. In <a href="https://arxiv.org/abs/1603.09246" rel="external nofollow noopener" target="_blank">Jigsaw</a>, the image is split up into patches and scrambled like a jigsaw puzzle. A network is then trained to solve the puzzle by predicting the permutation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/rotnet-480.webp 480w,/assets/img/blog/self_supervised_learning/rotnet-800.webp 800w,/assets/img/blog/self_supervised_learning/rotnet-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/self_supervised_learning/rotnet.png" width="100%" height="auto" alt="RotNet, SSL by predicting rotations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1803.07728" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>The problem with pretext task-based SSL is the same as supervised learning. There can be shortcuts to achieve high accuracy on the task. There have been attempts to avoid this. For example, in Jigsaw, each path is randomly cropped, so the task can’t be solved by simply lining up edges. However, the limitations still exist regardless, so more recent research has focused on contrastive learning.</p> <h1 id="contrastive-learning">Contrastive Learning</h1> <p>A neural network outputs a vector representation for every image. The goal of contrastive learning is to push these vectors closer for similar images and pull them apart unrelated images. This in different ways in different research papers.</p> <h2 id="cpc"><a href="https://arxiv.org/abs/1807.03748" rel="external nofollow noopener" target="_blank">CPC</a></h2> <p>Contrastive Predictive Coding is a method developed by Deepmind. It is a generic approach that can be applied to any data modality. In the paper, it is applied to images, audio, and text. It is a very general framework with two main components: an encoder, and an autoregressive model. These can be anything and are designed to fit the domain.</p> <p>The encoder simply encodes the data into a lower-dimensional vector \(z_t\). This can be any model. For images, this can be a convolutional neural network.</p> <p>Autoregressive models the variables in the data are given an order. In images, the pixels can be ordered from left to right and top to bottom. We can imagine unrolling each datapoint (ex: image, audio clip) into a list. We can call each element of this list an observation. CPC encodes a sequence of observations \(X\) into a sequence of encodings \(Z\).</p> \[X = [x_1, x_2, x_3, x_4 ... x_N]\\ Z = [z_1, z_2, z_3, z_4 ... z_N]\\ z_t = g_{enc}(x_t)\] <p>The prediction of an observation in the sequence depends only on the previous observations. This similar to predicting the future from the past in a time series. In CPC, the autoregressive model is used to generate context vectors from the encodings \(z_t\). Context vector \(c_t\) is a function of encodings \(z_{\leq t}\), but not any encoding after \(z_t\). Note that the autoregressive model is trying to predict the encodings of the observations, but not the observations themselves. The architecture of this autoregressive model depends on the application.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/cpc-480.webp 480w,/assets/img/blog/self_supervised_learning/cpc-800.webp 800w,/assets/img/blog/self_supervised_learning/cpc-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/self_supervised_learning/cpc.png" width="100%" height="auto" alt="CPC" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> CPC applied to audio, \(g_{enc}\) is the encoder, \(g_{ar}\) is the autoregressive model <a href="https://arxiv.org/abs/1807.03748" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>With these two models, we can generate an encoding of the data and context vectors. These vectors can be used as representations of the data. But how are these models trained? The self-supervised task is essentially predicting the input from the context. For example, given \(c_t\), we want to be able to go backwards and identify that it was generated from \(x_t/z_t\). The models are trained on a contrastive InfoNCE loss.</p> \[\begin{equation} \mathcal{L}_{\mathrm{InfoNCE}} = -\mathbb{E} \left[ \mathrm{log} \frac{ s(x, y) } { \sum_{y_j} s(x,y_j) } \right] \end{equation}\] <p>\(x\) is the sample we are trying to predict. \(c\) is the correct context. \(c_j\) are the context vectors for the negative samples. The negative samples come from other observations of the same datapoint and other datapoints in the batch. We want to maximize \(s(x,c)\) and minimize the sum of \(s(x, y_j)\). This is contrastive in that we are pushing \(y\) to be close to \(x\), and all other \(y_j\) to be far from \(x\).</p> \[\begin{equation} f_k(x_{t+k},c_t) = \mathrm{exp} \left( (g_{enc}(x_{t+k}))^TW_kc_t \right) = \mathrm{exp} \left( z_{t+k}^TW_kc_t \right) \end{equation}\] <p>The \(s\) function is modeled by \(f_k\) a log bilinear model. \(W_k\) is linear transforms the context vector, which can then be compared with the encoding \(z\).</p> <p>To apply this to vision, the image is split up into 7x7 patches (with 50% overlap) which will be considered the observations. Each patch is encoded by a CNN (ResNet without pretraining). If the encoding returns at 1024 dimensional vector, the encoded image will have a size of 7x7x1024. An autoregressive model (<a href="https://arxiv.org/abs/1606.05328" rel="external nofollow noopener" target="_blank">PixelCNN</a> or <a href="https://arxiv.org/abs/1601.06759" rel="external nofollow noopener" target="_blank">PixelRNN</a>) is applied to the encodings of the patches. For 1D data like audio, an RNN/LSTM scan be used. The self-supervised task in this case is predicting which patch generated each context vector. Refer to the PixelRNN paper for more information on autoregressive models and PixelCNN. The final representation is computed by mean pooling the encodings into a single 1024 dimensional vector. This can then be used for downstream tasks, like image classification.</p> <p>Why do we need the autoregressive model? We could optimize the InfoNCE loss using the 7x7 encodings. The self supervised task here is predicting the next context vector given a sequence of context vectors. This is similar to predicting the next patch of an image given all the previous patches. But rather predict the patch, which as we discussed is too difficult, we just predict a lower dimensional vector. Without this autoregressive constraint, we are just optimizing for generating unique embeddings for each patch and ignoring the relation between the patches. The InfoNCE loss is just ensuring that the predictions are correct.</p> <p>Why not just mask out the current context / observation? The architecture for this may be a masked fully connected layer that learns the context vector for each observation, while excluding the connection to the observation itself. Or there could be two PixelCNNs, one from the left and one from the right. We can then concatenate these two context vectors and possibly add additional neural network layers on top of it. Both methods would be more computationally expensive and complex, but likely still feasible. This would be bidirectional model for images similar to <a href="https://arxiv.org/abs/1810.04805" rel="external nofollow noopener" target="_blank">BERT</a>. This idea may be explored in other research papers or, it may be an open idea to try.</p> <h2 id="simclr"><a href="https://arxiv.org/abs/2002.05709" rel="external nofollow noopener" target="_blank">SimCLR</a></h2> <p>SimCLR is a method from Google Brain which takes a different approach for self-supervised learning of image representations. The basis of SimCLR is image augmentations. Image augmentation has long been used in supervised learning. The augmentations are transformations applied to the image and cropping, color change, and rotation. The idea is that these transformations do not change the content of the image and the network will learn to ignore and be invariant to these transformations. In supervised learning, data augmentation is used to just increase the size of the dataset for a supervised task like classification. Many SSL methods including SimCLR make invariance to the augmentation the actual learning objective. The augmented images are fed into an encoder to get the representation. These representations are then learned to be close of augmentations of the same image.</p> <p>However, the problem with just comparing within the same image is collapse. The network would learn the trivial solution of a constant vector for all representations (ex: a vector of all zeros). This would maximize the similarity between augmentations but obviously not contain any useful images. We need negative samples to minimize similarity with. In SimCLR, the negative samples are augmentations of other images from the same training batch. The assumption made here is that the other images are unrelated to the current image and the representations should be far apart.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/simclr_arch-480.webp 480w,/assets/img/blog/self_supervised_learning/simclr_arch-800.webp 800w,/assets/img/blog/self_supervised_learning/simclr_arch-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/self_supervised_learning/simclr_arch.png" width="500" height="auto" alt="SimCLR architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> The architecture of SimCLR. Diagram by Author, but dog images from SimCLR paper </figcaption> </figure> <p>Why do we need a projection head? It would not change the architecture much by optimizing the similarity losses on the representations themselves. The encoder may even include the same fully connected layers that would have been in the projection head. The projection head allows for a more complex and nonlinear similarity relationship between the encodings. Without it, the representations would have to have a high cosine similarity. This may restrict the expressivity of the vectors. The projection head can also ignore some information in the representations. For example, SimCLR may train to make the representations invariant to rotations. The rotation angle may be encoded in the representation but ignored by the projection head. If the rotation is encoded in the first 5 values of the vector, the projection MLP may have zero weights for those values. This may be desirable in a variant of the architecture in which the self-supervised learning happens simultaneously with a downstream task. The SimCLR architecture itself has no reason to include unnecessary information in the representation. It is unclear whether having “extra” information in the representation is desirable or not.</p> <p>Projection heads are very common in self-supervised learning. The autoregressive model in CPC can be viewed as a projection head.</p> <p>Aggressive augmentation yields the best results. This means applying multiple augmentations at a time. This makes the contrastive learning more challenging and forces the network to learn more about the image. The augmentations also avoid trivial solution to the contrastive objective. Without cropping, the network can match two augmented images by their local features (edges in the same place), instead of learning global features. Without color distortion, images can be matched by their color distribution. These augmentations can be composed with others, such as rotation and blur.</p> \[\begin{equation} \ell_{i,j} = \log{\frac{\exp(\mathrm{sim}(z_i,z_j)/\tau))}{\sum_{k=1}^{2N}\mathbb{1}_{[k\neq i]}\exp(\mathrm{sim}(z_i,z_k)/\tau)}} \end{equation}\] <p>The loss is referred to as NT-Xent (the normalized temperature-scaled cross entropy loss). The similarity function \(s\) can simply be cosine similarity (\(\frac{u^\top v}{\|u\|\|v\|}\)). This loss is similar to the InfoNCE loss. The main difference is the temperature \(\tau\). The temperature essentially controls how strongly should attract and repel the other vectors in the loss.</p> <h2 id="scaling">Scaling</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/scaling-480.webp 480w,/assets/img/blog/self_supervised_learning/scaling-800.webp 800w,/assets/img/blog/self_supervised_learning/scaling-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/self_supervised_learning/scaling.png" width="500" height="auto" alt="SSL scaling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2002.05709" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>Self-supervised learning is often evaluated on ImageNet classification. The projection head is replaced with a linear layer. The network is then trained to classify ImageNet with the encoder weights frozen. The encodings learned with self-supervision must be useful enough for a linear layer to classify them.</p> <p>An interesting property of self-supervised trained encoders, is how the scale in terms of depth and width. We see that only SimCLR(4x) is able to match the accuracy of fully supervised learning. “4x” means the network is 4 times as wide and as deep. It is not necessarily a bad thing that SSL requires a much larger network for ImageNet classification. This likely means the network is learning more information from the data than what is needed for supervised learning. Although this doesn’t help with ImageNet classification, the vectors may be more effective in other downstream tasks.</p> <p>One issue with SimCLR is its reliance on huge batch sizes. The best results come from a batch size of 4096. It needs many negative samples to be effective. This makes the network inefficient to train. Other approaches attempt to address this problem.</p> <h2 id="byol"><a href="https://arxiv.org/abs/2006.07733" rel="external nofollow noopener" target="_blank">BYOL</a></h2> <p>BYOL is a paper from Deepmind that aims to remove the need for negative samples. There are two networks: a target network and an online network. The target network’s weights are an exponential moving average of the online encoder. Similar to SimCLR, augmented versions of an image are passed through the encoders. Unlike SimCLR, the loss does not use negative examples so there is no need for large batch sizes. There is a projection head on top of the online encoder. The online encoder is used for downstream tasks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/byol-480.webp 480w,/assets/img/blog/self_supervised_learning/byol-800.webp 800w,/assets/img/blog/self_supervised_learning/byol-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/self_supervised_learning/byol.png" width="100%" height="auto" alt="BYOL architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> BYOL architecture <a href="https://arxiv.org/abs/2006.07733" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>Bootstrapping is a poorly defined word used in machine learning. It can mean simultaneously optimizing two objectives that depend on each. In BYOL, that refers to the two encoders.</p> <p>BYOL is able to learn useful representations without collapse because only the parameters of the online encoder are optimized. The online encoder can’t learn to output a constant because it is following the representations of the target encoder. The bootstrapping ensures that the trivial solution is avoided.</p> <p>BYOL is a non-contrastive method of SSL. However one criticism of BYOL is that batch normalization causes implicit contrastive learning by leaking information between batch elements. However, in a <a href="https://arxiv.org/abs/2010.10241" rel="external nofollow noopener" target="_blank">follow up paper</a>, the authors show that replacing batch normalization with group normalization and weight standardization leads to comparable performance.</p> <h1 id="clustering">Clustering</h1> <p>Clustering is an important class of unsupervised learning algorithms. Although more often used outside of deep learning, clustering can be applied to self supervised learning. Feature vectors can be clustered. Clusters can indicate a group of related images. In this sense clusters are similar to classes and can be used as labels in SSL.</p> <h2 id="deepcluster"><a href="https://arxiv.org/abs/1807.05520" rel="external nofollow noopener" target="_blank">DeepCluster</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/deepcluster-480.webp 480w,/assets/img/blog/self_supervised_learning/deepcluster-800.webp 800w,/assets/img/blog/self_supervised_learning/deepcluster-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/self_supervised_learning/deepcluster.png" width="100%" height="auto" alt="DeepCluster Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> DeepCluster architecture <a href="https://arxiv.org/abs/1807.05520" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>DeepCluster trains a neural network in two alternating steps: clustering and classification. In the clustering step, each image is assigned a cluster as a pseudolabel by clustering the feature vectors from the network. K-means is used for clustering. There are \(k\) clusters of the same dimension as the feature vectors. The network is then trained to predict the clusters from the images. After training on this classification objective, the features improve. The dataset is reclustered with better clusters. This iterative training procedure improves the clusters and the representations.</p> <p>The main problem with DeepCluster is that it requires periodically clustering the entire dataset. This limits this method in scaling to extremely large datasets. This is addressed by SwAV with an online approach to clustering based SSL.</p> <h2 id="swav"><a href="https://arxiv.org/abs/2006.09882" rel="external nofollow noopener" target="_blank">SwAV</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/self_supervised_learning/swav-480.webp 480w,/assets/img/blog/self_supervised_learning/swav-800.webp 800w,/assets/img/blog/self_supervised_learning/swav-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/self_supervised_learning/swav.png" width="500" height="auto" alt="SwAV" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> SwAV <a href="https://arxiv.org/abs/2006.09882" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>SwAV extends on DeepCluster to be online, while also taking inspiration from contrastive SSL methods. Two augmentations of an image are passed to an encoder. These representations are then assigned prototypes. There are K prototypes, which are vectors of the same representation as the encoding.</p> <h1 id="conclusion">Conclusion</h1> <p>There are many approaches to self supervised learning, however there are common elements. There are contrastive losses, data augmentation, bootstrapping, projection heads, and sometimes negative samples.</p> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Bandaru, Rohit (Aug 2021). Self-Supervised Learning  -  Getting more out of data. https://rohitbandaru.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">bandaru2021self-supervised-learning-getting-more-out-of-data</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Self-Supervised Learning  -  Getting more out of data}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Bandaru, Rohit}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Aug}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://rohitbandaru.github.io/blog/Self-Supervised-Learning/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Rohit Bandaru. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?3e7054dc4d3e3dd8f0731a48453e618e"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?3577194613afa04501eb52f8f4164de9" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-X48NHDB5RV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-X48NHDB5RV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-notes",title:"notes",description:"",section:"Navigation",handler:()=>{window.location.href="/notes/"}},{id:"post-transformer-design-guide-part-1-vanilla",title:"Transformer Design Guide (Part 1: Vanilla)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Transformer-Design-Guide-Pt1/"}},{id:"post-self-supervision-from-videos",title:"Self-Supervision from Videos",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Self-Supervision-from-Videos/"}},{id:"post-ssl-with-vision-transformers",title:"SSL with Vision Transformers",description:"",section:"Posts",handler:()=>{window.location.href="/blog/SSL-with-Vision-Transformers/"}},{id:"post-deep-dive-into-yann-lecun-s-jepa",title:"Deep Dive into Yann LeCun\u2019s JEPA",description:"",section:"Posts",handler:()=>{window.location.href="/blog/JEPA-Deep-Dive/"}},{id:"post-scaling-deep-learning",title:"Scaling Deep Learning",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Scaling-Deep-Learning/"}},{id:"post-knowledge-distillation-as-self-supervised-learning",title:"Knowledge Distillation as Self-Supervised Learning",description:"",section:"Posts",handler:()=>{window.location.href="/blog/knowledge-distillation-ssl/"}},{id:"post-self-supervised-learning-getting-more-out-of-data",title:"Self-Supervised Learning\u200a -\u200a Getting more out of\xa0data",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Self-Supervised-Learning/"}},{id:"post-domain-adaptation",title:"Domain Adaptation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Domain-Adaptation/"}},{id:"post-pruning-neural-networks",title:"Pruning Neural Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Neural-Network-Pruning/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%72%6F%68.%62%61%6E%64%61%72%75@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/rohit-bandaru","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/rohit_bandaru","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>