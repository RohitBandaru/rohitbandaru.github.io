<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="4zEwdc_gZtK7GAY69eUGNqwH6-hIgaXtASEwUuEDEho"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Domain Adaptation | Rohit Bandaru </title> <meta name="author" content="Rohit Bandaru"> <meta name="description" content="ML blog."> <meta name="keywords" content="machine-learning, ai, artificial-intelligence, research"> <meta property="og:site_name" content="Rohit Bandaru"> <meta property="og:type" content="article"> <meta property="og:title" content="Rohit Bandaru | Domain Adaptation"> <meta property="og:url" content="https://rohitbandaru.github.io/blog/Domain-Adaptation/"> <meta property="og:description" content="ML blog."> <meta property="og:image" content="https://rohitbandaru.github.io/assets/img/blog/domain_adaptation/framework.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Domain Adaptation"> <meta name="twitter:description" content="ML blog."> <meta name="twitter:image" content="https://rohitbandaru.github.io/assets/img/blog/domain_adaptation/framework.png"> <meta name="twitter:site" content="@rohit_bandaru"> <meta name="twitter:creator" content="@rohit_bandaru"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Rohit Bandaru"
        },
        "url": "https://rohitbandaru.github.io/blog/Domain-Adaptation/",
        "@type": "BlogPosting",
        "description": "ML blog.",
        "headline": "Domain Adaptation",
        
        "sameAs": ["https://www.linkedin.com/in/rohit-bandaru", "https://twitter.com/rohit_bandaru"],
        
        "name": "Rohit Bandaru",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/blog/favicon.ico?51e8729a60eb374d0910b53e1033a010"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rohitbandaru.github.io/blog/Domain-Adaptation/"> <script src="/assets/js/theme.js?bd888c560287cd675855c7662a167c4a"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rohit</span> Bandaru </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">notes </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Domain Adaptation</h1> <p class="post-meta"> Created in August 09, 2021 </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Machine learning performance depends on the dataset that it is trained on. Datasets are imperfect, so problems in the data affect the models. One type of problem is domain shift. This means that a model trained to learn a task on one dataset, may not be able to perform the same task on a slightly different dataset.</p> <p>Say you train a model to detect dogs in outdoor settings like public parks. It may perform very well on test images of dogs in outdoor places. However, that model may not function well when trying to detect dogs indoors, although the task itself is identical. This is a problem because the background of the image should not matter since you are just trying to detect dogs. We will explore four different research papers that address this problem.</p> <h1 id="vocabulary">Vocabulary</h1> <p>There are two datasets: a source dataset and a target dataset. The dataset that the model is trained on is the source dataset. The target dataset is the one that it will be tested on.</p> <p>For domain generalization, a similar problem, the target dataset is not available during training. The network is trained on the source dataset to not overfit to the domain-specific features.</p> <p>In domain adaptation, both the source and target datasets are available during training, but labels for the target dataset are not always available. For unsupervised domain adaptation, there are no labels available for the target dataset during training time. Semi-supervised domain adaptation involves a few labeled examples from the target dataset. With supervised domain adaptation, all the data from both the source and target datasets have labels.</p> <p>Unsupervised domain adaptation is the most commonly studied problem, as it has the most applications. Supervised DA can be useful when you have a labeled dataset, but it is too small to directly train on.</p> <p>These methods can be applied to many ML problems. However, a common application is image classification. I will focus on image classification on two common benchmark datasets: MNIST and SVHN. A model trained on handwritten digits (MNIST) often performs poorly on printed house number digits (SVHN).</p> <h1 id="adversarial-methods">Adversarial Methods</h1> <p>The most common approaches to the domain adaptation method follow an adversarial approach. For some context, I would suggest reading about <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29" rel="external nofollow noopener" target="_blank">Generative Adversarial Networks (GANs)</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/framework-480.webp 480w,/assets/img/blog/domain_adaptation/framework-800.webp 800w,/assets/img/blog/domain_adaptation/framework-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/domain_adaptation/framework.png" width="100%" height="auto" alt="Framework for Adversarial Domain Adaptation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> Framework for Adversarial Domain Adaptation </figcaption> </figure> <p>There are two encoders, which learn to produce a vector representation of each input. There is a classifier to classify the inputs and a discriminator that is trained to differentiate between the datasets. The goal is to eliminate differences in the domain from the encodings. This is similar to the GAN objective in that we want the encoders to fool the discriminator by generating encodings that are difficult to differentiate. However, this needs to be done such that the classifier is also effective for both datasets. The same classifier can then be applied to both datasets.</p> <p>There are many approaches to this with different training methods, architectures, and losses. The high-level goal is consistent. We want the encoders to generate encodings that contain the useful information needed for classification but remove the shift in domains.</p> <p>The key difference between the many algorithms is what the discriminator is and how it is trained. In simple cases, it is just an additional loss term. For example, maximum mean discrepancy (MMD) measures the difference between the encodings of the source and target datasets. Training the networks while minimizing the discrepancy can reduce domain shift. This may be useful for simple DA problems but does not work well for larger disparities.</p> <h2 id="adda"><a href="https://arxiv.org/abs/1702.05464" rel="external nofollow noopener" target="_blank">ADDA</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/adda-480.webp 480w,/assets/img/blog/domain_adaptation/adda-800.webp 800w,/assets/img/blog/domain_adaptation/adda-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/domain_adaptation/adda.png" width="100%" height="auto" alt="The steps of ADDA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> The steps of ADDA <a href="https://arxiv.org/abs/1702.05464" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>Adversarial Discriminative Domain Adaptation (ADDA) applies a simple approach to discriminative DA. There is only one encoder shared between the source and target datasets. The networks are trained in two steps.</p> <ol> <li> <p>The encoder and classifier are first trained to achieve high classification accuracy on the source dataset.</p> </li> <li> <p>The encoder is trained with the discriminator to lose domain discriminability. The discriminator is trained to classify the two domains with an adversarial loss. The encoder is trained with the negation of this loss since it is adversarial with respect to the discriminator. This negative is done through gradient reversal, which means in backpropagation, the gradients are negated before going to the encoder.</p> </li> </ol> <p>One major shortcoming of this approach is that the classification performance can be lost or forgotten in the adaptation step. This is because the labels are not used in this step.</p> <h2 id="dann"><a href="https://arxiv.org/abs/1505.07818" rel="external nofollow noopener" target="_blank">DANN</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/dann-480.webp 480w,/assets/img/blog/domain_adaptation/dann-800.webp 800w,/assets/img/blog/domain_adaptation/dann-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/domain_adaptation/dann.png" width="100%" height="auto" alt="DANN" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> DANN <a href="https://arxiv.org/abs/1505.07818" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>Domain-Adversarial Training of Neural Networks (DANN) is very similar to ADDA. Rather than have a separate adaptation step, the domain discriminator is trained alongside the classier. A gradient reversal layer is used because the domain discriminator and the classier have adversarial loss functions. This allows classification and discrimination to be trained together and avoid the network from forgetting the task.</p> <h1 id="image-translation">Image Translation</h1> <p>Another approach to addressing the domain gap is to convert examples from one domain to another. An example of this is transforming street-view digits (SVHN) to look like handwritten MNIST (digits). After this translation, you can apply an MNIST trained image classifier. The architectures are more complex because, in addition to the main task (image classification), the networks must translate images to and from the source and target domains.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/translation-480.webp 480w,/assets/img/blog/domain_adaptation/translation-800.webp 800w,/assets/img/blog/domain_adaptation/translation-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/domain_adaptation/translation.png" width="100%" height="auto" alt="SVHN DA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="image-to-image-translation"><a href="https://arxiv.org/abs/1712.00479" rel="external nofollow noopener" target="_blank">Image to Image translation</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/i2i-480.webp 480w,/assets/img/blog/domain_adaptation/i2i-800.webp 800w,/assets/img/blog/domain_adaptation/i2i-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/domain_adaptation/i2i.png" width="100%" height="auto" alt="I2I" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1712.00479" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>Like the adversarial methods Image to Image Translation (I2I) aims to learn a domain invariant encoding (Z) for the images. There are six networks in this architecture: the source encoder, source decoder, target encoder, target decoder, domain discriminator, and task network (ex: classifier). The decoders aim to reconstruct the images from the encoding. This also includes adversarial learning with the domain discriminator.</p> <p>The network is trained on a weighted combination of six different losses. The paper studies which combination of losses yields the best performance.</p> <ol> <li> <p>Qc is the classification loss on the source domain. We cannot get this loss for the target domain since there are no labels. However, the loss can be extended to include the target domain if labels exist.</p> </li> <li> <p>Qid is the loss of encoding an image and decoding it back into the same domain. Encoding an image into Z and decoding it back to the original domain should ideally return the same image. This loss can be the L1 norm of the difference between the original and decoded image.</p> </li> <li> <p>Qz is the domain discriminator’s loss. This is similar to ADDA in that it is trying to determine the domain of the encoding. We want this loss to increase as the encodings improve.</p> </li> <li> <p>Qtr is another discrimination loss in which the image is translated into the other domain before going to the domain discriminator.</p> </li> <li> <p>Qcyc is the cycle consistency loss. This loss is similar to Qid. The difference is that the images are decoded in the other domain before being encoding and decoded in the original domain. The image from the source domain is encoded into Z. This is decoded into the target domain and encoded back to Z. This is then decoded into the source domain and compared with the original image. A loss with the source and target switched is also applied. This aims to ensure encodings from similar images in different domains have similar encodings.</p> </li> <li> <p>Qtrc is similar to Qcyc, but instead of decoding back into the original domain, the encoding is classified. Unlike Qcyc, this is not symmetric since it involves labels. An image from the source domain is translated into the target domain and then classified.</p> </li> </ol> <h2 id="cycada"><a href="https://arxiv.org/abs/1711.03213" rel="external nofollow noopener" target="_blank">CyCADA</a></h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/domain_adaptation/cycada-480.webp 480w,/assets/img/blog/domain_adaptation/cycada-800.webp 800w,/assets/img/blog/domain_adaptation/cycada-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/domain_adaptation/cycada.png" width="100%" height="auto" alt="CyCADA" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/1711.03213" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>CyCADA is similar to I2I. Many of the I2I losses and networks have counterparts here. The main difference is that the target images are not translated to the source domain. Also, the GAN losses can be applied to both the images and the encodings.</p> <p>The source images are translated into the target domain. They are translated back into the source domain to apply the cycle consistency loss (L1 difference with the original image).</p> <p>The fs network is trained on the supervised learning task in the source domain. The semantic consistency loss ensures that the features from this network remain close before and after translation into the target domain. This ensures that the images retain the semantic information after translation.</p> <p>A GAN loss is then applied to the images and features (from fT) for the translated images and the target images. This loss is needed to train the translations to be similar to the target domain. There are two GAN losses to ensure that both the images and the features are similar.</p> <p>Finally, a task loss is applied to the translated images. This applies the task to the original target images.</p> <h1 id="other-domains">Other Domains</h1> <p>Image classification is the primary problem used to benchmark domain adaptation methods. However, domain adaptation can also be applied to other computer vision problems, such as image segmentation. It can also be applied in different research areas, such as natural language processing (NLP).</p> <p>One particularly interesting application of domain adaptation is self-driving cars and robotics. It is a common practice to train deep neural networks for these applications using data from simulated environments. It is much easier to collect large amounts of data in a simulation rather than in the real world. However, in order for a model trained on simulation data to function in a real-world environment, domain adaptation is often required to achieve good performance.</p> <p>There are also many variants to the problem, including few-shot domain adaptation, domain generalization, and multiclass domain adaptation.</p> <h1 id="conclusion">Conclusion</h1> <p>There are several approaches to domain adaptation but they often share some common characteristics. Adversarial learning with a domain discrimination network is common. There is also a lot of work using image to image translation with a cycle consistency loss. Apply domain adaptation to new problems will likely involve some combination of these components.</p> <h1 id="references">References</h1> <p>[1] Long, Mingsheng, et al. “Learning transferable features with deep adaptation networks.” International conference on machine learning. PMLR, 2015.</p> <p>[2] Eric Tzeng et al. “Adversarial discriminative domain adaptation”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017, pp. 7167-7176.</p> <p>[3] Yaroslav Ganin and Victor Lempitsky. “Unsupervised domain adaptation by backpropagation”. In: arXiv preprint arXiv:1409.7495 (2014).</p> <p>[4] Zak Murez et al. “Image to image translation for domain adaptation”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018, pp. 4500-4509.</p> <p>[5] Hoffman, Judy, et al. “Cycada: Cycle-consistent adversarial domain adaptation”. In: arXiv preprint arXiv:1711.03213 (2017).</p> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Bandaru, Rohit (Aug 2021). Domain Adaptation. https://rohitbandaru.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">bandaru2021domain-adaptation</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Domain Adaptation}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Bandaru, Rohit}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Aug}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://rohitbandaru.github.io/blog/Domain-Adaptation/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Rohit Bandaru. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?3e7054dc4d3e3dd8f0731a48453e618e"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?3577194613afa04501eb52f8f4164de9" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-X48NHDB5RV"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-X48NHDB5RV");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-notes",title:"notes",description:"",section:"Navigation",handler:()=>{window.location.href="/notes/"}},{id:"post-transformer-design-guide-part-2-modern-architecture",title:"Transformer Design Guide (Part 2: Modern Architecture)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Transformer-Design-Guide-Pt2/"}},{id:"post-transformer-design-guide-part-1-vanilla",title:"Transformer Design Guide (Part 1: Vanilla)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Transformer-Design-Guide-Pt1/"}},{id:"post-self-supervision-from-videos",title:"Self-Supervision from Videos",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Self-Supervision-from-Videos/"}},{id:"post-ssl-with-vision-transformers",title:"SSL with Vision Transformers",description:"",section:"Posts",handler:()=>{window.location.href="/blog/SSL-with-Vision-Transformers/"}},{id:"post-deep-dive-into-yann-lecun-s-jepa",title:"Deep Dive into Yann LeCun\u2019s JEPA",description:"",section:"Posts",handler:()=>{window.location.href="/blog/JEPA-Deep-Dive/"}},{id:"post-scaling-deep-learning",title:"Scaling Deep Learning",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Scaling-Deep-Learning/"}},{id:"post-knowledge-distillation-as-self-supervised-learning",title:"Knowledge Distillation as Self-Supervised Learning",description:"",section:"Posts",handler:()=>{window.location.href="/blog/knowledge-distillation-ssl/"}},{id:"post-self-supervised-learning-getting-more-out-of-data",title:"Self-Supervised Learning\u200a -\u200a Getting more out of\xa0data",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Self-Supervised-Learning/"}},{id:"post-domain-adaptation",title:"Domain Adaptation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Domain-Adaptation/"}},{id:"post-pruning-neural-networks",title:"Pruning Neural Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Neural-Network-Pruning/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%72%6F%68.%62%61%6E%64%61%72%75@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/rohit-bandaru","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/rohit_bandaru","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>