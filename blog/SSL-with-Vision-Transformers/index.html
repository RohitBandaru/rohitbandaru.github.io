<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="4zEwdc_gZtK7GAY69eUGNqwH6-hIgaXtASEwUuEDEho"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SSL with Vision Transformers | Rohit Bandaru </title> <meta name="author" content="Rohit Bandaru"> <meta name="description" content="ML blog."> <meta name="keywords" content="machine-learning, ai, artificial-intelligence, research"> <meta property="og:site_name" content="Rohit Bandaru"> <meta property="og:type" content="article"> <meta property="og:title" content="Rohit Bandaru | SSL with Vision Transformers"> <meta property="og:url" content="https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers/"> <meta property="og:description" content="ML blog."> <meta property="og:image" content="https://rohitbandaru.github.io/assets/img/blog/ssl-vit/data2vec_architecture.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="SSL with Vision Transformers"> <meta name="twitter:description" content="ML blog."> <meta name="twitter:image" content="https://rohitbandaru.github.io/assets/img/blog/ssl-vit/data2vec_architecture.png"> <meta name="twitter:site" content="@rohit_bandaru"> <meta name="twitter:creator" content="@rohit_bandaru"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Rohit Bandaru"
        },
        "url": "https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers/",
        "@type": "BlogPosting",
        "description": "ML blog.",
        "headline": "SSL with Vision Transformers",
        
        "sameAs": ["https://www.linkedin.com/in/rohit-bandaru", "https://twitter.com/rohit_bandaru"],
        
        "name": "Rohit Bandaru",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers/"> <script src="/assets/js/theme.js?bd888c560287cd675855c7662a167c4a"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rohit</span> Bandaru </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">notes </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">SSL with Vision Transformers</h1> <p class="post-meta"> Created in August 01, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/self-supervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> self-supervised-learning</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In recent years, self-supervised learning (SSL) has emerged as a powerful paradigm in computer vision, allowing models to learn meaningful representations from unlabeled data. Prior work in this field focuses on using CNN architectures such as ResNet for this task. However, as evidenced by the success of self-supervised language models, transformers are a natural fit for self-supervised training. We will cover a set of recent papers that apply transformers for self-supervised visual learning.</p> <p>One key variation is that you often see masking in these methods. CNN-based SSL methods rely more on data augmentations to create a prediction task for the model. Masking is advantageous for several reasons outlined below, and it also aligns more with language model training (example: BERT).</p> <ul> <li>Computational efficiency <ul> <li>You do not have to process the masked regions of the image when a large portion of the image is masked.</li> </ul> </li> <li>Data augmentations can introduce unwanted invariances and remove useful information <ul> <li>For example, a data augmentation that strongly distorts the color may result in representations that do not encode color.</li> </ul> </li> </ul> <p>Masking is more naturally enabled by the transformer architecture. There is a reason that masking-based SSL training hasn’t worked well with CNNs.</p> <p>By examining these different methods, we’ll discuss what makes transformers work for vision.</p> <h1 id="dino"><a href="https://arxiv.org/abs/2104.14294" rel="external nofollow noopener" target="_blank"><strong>DINO</strong></a></h1> <p>This paper (Emerging Properties in Self-Supervised Vision Transformers) by Caron et al. introduces a new self-supervised training method called DINO, which they apply to vision transformers. They argue that transformers are better than CNNs for images with SSL training, more so than with supervised training. Transformers can match the performance of CNNs with supervised training, albeit with more training cost. However, they have more useful properties with SSL training. This follows our intuition that SSL and transformers are a natural combination.</p> <p>DINO takes inspiration from <a href="https://arxiv.org/abs/2006.07733" rel="external nofollow noopener" target="_blank">BYOL</a> but introduces two key innovations:</p> <ol> <li>A novel loss function that enables direct matching between student and teacher outputs</li> <li>Elimination of the prediction layer on the student, simplifying the architecture</li> </ol> <p>These changes result in a self-distillation approach that proves particularly effective with vision transformers.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/dino-480.webp 480w,/assets/img/blog/ssl-vit/dino-800.webp 800w,/assets/img/blog/ssl-vit/dino-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/ssl-vit/dino.png" class="img-fluid mx-auto d-block" width="400" height="auto" alt="DINO architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2006.07733" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <ol> <li>Two views of an image \(x\), \(x_1\) and \(x_2\) are generated through data augmentations. <ol> <li>A multi crop strategy is used in which two large global views are generated along with a set of smaller cropped local views. The teacher only processes global views, while the student processes all views, with the constraint that the loss is not trying to match the same views to each other. This method was introduced in the <a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper_files/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf&amp;hl=en&amp;sa=T&amp;oi=gsr-r-gga&amp;ct=res&amp;cd=0&amp;d=13209348926291080860&amp;ei=QYYkZu2RB5SCy9YP29Cc0AY&amp;scisig=AFWwaea44-zuGhikZl27njOvnygp" rel="external nofollow noopener" target="_blank">SwAV</a> paper, and helps the model learn local to global correspondences. Restricting the teacher to only global views also encourages the encoders to output global representations.</li> <li>Are position embeddings used?</li> </ol> </li> <li>The views are passed to their respective encoder (teacher/student)</li> <li>The teacher encoding is “centered”. <ol> <li>Perhaps centering allows this method to work without having the predictor layer. The center is a exponential moving average of the teacher encoding (of both views). This vector is subtracted from the teacher’s encoding before the softmax. A temperature is also applied with the softmax to achieve a “sharpening”. These methods help the teacher avoid collapse. Centering ensures that a single component of the vector doesn’t dominate. Sharpening ensures that it doesn’t collapse to a uniform vector.</li> </ol> </li> <li>Softmax is applied to each encoding. The student is trained with a cross entropy loss to match the teacher. The teachers weights are updated as an exponential moving average of the student.</li> </ol> <p>This paper compares the performance of DINO with ResNet and ViT architectures against <a href="https://rohitbandaru.github.io/blog//SSL-with-Vision-Transformers/">SOTA SSL methods</a> such as <a href="https://arxiv.org/abs/2006.07733" rel="external nofollow noopener" target="_blank">BYOL</a>, MoCov2, and SwAV. The combination os DINO and ViT has the most significant advantage. Interestingly, it is 6.6% better than ViT with BYOL training on linear ImageNet evaluation, despite minor differences in the methods. The SSL methods that are used for comparison were developed for CNN architectures, which put them at a disadvantage. DINO is designed for transformers, but what about it makes it work better with transformers? One possible explanation is that transformers handle different resolutions of images better. Higher resolution images results in more image patches generated in the transformer. The computation also scales quadratically in the attention operations with respect to the number of patches. For ResNet, the computation increases linearly.</p> <p>The two main “emerging properties” they observe is that DINO ViT features are useful for dense predictions such as semantic segmentation. Another property is that k nearest neighbors on the output encodings, without any finetuning. This enables image retrieval applications.</p> <p>They observe the teacher outperforms the student in DINO training. This is not observed with other SSL methods. They cite “Polyak-Ruppert averaging” as an explantation of this. This means the teacher simulates an ensemble model with its momentum weights.</p> <p>The multi-crop strategy enforces that the inputs be rectangular. This makes this method compatible with CNNs in addition to ViTs. DINO shows that SSL is effective with vision transformers. However, it is designed in a way that makes the training method compatible with CNNs. This leads to some very interesting comparisons between the properties of SSL CNN and ViT models. The other works we will discuss take advantage of the flexibility of the transformer architecture, at the cost of CNN compatibility.</p> <p><a href="https://arxiv.org/abs/2304.07193" rel="external nofollow noopener" target="_blank">DINOv2: Learning Robust Visual Features without Supervision</a> scales DINO using a 1 billion parameter ViT model along with a larger proprietary dataset. They used an interesting data processing pipeline to combine curated and uncurated data, to get a large dataset of high quality and diverse images. This step is important because unprocessed uncurated data can be of low quality and dominated by certain modes of data and duplicated data.</p> <p>There are several architectural and training changes applied on top DINO v1 that allow it to scale effectively. Notably, in addition to DINO, they add an <a href="https://arxiv.org/abs/2111.07832" rel="external nofollow noopener" target="_blank">iBOT</a> loss. This method masks some of the input tokens of the student. In order to combine DINO and iBOT losses, they learn separate heads on the student and teacher for each loss. iBOT does BERT style pretraining of image transformers, which we will also cover in this post.</p> <h1 id="data2vec"><a href="https://arxiv.org/abs/2202.03555" rel="external nofollow noopener" target="_blank">data2vec</a></h1> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec-480.webp 480w,/assets/img/blog/ssl-vit/data2vec-800.webp 800w,/assets/img/blog/ssl-vit/data2vec-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/ssl-vit/data2vec.png" class="image-fluid mx-auto d-block" width="100%" height="auto" alt="data2vec architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2202.03555" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>The teacher model predicts representations from unmasked input, while the student model predicts representations from masked input. The student aims to match the teacher’s output by predicting the representations of the masked tokens. To avoid collapse, the teacher’s weights are an exponential moving average of the student’s weights.</p> <p>Instead of training a multimodal model, independent models are trained on different modalities. Data2VecAudio, Data2VecText, and Data2VecVision are developed. The learning objective remains the same, but the generation of embeddings and masking strategies differ.</p> <ol> <li>Encoding of inputs into embeddings: <ol> <li>Text is tokenized, and learned embeddings for each token are retrieved.</li> <li>Images are divided into 16x16 patches and linearly projected into an embedding.</li> <li>Audio is encoded by a 1D convolutional neural network with multiple layers. A 16 kHz waveform is mapped to a 50 Hz representation. This means a sequence of 320 integers is mapped to a single representation. <ol> <li>Unlike images, a multiple-layer network is used for audio, likely due to the absence of a Fourier transform.</li> </ol> </li> </ol> </li> <li>Masking: <ol> <li>Some of the student input embeddings are replaced by the MASK token embedding. <ol> <li>Text: Random tokens are masked.</li> <li>Images: Embeddings corresponding to rectangular blocks are masked.</li> <li>Audio: Continuous spans of embeddings are masked.</li> </ol> </li> </ol> </li> <li>Addition of position encoding.</li> <li>Both the teacher and student transformer models receive the input.</li> <li>Representations at different layers are distilled from the teacher to the student. Outputs from the masked tokens of the top \(K\) transformer blocks are normalized and averaged into a single vector.</li> <li>A regression loss (Smooth L1) is applied to the averaged vectors of each network. <ol> <li>The loss transitions from a squared loss to an L2 loss when the error margin goes below the hyperparameter \(\beta\). The L2 loss is only applied when the student and teacher predictions are close. This loss is designed to be less sensitive to outliers.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec_loss-480.webp 480w,/assets/img/blog/ssl-vit/data2vec_loss-800.webp 800w,/assets/img/blog/ssl-vit/data2vec_loss-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/ssl-vit/data2vec_loss" class="mx-auto d-block" width="500" height="auto" alt="data2vec loss" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2202.03555" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <ol> <li>The students weights are updated with SGD. The teacher’s weights are updated as a EMA of the students weights: \(\Delta \leftarrow \tau \Delta + (1-\tau)\theta\) <ol> <li>\(\Delta\) represents the teacher’s parameters, while \(\theta\) represents the student’s parameters.</li> </ol> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec_architecture-480.webp 480w,/assets/img/blog/ssl-vit/data2vec_architecture-800.webp 800w,/assets/img/blog/ssl-vit/data2vec_architecture-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/ssl-vit/data2vec_architecture.png" class="mx-auto d-block" width="500" height="auto" alt="data2vec architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The position encoding and feature encoder weights are shared between the two models. However, the teacher’s transformer weights are specified through an exponential moving average.</p> <p><a href="https://arxiv.org/abs/2212.07525" rel="external nofollow noopener" target="_blank"><strong>data2vec 2.0</strong></a></p> <p>Data2Vec 2.0 introduces several architectural and loss function changes that lead to a significant speed up in training.</p> <p>They use target representations for multiple masked predictions of a sample. This is more computationally efficient because we only need to run the teacher model once to train with \(M\) different masks of the input instead of 1. Further efficiency gains are implemented through not processing the masked parts of the image with the student, and sharing the feature encoder output across all masks.</p> <p>They use a L2 loss instead of a smooth L1 loss. This is a simplification of the earlier loss. They also use a convolutional decoder to predict the masked representations rather than a transformer.</p> <p>They also introduce inverse block masking. Rather than masking blocks. Blocks are chosen to be unmasked areas. The representations outside of the block will be predicted. There are multiple blocks which may overlap. A mask consists of multiple blocks. Training includes multiple masks for each target.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/data2vec_2-480.webp 480w,/assets/img/blog/ssl-vit/data2vec_2-800.webp 800w,/assets/img/blog/ssl-vit/data2vec_2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/ssl-vit/data2vec_2.png" class="mx-auto d-block" width="100%" height="auto" alt="data2vec 2.0" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2212.07525" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>They also add a linear attention bias (<a href="https://arxiv.org/abs/2108.12409" rel="external nofollow noopener" target="_blank">ALiBi</a>). This essentially modifies self attention to increase the bias for query key pairs that are far apart. This enables faster training by providing an inductive bias.</p> <h1 id="masked-autoencoders-are-scalable-vision-learners"><a href="https://arxiv.org/abs/2111.06377" rel="external nofollow noopener" target="_blank">Masked Autoencoders Are Scalable Vision Learners</a></h1> <p>This paper uses a simple autoencoder architecture to learn image representations. Parts of the images are masked, and the model is tasked to predict what is in the masked regions. This model can be trained through this <a href="https://github.com/ariG23498/mae-scalable-vision-learners/blob/master/mae-pretraining.ipynb" rel="external nofollow noopener" target="_blank">notebook</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/mae-480.webp 480w,/assets/img/blog/ssl-vit/mae-800.webp 800w,/assets/img/blog/ssl-vit/mae-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/ssl-vit/mae.png" class="mx-auto d-block" width="100%" height="auto" alt="Masked Autoencoder" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2111.06377" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <ol> <li>The image is split into patches, as done in Vision Transformers.</li> <li>Using a mask ratio (75%–95%), patches are selected randomly without replacement.</li> <li>The unmasked patches are input into the encoder. Note that the mask tokens do not get processed by the encoder (difference from BERT). The encoder uses a vanilla ViT architecture, where the unmasked patches are linearly projected into token embeddings which get processed by transformer blocks. The output is a ViT-processed embedding for each unmasked patch. Each patch has an added position embedding.</li> <li>The encoded tokens and the masked tokens are combined as an input to the decoder. The mask tokens map to a learned embedding. This embedding will be the same at all positions because it is not transformed by the encoder. At this stage, position embeddings are added to the full set. <ol> <li>Note that for unmasked tokens, position embeddings are added twice, once before the encoder and once before the decoder.</li> </ol> </li> <li>The decoder reconstructs the unmasked image from the set of patch embeddings. The decoder is trained by a mean squared error loss with respect to the unmasked input image.</li> </ol> <p>This architecture builds on the vision transformer. An alternative is to use CNNs. This would involve directly setting pixels in the input image to zero, learning a vector representation, and then decoding it back to the image. The reason this fails is that it aims to globally decode an image. With transformers, you first predict representations of the masked patches, and then decode into the image patch. This breaks it down into two easier problems. Also, with CNNs, you can’t explicitly encode masked regions like you can with a ViT. Having a mask token more explicitly indicates the mask.</p> <p>They mask a very high percentage of patches (80%). This reduces spatial redundancy and forces the model to learn more higher-level and useful features. With a lower mask ratio, the model might learn to represent small local changes, like color and lighting variation. It doesn’t need to understand the higher-level structure of the image, because it’s mostly already there. This is a notable change from language models. BERT masks 15% of tokens. MAE and related works mask a majority of the image (75%+).</p> <p>The model uses the ImageNet-1K dataset for pretraining and evaluation. Evaluation is done by either finetuning the full encoder model or using a linear probe (training one MLP layer on the output of the encoder) on the task of classification.</p> <p>One interesting result is that the performance of finetuning and linear probing has different trends when ablating the masking ratio. Linear probing accuracy increases linearly with masking ratio until 75%. Finetuning has relatively consistent performance between 40% and 80%.</p> <p>Having a deep decoder allows for the representations to be more abstract, because the decoder has more capacity for reconstruction. A shallower decoder would lead to the encoder having to represent more of the details needed for reconstruction. This is less relevant for finetuning than it is for linear probing, as during finetuning, the encoder can shift from focusing on reconstruction to recognition. In my opinion, linear probing results are more interesting since the goal is to build useful representations that can be used for various tasks. Finetuning offers just a marginal improvement over just training on the classification task directly without pretraining at all. However, linear probing discourages learning nonlinear features in the representation. To address this, the authors evaluate “partial finetuning” in which the last few blocks of the transformer are finetuned.</p> <p>Excluding mask tokens from the input and using a lightweight decoder makes this model very efficient to train. Using mask tokens in the encoder also creates a domain shift between pretraining and downstream tasks, which hurts performance. This is because a large portion of the pretraining input will be mask tokens, which is significantly different from what the model will see downstream.</p> <h1 id="beit-bert-pre-training-of-image-transformers"><a href="https://arxiv.org/abs/2106.08254" rel="external nofollow noopener" target="_blank"><strong>BEiT: BERT Pre-Training of Image Transformers</strong></a></h1> <p>This approach is most similar to BERT / NLP SSL models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/ssl-vit/beit-480.webp 480w,/assets/img/blog/ssl-vit/beit-800.webp 800w,/assets/img/blog/ssl-vit/beit-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/ssl-vit/beit.png" class="mx-auto d-block" width="100%" height="auto" alt="beit" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> <a href="https://arxiv.org/abs/2106.08254" rel="external nofollow noopener" target="_blank"> Source </a> </figcaption> </figure> <p>A fundamental difference in applying SSL to images compared to text is that images are continuous. Text has a finite number of tokens. You can use a softmax to get a probability distribution across all tokens. In ViTs, patches of an image are treated as tokens. However, you can’t get an explicit probability distribution over all possible image patches. BEiT addresses this problem by training a discrete variational autoencoder (dVAE) to learn discrete visual tokens. These discrete tokens are an approximation or compression of image patches.</p> <p>The main difference between this and a vanilla ViT architecture is the usage of discrete visual tokens.</p> <p>There are two steps to training:</p> <ol> <li>Tokenizer and Decoder are trained as a VAE to learn discrete visual tokens</li> <li>The discrete tokens from the learned tokenizer are used to pretrain a BEiT encoder.</li> </ol> <p>Why aren’t the tokens used as the input directly? The softmax distribution of tokens could be used as a soft label for the BEiT encoder.</p> <p>The transformer training task is named masked image modeling (MIM), as it is designed after BERT’s masked language modeling (MLM). 40% of the tokens are masked. Similar to other methods, BEiT masks a large portion of the image to make the pretraining task sufficiently difficult.</p> <h1 id="conclusion">Conclusion</h1> <p>The landscape of self-supervised learning for image processing is undergoing a significant transformation. While it originated with Convolutional Neural Networks (CNNs), a strong coupling with transformer-based architectures is emerging and may lead the way for further advancements.</p> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Bandaru, Rohit (Aug 2024). SSL with Vision Transformers. https://rohitbandaru.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">bandaru2024ssl-with-vision-transformers</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{SSL with Vision Transformers}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Bandaru, Rohit}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Aug}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://rohitbandaru.github.io/blog/SSL-with-Vision-Transformers/}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Rohit Bandaru. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?3e7054dc4d3e3dd8f0731a48453e618e"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?3577194613afa04501eb52f8f4164de9" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-notes",title:"notes",description:"",section:"Navigation",handler:()=>{window.location.href="/notes/"}},{id:"post-transformer-design-guide-part-2-modern-architecture",title:"Transformer Design Guide (Part 2: Modern Architecture)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Transformer-Design-Guide-Pt2/"}},{id:"post-transformer-design-guide-part-1-vanilla",title:"Transformer Design Guide (Part 1: Vanilla)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Transformer-Design-Guide-Pt1/"}},{id:"post-self-supervision-from-videos",title:"Self-Supervision from Videos",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Self-Supervision-from-Videos/"}},{id:"post-ssl-with-vision-transformers",title:"SSL with Vision Transformers",description:"",section:"Posts",handler:()=>{window.location.href="/blog/SSL-with-Vision-Transformers/"}},{id:"post-deep-dive-into-yann-lecun-s-jepa",title:"Deep Dive into Yann LeCun\u2019s JEPA",description:"",section:"Posts",handler:()=>{window.location.href="/blog/JEPA-Deep-Dive/"}},{id:"post-scaling-deep-learning",title:"Scaling Deep Learning",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Scaling-Deep-Learning/"}},{id:"post-knowledge-distillation-as-self-supervised-learning",title:"Knowledge Distillation as Self-Supervised Learning",description:"",section:"Posts",handler:()=>{window.location.href="/blog/knowledge-distillation-ssl/"}},{id:"post-self-supervised-learning-getting-more-out-of-data",title:"Self-Supervised Learning\u200a -\u200a Getting more out of\xa0data",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Self-Supervised-Learning/"}},{id:"post-domain-adaptation",title:"Domain Adaptation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Domain-Adaptation/"}},{id:"post-pruning-neural-networks",title:"Pruning Neural Networks",description:"",section:"Posts",handler:()=>{window.location.href="/blog/Neural-Network-Pruning/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%72%6F%68.%62%61%6E%64%61%72%75@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/rohit-bandaru","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/rohit_bandaru","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>